{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491b1df7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-03T12:08:41.753672Z",
     "iopub.status.busy": "2022-07-03T12:08:41.753154Z",
     "iopub.status.idle": "2022-07-03T12:08:41.763093Z",
     "shell.execute_reply": "2022-07-03T12:08:41.762356Z"
    },
    "papermill": {
     "duration": 0.024366,
     "end_time": "2022-07-03T12:08:41.765684",
     "exception": false,
     "start_time": "2022-07-03T12:08:41.741318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b739e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:08:41.786627Z",
     "iopub.status.busy": "2022-07-03T12:08:41.785712Z",
     "iopub.status.idle": "2022-07-03T12:09:11.944701Z",
     "shell.execute_reply": "2022-07-03T12:09:11.943594Z"
    },
    "papermill": {
     "duration": 30.172136,
     "end_time": "2022-07-03T12:09:11.947353",
     "exception": false,
     "start_time": "2022-07-03T12:08:41.775217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from talib-binary==0.4.19) (1.21.6)\r\n",
      "Installing collected packages: talib-binary\r\n",
      "Successfully installed talib-binary-0.4.19\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ../input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\n",
    "import talib as ta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fbc1022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:09:11.968895Z",
     "iopub.status.busy": "2022-07-03T12:09:11.968506Z",
     "iopub.status.idle": "2022-07-03T12:09:14.537217Z",
     "shell.execute_reply": "2022-07-03T12:09:14.536295Z"
    },
    "papermill": {
     "duration": 2.582198,
     "end_time": "2022-07-03T12:09:14.539624",
     "exception": false,
     "start_time": "2022-07-03T12:09:11.957426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import early_stopping\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler\n",
    "import missingno as msno\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import ta\n",
    "from talib import abstract\n",
    "\n",
    "\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from decimal import ROUND_HALF_UP, Decimal\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# sys.path.insert(0, '../input/jpx-local-api')\n",
    "# from local_api import local_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f63e153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:09:14.560892Z",
     "iopub.status.busy": "2022-07-03T12:09:14.560548Z",
     "iopub.status.idle": "2022-07-03T12:09:24.043870Z",
     "shell.execute_reply": "2022-07-03T12:09:24.042911Z"
    },
    "papermill": {
     "duration": 9.496489,
     "end_time": "2022-07-03T12:09:24.045864",
     "exception": false,
     "start_time": "2022-07-03T12:09:14.549375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjustmentFactor</th>\n",
       "      <th>ExpectedDividend</th>\n",
       "      <th>SupervisionFlag</th>\n",
       "      <th>Target</th>\n",
       "      <th>33SectorCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170104_1301</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1301</td>\n",
       "      <td>2734.0</td>\n",
       "      <td>2755.0</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>2742.0</td>\n",
       "      <td>31400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170104_1332</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1332</td>\n",
       "      <td>568.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2798500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.012324</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170104_1333</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1333</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>270800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170104_1376</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1376</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>11300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.011053</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170104_1377</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1377</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>150800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602407</th>\n",
       "      <td>20220624_9990</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>9990</td>\n",
       "      <td>576.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>24200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.027073</td>\n",
       "      <td>6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602408</th>\n",
       "      <td>20220624_9991</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>9991</td>\n",
       "      <td>810.0</td>\n",
       "      <td>815.0</td>\n",
       "      <td>804.0</td>\n",
       "      <td>815.0</td>\n",
       "      <td>8700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>6050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602409</th>\n",
       "      <td>20220624_9993</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>9993</td>\n",
       "      <td>1548.0</td>\n",
       "      <td>1548.0</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>12600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602410</th>\n",
       "      <td>20220624_9994</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>9994</td>\n",
       "      <td>2507.0</td>\n",
       "      <td>2527.0</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>2527.0</td>\n",
       "      <td>7300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602411</th>\n",
       "      <td>20220624_9997</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>9997</td>\n",
       "      <td>710.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>719.0</td>\n",
       "      <td>139600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.015089</td>\n",
       "      <td>6100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2602412 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RowId        Date SecuritiesCode    Open    High     Low  \\\n",
       "0        20170104_1301  2017-01-04           1301  2734.0  2755.0  2730.0   \n",
       "1        20170104_1332  2017-01-04           1332   568.0   576.0   563.0   \n",
       "2        20170104_1333  2017-01-04           1333  3150.0  3210.0  3140.0   \n",
       "3        20170104_1376  2017-01-04           1376  1510.0  1550.0  1510.0   \n",
       "4        20170104_1377  2017-01-04           1377  3270.0  3350.0  3270.0   \n",
       "...                ...         ...            ...     ...     ...     ...   \n",
       "2602407  20220624_9990  2022-06-24           9990   576.0   576.0   563.0   \n",
       "2602408  20220624_9991  2022-06-24           9991   810.0   815.0   804.0   \n",
       "2602409  20220624_9993  2022-06-24           9993  1548.0  1548.0  1497.0   \n",
       "2602410  20220624_9994  2022-06-24           9994  2507.0  2527.0  2498.0   \n",
       "2602411  20220624_9997  2022-06-24           9997   710.0   725.0   710.0   \n",
       "\n",
       "          Close   Volume  AdjustmentFactor  ExpectedDividend  SupervisionFlag  \\\n",
       "0        2742.0    31400               1.0               NaN            False   \n",
       "1         571.0  2798500               1.0               NaN            False   \n",
       "2        3210.0   270800               1.0               NaN            False   \n",
       "3        1550.0    11300               1.0               NaN            False   \n",
       "4        3330.0   150800               1.0               NaN            False   \n",
       "...         ...      ...               ...               ...              ...   \n",
       "2602407   564.0    24200               1.0               NaN            False   \n",
       "2602408   815.0     8700               1.0               NaN            False   \n",
       "2602409  1497.0    12600               1.0               NaN            False   \n",
       "2602410  2527.0     7300               1.0               NaN            False   \n",
       "2602411   719.0   139600               1.0               NaN            False   \n",
       "\n",
       "           Target 33SectorCode  \n",
       "0        0.000730           50  \n",
       "1        0.012324           50  \n",
       "2        0.006154           50  \n",
       "3        0.011053           50  \n",
       "4        0.003026           50  \n",
       "...           ...          ...  \n",
       "2602407  0.027073         6100  \n",
       "2602408  0.001220         6050  \n",
       "2602409  0.001329         6100  \n",
       "2602410  0.003185         6100  \n",
       "2602411  0.015089         6100  \n",
       "\n",
       "[2602412 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')\n",
    "\n",
    "# using supplement data as test data\n",
    "supp_data = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv')\n",
    "\n",
    "# stock code\n",
    "stock_list = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv')\n",
    "main_list = stock_list.set_index('SecuritiesCode').loc[np.sort(train['SecuritiesCode'].unique())]\n",
    "\n",
    "# add stock type to data\n",
    "train = pd.merge(train, main_list['33SectorCode'].astype('category'), on='SecuritiesCode').sort_values(['Date','SecuritiesCode']).astype({'SecuritiesCode':'category'})\n",
    "\n",
    "supp_data = pd.merge(supp_data, main_list['33SectorCode'].astype('category'), on='SecuritiesCode').sort_values(['Date','SecuritiesCode']).astype({'SecuritiesCode':'category'})\n",
    "\n",
    "train_with_supp = pd.concat([train, supp_data]).reset_index(drop=True)\n",
    "# train_with_supp = train.copy()\n",
    "train_with_supp\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e31b51a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:09:24.067443Z",
     "iopub.status.busy": "2022-07-03T12:09:24.067159Z",
     "iopub.status.idle": "2022-07-03T12:09:24.075292Z",
     "shell.execute_reply": "2022-07-03T12:09:24.074264Z"
    },
    "papermill": {
     "duration": 0.021472,
     "end_time": "2022-07-03T12:09:24.077545",
     "exception": false,
     "start_time": "2022-07-03T12:09:24.056073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): predicted results\n",
    "        portfolio_size (int): # of equities to buy/sell\n",
    "        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "    Returns:\n",
    "        (float): sharpe ratio\n",
    "    \"\"\"\n",
    "    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): predicted results\n",
    "            portfolio_size (int): # of equities to buy/sell\n",
    "            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "        Returns:\n",
    "            (float): spread return\n",
    "        \"\"\"\n",
    "        assert df['Rank'].min() == 0\n",
    "        assert df['Rank'].max() == len(df['Rank']) - 1\n",
    "        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n",
    "        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        return purchase - short\n",
    "\n",
    "    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n",
    "    sharpe_ratio = buf.mean() / buf.std()\n",
    "    return sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e071ca02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:09:24.103301Z",
     "iopub.status.busy": "2022-07-03T12:09:24.102980Z",
     "iopub.status.idle": "2022-07-03T12:09:24.111213Z",
     "shell.execute_reply": "2022-07-03T12:09:24.109858Z"
    },
    "papermill": {
     "duration": 0.023563,
     "end_time": "2022-07-03T12:09:24.113387",
     "exception": false,
     "start_time": "2022-07-03T12:09:24.089824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_adjusted_close(df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n",
    "    Returns:\n",
    "        df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n",
    "    \"\"\"\n",
    "    # sort data to generate CumulativeAdjustmentFactor\n",
    "    df = df.sort_values(\"Date\", ascending=False)\n",
    "    # generate CumulativeAdjustmentFactor\n",
    "    df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n",
    "    # generate AdjustedClose\n",
    "    df.loc[:, \"AdjustedClose\"] = (\n",
    "        df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n",
    "    ).map(lambda x: float(\n",
    "        Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n",
    "    ))\n",
    "    # reverse order\n",
    "    df = df.sort_values(\"Date\")\n",
    "    # to fill AdjustedClose, replace 0 into np.nan\n",
    "    df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n",
    "    # forward fill AdjustedClose\n",
    "    df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n",
    "\n",
    "    df = df.drop(columns=['CumulativeAdjustmentFactor'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd9b53a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:09:24.136419Z",
     "iopub.status.busy": "2022-07-03T12:09:24.136118Z",
     "iopub.status.idle": "2022-07-03T12:09:24.161336Z",
     "shell.execute_reply": "2022-07-03T12:09:24.160381Z"
    },
    "papermill": {
     "duration": 0.03949,
     "end_time": "2022-07-03T12:09:24.163614",
     "exception": false,
     "start_time": "2022-07-03T12:09:24.124124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ta_features(df, inf=None, train=True):\n",
    "    \"\"\"\n",
    "    Get technical features from TA-Lib\n",
    "    ref : https://www.kaggle.com/code/daosword/jpx-pytorch-neural-network-with-ta-lib-features\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        op = df['Open']\n",
    "        hi = df['High']\n",
    "        lo = df['Low']\n",
    "        cl = df['AdjustedClose']  #df['Close']\n",
    "        vo = df['Volume']\n",
    "\n",
    "    #     # Overlap Studies\n",
    "    #     df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    #     df['DEMA'] = ta.DEMA(cl, timeperiod=30)\n",
    "        for period in [7,15,30]:\n",
    "            df['EMA{}'.format(period)] = cl.ewm(span=period,  adjust=False).mean().values/cl\n",
    "            df['return{}'.format(period)] = cl.pct_change(period)\n",
    "            df['volatility{}'.format(period)] = np.log(cl).diff().rolling(period).std()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        # df['EMA7'] = ta.EMA(cl, 7)/cl\n",
    "        # df['EMA15'] = ta.EMA(cl, 15)/cl\n",
    "        # df['EMA30'] = ta.EMA(cl, 30)/cl\n",
    "        # df['EMA90'] = ta.EMA(cl, 90)/cl\n",
    "\n",
    "    #     df['HT_TRENDLINE'] = ta.HT_TRENDLINE(cl)\n",
    "    #     df['KAMA'] = ta.KAMA(cl, timeperiod=30)\n",
    "    #     df['MA'] = ta.MA(cl, timeperiod=30, matype=0)\n",
    "    #     df['MIDPOINT'] = ta.MIDPOINT(cl, timeperiod=14)\n",
    "    #     df['SAR'] = ta.SAR(hi, lo, acceleration=0, maximum=0)\n",
    "    #     df['SAREXT'] = ta.SAREXT(hi, lo, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n",
    "    #     df['SMA'] = ta.SMA(cl, timeperiod=30)\n",
    "    #     df['T3'] = ta.T3(df['Close'], timeperiod=5, vfactor=0)\n",
    "    #     df['TEMA'] = ta.TEMA(df['Close'], timeperiod=30)\n",
    "    #     df['TRIMA'] = ta.TRIMA(df['Close'], timeperiod=30)\n",
    "    #     df['WMA'] = ta.WMA(df['Close'], timeperiod=30)\n",
    "\n",
    "        # Momentum Indicators\n",
    "    #     df['ADX'] = ta.ADX(hi, lo, cl, timeperiod=14)\n",
    "    #     df['ADXR'] = ta.ADXR(hi, lo, cl, timeperiod=14)\n",
    "    #     df['APO'] = ta.APO(cl, fastperiod=12, slowperiod=26, matype=0)\n",
    "    #     df['AROON_down'], df['AROON_up'] = ta.AROON(hi, lo, timeperiod=14)\n",
    "    #     df['AROONOSC'] = ta.AROONOSC(hi, lo, timeperiod=14)\n",
    "    #     df['BOP'] = ta.BOP(op, hi, lo, cl)\n",
    "    #     df['CCI'] = ta.CCI(hi, lo, cl, timeperiod=14)\n",
    "    #     df['DX'] = ta.DX(hi, lo, cl, timeperiod=14)\n",
    "    #     df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = ta.MACD(cl, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    #     df['MFI'] = ta.MFI(hi, lo, cl, vo, timeperiod=14)\n",
    "    #     df['MINUS_DI'] = ta.MINUS_DI(hi, lo, cl, timeperiod=14)\n",
    "    #     df['MINUS_DM'] = ta.MINUS_DM(hi, lo, timeperiod=14)\n",
    "        df['MOM'] = ta.MOM(cl, timeperiod=10)\n",
    "    #     df['PLUS_DI'] = ta.PLUS_DI(hi, lo, cl, timeperiod=14)\n",
    "    #     df['PLUS_DM'] = ta.PLUS_DM(hi, lo, timeperiod=14)\n",
    "        df['RSI'] = ta.RSI(cl, timeperiod=14)\n",
    "        df['STOCH_slowk'], df['STOCH_slowd'] = ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "        df['STOCHF_fastk'], df['STOCHF_fastd'] = ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "        df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "    #     df['TRIX'] = ta.TRIX(cl, timeperiod=30)\n",
    "    #     df['ULTOSC'] = ta.ULTOSC(hi, lo, cl, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "    #     df['WILLR'] = ta.WILLR(hi, lo, cl, timeperiod=14)\n",
    "\n",
    "        # Volume Indicators\n",
    "    #     df['AD'] = ta.AD(hi, lo, cl, vo)\n",
    "    #     df['ADOSC'] = ta.ADOSC(hi, lo, cl, vo, fastperiod=3, slowperiod=10)\n",
    "    #     df['OBV'] = ta.OBV(cl, vo)\n",
    "\n",
    "        # Volatility Indicators\n",
    "        df['ATR'] = ta.ATR(hi, lo, cl, timeperiod=14)\n",
    "        df['NATR'] = ta.NATR(hi, lo, cl, timeperiod=14)\n",
    "        df['TRANGE'] = ta.TRANGE(hi, lo, cl)\n",
    "\n",
    "        # Cycle Indicators\n",
    "    #     df['HT_DCPERIOD'] = ta.HT_DCPERIOD(cl)\n",
    "    #     df['HT_DCPHASE'] = ta.HT_DCPHASE(cl)\n",
    "    #     df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = ta.HT_PHASOR(cl)\n",
    "    #     df['HT_SINE_sine'], df['HT_SINE_leadsine'] = ta.HT_SINE(cl)\n",
    "    #     df['HT_TRENDMODE'] = ta.HT_TRENDMODE(cl)\n",
    "\n",
    "        # Statistic Functions\n",
    "    #     df['BETA'] = ta.BETA(hi, lo, timeperiod=5)\n",
    "    #     df['CORREL'] = ta.CORREL(hi, lo, timeperiod=30)\n",
    "    #     df['LINEARREG'] = ta.LINEARREG(cl, timeperiod=14) - cl\n",
    "    #     df['LINEARREG_ANGLE'] = ta.LINEARREG_ANGLE(cl, timeperiod=14)\n",
    "    #     df['LINEARREG_INTERCEPT'] = ta.LINEARREG_INTERCEPT(cl, timeperiod=14) - cl\n",
    "    #     df['LINEARREG_SLOPE'] = ta.LINEARREG_SLOPE(cl, timeperiod=14)\n",
    "        df['STDDEV'] = ta.STDDEV(cl, timeperiod=5, nbdev=1)   \n",
    "    \n",
    "    \n",
    "    else: #inference \n",
    "        op = inf['Open']\n",
    "        hi = inf['High']\n",
    "        lo = inf['Low']\n",
    "        cl = inf['Close']\n",
    "        vo = inf['Volume']\n",
    "        \n",
    "            #     # Overlap Studies\n",
    "    #     df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    #     df['DEMA'] = ta.DEMA(cl, timeperiod=30)\n",
    "\n",
    "        # df['EMA7'] = cl.ewm(span=7, min_periods=3, adjust=False).mean().iloc[-1]/cl\n",
    "        # df['EMA15'] = cl.ewm(span=15, min_periods=7, adjust=False).mean().iloc[-1]/cl\n",
    "        # df['EMA30'] = cl.ewm(span=30, min_periods=15, adjust=False).mean().iloc[-1]/cl\n",
    "        # df['EMA90'] = cl.ewm(span=90, min_periods=30, adjust=False).mean().iloc[-1]/cl\n",
    "\n",
    "        for period in [7,15,30]:\n",
    "            df['EMA{}'.format(period)] = (cl.ewm(span=period,  adjust=False).mean().iloc[-1]/cl).iloc[-1]\n",
    "            df['return{}'.format(period)] = (cl.pct_change(period)).iloc[-1]\n",
    "            df['volatility{}'.format(period)] = (np.log(cl).diff().rolling(period).std()).iloc[-1]\n",
    "\n",
    "    #     df['HT_TRENDLINE'] = ta.HT_TRENDLINE(cl)\n",
    "    #     df['KAMA'] = ta.KAMA(cl, timeperiod=30)\n",
    "    #     df['MA'] = ta.MA(cl, timeperiod=30, matype=0)\n",
    "    #     df['MIDPOINT'] = ta.MIDPOINT(cl, timeperiod=14)\n",
    "    #     df['SAR'] = ta.SAR(hi, lo, acceleration=0, maximum=0)\n",
    "    #     df['SAREXT'] = ta.SAREXT(hi, lo, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n",
    "    #     df['SMA'] = ta.SMA(cl, timeperiod=30)\n",
    "    #     df['T3'] = ta.T3(df['Close'], timeperiod=5, vfactor=0)\n",
    "    #     df['TEMA'] = ta.TEMA(df['Close'], timeperiod=30)\n",
    "    #     df['TRIMA'] = ta.TRIMA(df['Close'], timeperiod=30)\n",
    "    #     df['WMA'] = ta.WMA(df['Close'], timeperiod=30)\n",
    "\n",
    "        # Momentum Indicators\n",
    "    #     df['ADX'] = ta.ADX(hi, lo, cl, timeperiod=14)\n",
    "    #     df['ADXR'] = ta.ADXR(hi, lo, cl, timeperiod=14)\n",
    "    #     df['APO'] = ta.APO(cl, fastperiod=12, slowperiod=26, matype=0)\n",
    "    #     df['AROON_down'], df['AROON_up'] = ta.AROON(hi, lo, timeperiod=14)\n",
    "    #     df['AROONOSC'] = ta.AROONOSC(hi, lo, timeperiod=14)\n",
    "    #     df['BOP'] = ta.BOP(op, hi, lo, cl)\n",
    "    #     df['CCI'] = ta.CCI(hi, lo, cl, timeperiod=14)\n",
    "    #     df['DX'] = ta.DX(hi, lo, cl, timeperiod=14)\n",
    "    #     df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = ta.MACD(cl, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    #     df['MFI'] = ta.MFI(hi, lo, cl, vo, timeperiod=14)\n",
    "    #     df['MINUS_DI'] = ta.MINUS_DI(hi, lo, cl, timeperiod=14)\n",
    "    #     df['MINUS_DM'] = ta.MINUS_DM(hi, lo, timeperiod=14)\n",
    "        df['MOM'] = ta.MOM(cl, timeperiod=10).iloc[-1]\n",
    "    #     df['PLUS_DI'] = ta.PLUS_DI(hi, lo, cl, timeperiod=14)\n",
    "    #     df['PLUS_DM'] = ta.PLUS_DM(hi, lo, timeperiod=14)\n",
    "        df['RSI'] = ta.RSI(cl, timeperiod=14).iloc[-1]\n",
    "        df['STOCH_slowk'], df['STOCH_slowd'] = (ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)[0].iloc[-1],\n",
    "                                                ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)[1].iloc[-1])\n",
    "        df['STOCHF_fastk'], df['STOCHF_fastd'] = (ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)[0].iloc[-1],\n",
    "                                                  ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)[1].iloc[-1])\n",
    "        df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = (ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[0].iloc[-1],\n",
    "                                                      ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[1].iloc[-1])\n",
    "    #     df['TRIX'] = ta.TRIX(cl, timeperiod=30)\n",
    "    #     df['ULTOSC'] = ta.ULTOSC(hi, lo, cl, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "    #     df['WILLR'] = ta.WILLR(hi, lo, cl, timeperiod=14)\n",
    "\n",
    "        # Volume Indicators\n",
    "    #     df['AD'] = ta.AD(hi, lo, cl, vo)\n",
    "    #     df['ADOSC'] = ta.ADOSC(hi, lo, cl, vo, fastperiod=3, slowperiod=10)\n",
    "    #     df['OBV'] = ta.OBV(cl, vo)\n",
    "\n",
    "        # Volatility Indicators\n",
    "        df['ATR'] = ta.ATR(hi, lo, cl, timeperiod=14).iloc[-1]\n",
    "        df['NATR'] = ta.NATR(hi, lo, cl, timeperiod=14).iloc[-1]\n",
    "        df['TRANGE'] = ta.TRANGE(hi, lo, cl).iloc[-1]\n",
    "\n",
    "        # Cycle Indicators\n",
    "    #     df['HT_DCPERIOD'] = ta.HT_DCPERIOD(cl)\n",
    "    #     df['HT_DCPHASE'] = ta.HT_DCPHASE(cl)\n",
    "    #     df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = ta.HT_PHASOR(cl)\n",
    "    #     df['HT_SINE_sine'], df['HT_SINE_leadsine'] = ta.HT_SINE(cl)\n",
    "    #     df['HT_TRENDMODE'] = ta.HT_TRENDMODE(cl)\n",
    "\n",
    "        # Statistic Functions\n",
    "    #     df['BETA'] = ta.BETA(hi, lo, timeperiod=5)\n",
    "    #     df['CORREL'] = ta.CORREL(hi, lo, timeperiod=30)\n",
    "    #     df['LINEARREG'] = ta.LINEARREG(cl, timeperiod=14) - cl\n",
    "    #     df['LINEARREG_ANGLE'] = ta.LINEARREG_ANGLE(cl, timeperiod=14)\n",
    "    #     df['LINEARREG_INTERCEPT'] = ta.LINEARREG_INTERCEPT(cl, timeperiod=14) - cl\n",
    "    #     df['LINEARREG_SLOPE'] = ta.LINEARREG_SLOPE(cl, timeperiod=14)\n",
    "        df['STDDEV'] = ta.STDDEV(cl, timeperiod=5, nbdev=1).iloc[-1]   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ee88776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:09:24.187650Z",
     "iopub.status.busy": "2022-07-03T12:09:24.187207Z",
     "iopub.status.idle": "2022-07-03T12:09:29.637128Z",
     "shell.execute_reply": "2022-07-03T12:09:29.634399Z"
    },
    "papermill": {
     "duration": 5.469005,
     "end_time": "2022-07-03T12:09:29.643903",
     "exception": false,
     "start_time": "2022-07-03T12:09:24.174898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def divideSecurities(df):\n",
    "    sec_list = []\n",
    "#     print('Divide securities individually..')\n",
    "    for code in np.sort(df.SecuritiesCode.unique()):\n",
    "        sec_list.append(df.loc[df.SecuritiesCode == code, :].reset_index(drop=True))\n",
    "    return sec_list\n",
    "\n",
    "sec_list = divideSecurities(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9893b568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:09:29.674223Z",
     "iopub.status.busy": "2022-07-03T12:09:29.673353Z",
     "iopub.status.idle": "2022-07-03T12:09:29.689644Z",
     "shell.execute_reply": "2022-07-03T12:09:29.688258Z"
    },
    "papermill": {
     "duration": 0.033177,
     "end_time": "2022-07-03T12:09:29.692566",
     "exception": false,
     "start_time": "2022-07-03T12:09:29.659389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_features_train(sec_list):\n",
    "    df_list = []\n",
    "    for df in tqdm(sec_list):\n",
    "\n",
    "        # 결측치 채워넣기\n",
    "        df = df.fillna(method='pad')\n",
    "\n",
    "        # shadows\n",
    "        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n",
    "        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n",
    "        \n",
    "        \n",
    "\n",
    "        # adjusted close\n",
    "        df = generate_adjusted_close(df)\n",
    "\n",
    "        ## Rolling features ##\n",
    "        \n",
    "        # lagged features\n",
    "        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n",
    "        # lagged close, target (target 은 정확히 무엇? return인가)\n",
    "        \n",
    "        \n",
    "        # df = df.interpolate()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # All indicators in ta\n",
    "        df = get_ta_features(df.copy())\n",
    "        # try:\n",
    "        #     df = get_ta_features(df.copy())\n",
    "        # except:\n",
    "        #     print(f\"error in SecuritiesCode: {df['SecuritiesCode'][0]}\")\n",
    "        #     display(df)\n",
    "        #     continue\n",
    "                \n",
    "        # not add pattern recognition - feature importance 가 거의 0임. \n",
    "#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n",
    "#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n",
    "\n",
    "\n",
    "        ####  For test!! df['STOCHF_fastd'] df['STOCHRSI_fastk'] df['ATR']\n",
    "        ####  open high low close 등 제거\n",
    "        df = df.drop(columns=['STOCHF_fastd', 'STOCHRSI_fastk', 'STOCH_slowk', 'ATR', \n",
    "                                'Open', 'High', 'Low'])\n",
    "    #     df = df.drop(columns=['upper_shadow', 'Low', 'TRANGE', 'High',\n",
    "    #    'STOCHRSI_fastk', 'STOCHRSI_fastd', 'Open'])\n",
    "        # fill ema features by backward -- 이렇게 채워진 것은 false data 이므로 일단 test 해보고 없애는 것을 검토하자.\n",
    "        # df = df.fillna(method='ffill')\n",
    "        # df = df.interpolate()\n",
    "        \n",
    "        # volatility\n",
    "        \n",
    "        df_list.append(df)\n",
    "        \n",
    "        del df\n",
    "        \n",
    "    # gc.collect()\n",
    "\n",
    "    df_feature_added = pd.concat(df_list).sort_values(['Date','SecuritiesCode'])\n",
    "    \n",
    "    dfc = df_feature_added.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n",
    "    dfc = dfc.dropna()   # 60000 nan rows deleted\n",
    "    \n",
    "    y = dfc.set_index(['Date']).loc[:, 'Target']\n",
    "\n",
    "    dfc = dfc.set_index(['Date']).drop(columns=['Target', 'Close'])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dfc, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2086c83c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:09:29.719384Z",
     "iopub.status.busy": "2022-07-03T12:09:29.718918Z",
     "iopub.status.idle": "2022-07-03T12:10:28.533636Z",
     "shell.execute_reply": "2022-07-03T12:10:28.532130Z"
    },
    "papermill": {
     "duration": 58.831918,
     "end_time": "2022-07-03T12:10:28.536346",
     "exception": false,
     "start_time": "2022-07-03T12:09:29.704428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:53<00:00, 37.54it/s]\n"
     ]
    }
   ],
   "source": [
    "df_added, y = add_features_train(sec_list)\n",
    "# df_added = df_added[(df_added['Date'] >= '2021-06-03') & (df_added['Date'] <= '2021-12-03')]\n",
    "# df_added = df_added[df_added['Date'] >= '2021-05-27']\n",
    "# df_added.to_csv('train_with_supp_feature_added.csv', index=False)\n",
    "\n",
    "# load data\n",
    "# df_added = pd.read_csv('/kaggle/input/train-with-supp-feature-added-v1-all-cdl/train_with_supp_feature_added.csv')\n",
    "\n",
    "# df_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6affbb18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:10:28.638477Z",
     "iopub.status.busy": "2022-07-03T12:10:28.637936Z",
     "iopub.status.idle": "2022-07-03T12:10:28.654806Z",
     "shell.execute_reply": "2022-07-03T12:10:28.653204Z"
    },
    "papermill": {
     "duration": 0.070722,
     "end_time": "2022-07-03T12:10:28.657660",
     "exception": false,
     "start_time": "2022-07-03T12:10:28.586938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_features_infer(input_df, close_df): #input df 는 price 데이터\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    print('Divide input securities...')\n",
    "    sec_list = divideSecurities(input_df)\n",
    "    print('Divide close_df securities...')\n",
    "    close_list = divideSecurities(close_df) # for rolling features\n",
    "    print('='*10 + 'feature adding' + '='*10)\n",
    "    \n",
    "    assert len(sec_list) == len(close_list)\n",
    "    \n",
    "    for i in range(len(sec_list)):\n",
    "        \n",
    "        \n",
    "        close = close_list[i] #.loc[close_df.SecuritiesCode == code, :].fillna(method='ffill')\n",
    "        df = sec_list[i]\n",
    "#         # test data의 open, high, low, close 중 nan 있으면 이전 값에서 가져와 채움\n",
    "        if df.loc[:, ['Open', 'High', 'Low', 'Close']].isna().any().any():\n",
    "            print('NaN detected..')\n",
    "            display(df)\n",
    "#             display(close)\n",
    "# #             print(close.iloc[-2]['Open', 'High', 'Low', 'Close'].values())\n",
    "            df.loc[:, ['Open', 'High', 'Low', 'Close', 'Volume']] = close.loc[close['Date'] == close.iloc[-2]['Date'], ['Open', 'High', 'Low', 'Close', 'Volume']].values \n",
    "            print('NaN filled...')\n",
    "            display(df)\n",
    "        \n",
    "\n",
    "        \n",
    "        # shadows\n",
    "        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n",
    "        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n",
    "\n",
    "        # lagged features\n",
    "        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n",
    "        # lagged close, target (target 은 정확히 무엇? return인가)\n",
    "        \n",
    "        # adjusted close\n",
    "        df = generate_adjusted_close(df)\n",
    "        \n",
    "        ## Rolling features ##\n",
    "        # TA-lib features\n",
    "        df = get_ta_features(df, close, train=False)\n",
    "        \n",
    "        ####  open high low close 등 제거\n",
    "        df = df.drop(columns=['STOCHF_fastd', 'STOCHRSI_fastk', 'STOCH_slowk', 'NATR', 'ATR'])\n",
    "        \n",
    "#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n",
    "#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n",
    "\n",
    "\n",
    "        # volatility\n",
    "        \n",
    "        df_list.append(df)\n",
    "        del df\n",
    "    \n",
    "    df_feature_added = pd.concat(df_list)\n",
    "    \n",
    "    return df_feature_added\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc6d8b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:10:28.758446Z",
     "iopub.status.busy": "2022-07-03T12:10:28.758092Z",
     "iopub.status.idle": "2022-07-03T12:10:28.768774Z",
     "shell.execute_reply": "2022-07-03T12:10:28.768012Z"
    },
    "papermill": {
     "duration": 0.063973,
     "end_time": "2022-07-03T12:10:28.771282",
     "exception": false,
     "start_time": "2022-07-03T12:10:28.707309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_inference(df, trained_scalers: list):\n",
    "    ordinal = trained_scalers[0]\n",
    "    stdsc = trained_scalers[1]\n",
    "    \n",
    "      \n",
    "    # remove columns - Date removed temporarily\n",
    "    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n",
    "    \n",
    "    \n",
    "    target = ['Target']\n",
    "    ord_features = ['SecuritiesCode'] \n",
    "    scaled_features = [i for i in df.columns if i not in ['Date', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag',\n",
    "                                                         'Target', 'SecuritiesCode']]\n",
    "#     print('scaled_features:', scaled_features)\n",
    "    date_code_ord = ordinal.transform(dfc.loc[:,ord_features])\n",
    "    scaled = stdsc.transform(dfc.loc[:,scaled_features])\n",
    "    \n",
    "\n",
    "\n",
    "    dfc_scaled = pd.concat([df['Date'].reset_index(drop=True),\n",
    "                            pd.DataFrame(date_code_ord, columns=ord_features),\n",
    "                            pd.DataFrame(scaled, columns=scaled_features)], axis=1)\n",
    "\n",
    "    dfc_scaled = dfc_scaled.set_index(['Date']).drop(columns=['Close'])\n",
    "\n",
    "    \n",
    "\n",
    "#     y = dfc.loc[:, ['Target']]\n",
    "    \n",
    "    return dfc_scaled\n",
    "\n",
    "\n",
    "# X_test_scaled = preprocess_train(df_added, trained_scalers)\n",
    "\n",
    "# X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bcc1077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:10:28.874028Z",
     "iopub.status.busy": "2022-07-03T12:10:28.873458Z",
     "iopub.status.idle": "2022-07-03T12:10:29.610428Z",
     "shell.execute_reply": "2022-07-03T12:10:29.608995Z"
    },
    "papermill": {
     "duration": 0.791955,
     "end_time": "2022-07-03T12:10:29.613342",
     "exception": false,
     "start_time": "2022-07-03T12:10:28.821387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaled\n",
    "stdsc = StandardScaler()\n",
    "scaled_features = [i for i in df_added.columns if i not in ['SecuritiesCode', '33SectorCode']]\n",
    "scaled = stdsc.fit_transform(df_added.loc[:,scaled_features])\n",
    "df_added_scaled =  pd.concat([\n",
    "                            df_added.loc[:,['SecuritiesCode','33SectorCode']].reset_index(),\n",
    "                            pd.DataFrame(scaled, columns=scaled_features)\n",
    "                            ], axis=1).set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e7fb119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:10:29.715000Z",
     "iopub.status.busy": "2022-07-03T12:10:29.714422Z",
     "iopub.status.idle": "2022-07-03T12:10:29.718659Z",
     "shell.execute_reply": "2022-07-03T12:10:29.718102Z"
    },
    "papermill": {
     "duration": 0.057876,
     "end_time": "2022-07-03T12:10:29.720824",
     "exception": false,
     "start_time": "2022-07-03T12:10:29.662948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# maxsr_study = joblib.load('../models/lgbm_fin_maxSR_10trial_stationary_scaled_train130_test60_fold8.pkl')\n",
    "maxsr_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.001434850342256775,\n",
    "    'num_leaves': 226,\n",
    "    'max_depth': 5,\n",
    "    'min_data_in_leaf': 4900,\n",
    "    'lambda_l1': 0.5144342052290969,\n",
    "    'lambda_l2': 0.0016086108752855683,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'feature_fraction': 0.7,\n",
    "    'max_bin': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a155467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:10:29.824775Z",
     "iopub.status.busy": "2022-07-03T12:10:29.824157Z",
     "iopub.status.idle": "2022-07-03T12:11:26.794087Z",
     "shell.execute_reply": "2022-07-03T12:11:26.793039Z"
    },
    "papermill": {
     "duration": 57.025607,
     "end_time": "2022-07-03T12:11:26.797124",
     "exception": false,
     "start_time": "2022-07-03T12:10:29.771517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Training fold 1========\n",
      "training size: 130   test size: 60\n",
      "Train Date range: 2017-06-02 to 2017-12-08\n",
      "Valid Date range: 2017-12-11 to 2018-03-09\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5144342052290969, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5144342052290969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0016086108752855683, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0016086108752855683\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4900\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 0.0234228\tvalid_0's l2: 0.000548627\n",
      "Fold 1 evaluated in 11.741s\n",
      "Fold 1 sharpe ratio score: 0.351639\n",
      "R2-score: -0.001244\n",
      "========Training fold 2========\n",
      "training size: 130   test size: 60\n",
      "Train Date range: 2017-12-11 to 2018-06-21\n",
      "Valid Date range: 2018-06-22 to 2018-09-14\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5144342052290969, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5144342052290969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0016086108752855683, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0016086108752855683\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4900\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's rmse: 0.0221923\tvalid_0's l2: 0.000492496\n",
      "Fold 2 evaluated in 6.978s\n",
      "Fold 2 sharpe ratio score: 0.208670\n",
      "R2-score: 0.000424\n",
      "========Training fold 3========\n",
      "training size: 130   test size: 60\n",
      "Train Date range: 2018-06-22 to 2018-12-28\n",
      "Valid Date range: 2019-01-04 to 2019-04-02\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5144342052290969, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5144342052290969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0016086108752855683, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0016086108752855683\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4900\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's rmse: 0.0230182\tvalid_0's l2: 0.000529838\n",
      "Fold 3 evaluated in 1.901s\n",
      "Fold 3 sharpe ratio score: 0.089280\n",
      "R2-score: -0.012854\n",
      "========Training fold 4========\n",
      "training size: 130   test size: 60\n",
      "Train Date range: 2019-01-04 to 2019-07-18\n",
      "Valid Date range: 2019-07-19 to 2019-10-16\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5144342052290969, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5144342052290969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0016086108752855683, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0016086108752855683\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4900\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmse: 0.0203282\tvalid_0's l2: 0.000413235\n",
      "Fold 4 evaluated in 2.968s\n",
      "Fold 4 sharpe ratio score: 0.095652\n",
      "R2-score: -0.000108\n",
      "========Training fold 5========\n",
      "training size: 130   test size: 60\n",
      "Train Date range: 2019-07-19 to 2020-01-31\n",
      "Valid Date range: 2020-02-03 to 2020-04-30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5144342052290969, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5144342052290969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0016086108752855683, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0016086108752855683\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4900\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 0.0383746\tvalid_0's l2: 0.00147261\n",
      "Fold 5 evaluated in 11.481s\n",
      "Fold 5 sharpe ratio score: -0.134147\n",
      "R2-score: -0.004969\n",
      "========Training fold 6========\n",
      "training size: 130   test size: 60\n",
      "Train Date range: 2020-02-03 to 2020-08-14\n",
      "Valid Date range: 2020-08-17 to 2020-11-11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5144342052290969, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5144342052290969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0016086108752855683, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0016086108752855683\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4900\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's rmse: 0.0241329\tvalid_0's l2: 0.000582397\n",
      "Fold 6 evaluated in 2.527s\n",
      "Fold 6 sharpe ratio score: 0.095740\n",
      "R2-score: -0.002839\n",
      "========Training fold 7========\n",
      "training size: 130   test size: 60\n",
      "Train Date range: 2020-08-17 to 2021-02-25\n",
      "Valid Date range: 2021-02-26 to 2021-05-26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5144342052290969, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5144342052290969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0016086108752855683, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0016086108752855683\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4900\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 0.0217877\tvalid_0's l2: 0.000474705\n",
      "Fold 7 evaluated in 11.747s\n",
      "Fold 7 sharpe ratio score: 0.242578\n",
      "R2-score: -0.000578\n",
      "========Training fold 8========\n",
      "training size: 130   test size: 60\n",
      "Train Date range: 2021-02-26 to 2021-09-06\n",
      "Valid Date range: 2021-09-07 to 2021-12-03\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5144342052290969, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5144342052290969\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0016086108752855683, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0016086108752855683\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4900, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4900\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[239]\tvalid_0's rmse: 0.0228139\tvalid_0's l2: 0.000520475\n",
      "Fold 8 evaluated in 7.242s\n",
      "Fold 8 sharpe ratio score: 0.206314\n",
      "R2-score: -0.004154\n",
      "cv mean sharpe ratio score: 0.144466\n",
      "cv STD sharpe ratio score: 0.135056\n",
      "cv mean r2 score: -0.003290\n"
     ]
    }
   ],
   "source": [
    "# base model - lgbm - only 130 days for training\n",
    "# test - time-series cv 5-fold ensemble\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models: list):\n",
    "        self.models = models\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predicted = np.zeros((len(x), len(self.models)))\n",
    "        for i, model in enumerate(self.models):\n",
    "            predicted[:, i] = model.predict(x)\n",
    "        return np.sum(predicted, axis=1) / len(self.models)\n",
    "        \n",
    "def group_timeseriesCV(grid, train_size=210, test_size=60, fold=5, total=False):  # overlap last test and train\n",
    "    # last cv should be end the previous day of eval start  -  마지막 cv eval이 test data 직전에 끝나게 설정\n",
    "    # cv_size = train_size + test_size\n",
    "    if total:\n",
    "        cv_train = grid[:-test_size]\n",
    "        cv_test = grid[-test_size:]\n",
    "        yield cv_train, cv_test\n",
    "    else:\n",
    "        for i in range(fold):\n",
    "            cv_train = grid[-(train_size + test_size + train_size*(fold-i-1)):-(test_size + train_size*(fold-i-1))]\n",
    "            test_end = -train_size*(fold-i-1) if i < fold-1 else None\n",
    "            cv_test = grid[-(test_size + train_size*(fold-i-1)): test_end]              \n",
    "            yield cv_train, cv_test\n",
    "\n",
    "\n",
    "# cates = ['SecuritiesCode', '33SectorCode']  # categorical columns\n",
    "\n",
    "grid = df_added.index.unique() \n",
    "# print(min(grid), max(grid))\n",
    "# tscv = TimeSeriesSplit(n_splits=7, test_size=60)\n",
    "\n",
    "k = 0\n",
    "feat_importance = pd.DataFrame()\n",
    "models = []\n",
    "cv_score = []\n",
    "r2_list = []\n",
    "\n",
    "for train_idx, eval_idx in group_timeseriesCV(grid, 130, 60, fold=8): #  #\n",
    "\n",
    "# for train_idx, eval_idx in tscv.split(grid):\n",
    "    # train_idx = grid[train_idx]\n",
    "    # eval_idx = grid[eval_idx]\n",
    "\n",
    "    k += 1\n",
    "    t0 = time.time()\n",
    "    print(f'========Training fold {k}========')\n",
    "   \n",
    "    print('training size:', len(train_idx), '  test size:', len(eval_idx))\n",
    "    \n",
    "    print(\"Train Date range: {} to {}\".format(train_idx.min(),train_idx.max()))\n",
    "    print(\"Valid Date range: {} to {}\".format(eval_idx.min(),eval_idx.max()))\n",
    "    \n",
    "\n",
    "    X, y = df_added_scaled.copy(), y.copy()\n",
    "    # X, y = X_lgb.copy(), y.copy()\n",
    "\n",
    "    X_train, X_test = X.loc[train_idx].reset_index(drop=True), X.loc[eval_idx].reset_index(drop=True)\n",
    "    y_train, y_test = y.loc[train_idx].reset_index(drop=True), y.loc[eval_idx].reset_index(drop=True)\n",
    "\n",
    "    # handpick_params = {\n",
    "    #     'n_estimators': 2000,\n",
    "    #     'learning_rate': 0.001,\n",
    "    #     'num_leaves': 109,\n",
    "    #     'max_depth': 10,\n",
    "    #     'min_data_in_leaf': 2400,\n",
    "    #     'bagging_fraction': 0.5,\n",
    "    #     'feature_fraction': 0.2,\n",
    "    #     'max_bin': 106\n",
    "    #         }\n",
    "    # optuna_params = {\n",
    "    #     'lambda_l1': 0.00037684382460392624,\n",
    "    #     'lambda_l2': 0.00751489594500413,\n",
    "    #         }\n",
    "\n",
    "    # lgb = LGBMRegressor(**handpick_params, **optuna_params, seed=2022)#, categorical_feature='name:SecuritiesCode,33SectorCode')\n",
    "    \n",
    "    lgb = LGBMRegressor(**maxsr_params, seed=2022)\n",
    "    lgb.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set = [(X_test, y_test)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[\n",
    "                early_stopping(stopping_rounds=20)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    models.append(lgb)\n",
    "\n",
    "    # feat_importance[\"Importance_Fold\"+str(k)]=lgb.feature_importances_\n",
    "    # feat_importance.set_index(X_train.columns, inplace=True)\n",
    "\n",
    "    # y_pred = lgb.predict(X_test)\n",
    "    y_pred = lgb.predict(X_test)\n",
    "    \n",
    "    # Sharpe Ratio \n",
    "    df_sharpe = pd.DataFrame([])\n",
    "    df_sharpe['Date'] = X.loc[eval_idx].index\n",
    "    df_sharpe['predict'] = y_pred\n",
    "    df_sharpe['Target'] = y_test\n",
    "    df_sharpe['Rank'] = (df_sharpe.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n",
    "#         display(X_test.groupby('Date')['Rank'].min())\n",
    "#         display(X_test.groupby('Date')['Rank'].max())\n",
    "#         display(X_test)\n",
    "    eval_score = calc_spread_return_sharpe(df_sharpe)\n",
    "    cv_score.append(eval_score)\n",
    "\n",
    "    print(f'Fold {k} evaluated in {time.time() - t0 :.3f}s')\n",
    "    print(f'Fold {k} sharpe ratio score: {eval_score :.6f}')\n",
    "    \n",
    "    r2_list.append(r2_score(y_test.values, y_pred))\n",
    "    print('R2-score: {:.6f}'.format(r2_score(y_test.values, y_pred)))\n",
    "\n",
    "print(f'cv mean sharpe ratio score: {np.mean(cv_score) :.6f}')\n",
    "print(f'cv STD sharpe ratio score: {np.std(cv_score) :.6f}')\n",
    "print(f'cv mean r2 score: {np.mean(r2_list) :.6f}')\n",
    "# score_type = 'scaled_6fold_basicfeatonly'\n",
    "# if score_type in score_board.keys():\n",
    "#     raise ValueError('key already exists')\n",
    "# else:\n",
    "#     score_board[score_type] = np.mean(cv_score)\n",
    "\n",
    "# lgb = LGBMRegressor(device='GPU',**params).fit(X_scaled, y)\n",
    "\n",
    "\n",
    "\n",
    "# feat_importance['avg'] = feat_importance.mean(axis=1)\n",
    "# feat_importance = feat_importance.sort_values(by='avg',ascending=True)\n",
    "# display(feat_importance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70649f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:11:26.903834Z",
     "iopub.status.busy": "2022-07-03T12:11:26.903486Z",
     "iopub.status.idle": "2022-07-03T12:11:26.908232Z",
     "shell.execute_reply": "2022-07-03T12:11:26.906742Z"
    },
    "papermill": {
     "duration": 0.059884,
     "end_time": "2022-07-03T12:11:26.910306",
     "exception": false,
     "start_time": "2022-07-03T12:11:26.850422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensemble the cv models\n",
    "ensemble = EnsembleModel(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe564c5",
   "metadata": {
    "papermill": {
     "duration": 0.051703,
     "end_time": "2022-07-03T12:11:27.014204",
     "exception": false,
     "start_time": "2022-07-03T12:11:26.962501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### LGBM Fitting result log\n",
    "\n",
    "1. ~30 window, tscv, 7-fold, stationary+scaled  \n",
    "cv mean sharpe ratio score: 0.069140  \n",
    "cv STD sharpe ratio score: 0.094025  \n",
    "cv mean r2 score: -0.004279   \n",
    "\n",
    "\n",
    "1. <mark> **[Selected for Optuna HP tuning]** ~30 window, 130 days, 8-fold, stationary features + scaled    \n",
    "cv mean sharpe ratio score: 0.123441  \n",
    "cv STD sharpe ratio score: 0.109005  \n",
    "cv mean r2 score: -0.003533</mark>\n",
    "\n",
    "1. <mark> **[Optuna HP tuned]** ~30 window, 130 days, 8-fold, stationary features + scaled    \n",
    "cv mean sharpe ratio score: 0.139032  \n",
    "cv STD sharpe ratio score: 0.146264  \n",
    "cv mean r2 score: -0.003247  \n",
    "\n",
    "\n",
    "1. ~30 window, 130 days, 8-fold, features scaled except categorical   \n",
    "cv mean sharpe ratio score: 0.128036  \n",
    "cv STD sharpe ratio score: 0.144154  \n",
    "cv mean r2 score: -0.003573  \n",
    "\n",
    "1. ~30 window, all days + 2022-03~05 test with restored features and ensemble model(~2022-03)  \n",
    "cv mean sharpe ratio score: 0.022831  \n",
    "cv STD sharpe ratio score: 0.000000  \n",
    "cv mean r2 score: -0.000224\n",
    "\n",
    "\n",
    "1. ~30 window, 130 days, + test period all (2022-05), removed features restored , ensemble model(~2022-03)  \n",
    "cv mean sharpe ratio score: 0.241770  \n",
    "cv STD sharpe ratio score: 0.155396  \n",
    "cv mean r2 score: -0.001687  \n",
    "\n",
    "1. ~30 window, 130 days, + test period, removed features restored , ensemble model   \n",
    "cv mean sharpe ratio score: 0.232070   \n",
    "cv STD sharpe ratio score: 0.245231   \n",
    "cv mean r2 score: -0.003154  \n",
    "\n",
    "1. ~30 window, 130 days, + test period, removed features restored  \n",
    "cv mean sharpe ratio score: 0.089397  \n",
    "cv STD sharpe ratio score: 0.155554  \n",
    "cv mean r2 score: -0.003571  \n",
    "\n",
    "1. ~30 window, 130 days + test period  \n",
    "cv mean sharpe ratio score: -0.012974  \n",
    "cv STD sharpe ratio score: 0.172770  \n",
    "cv mean r2 score: -0.003650  \n",
    "\n",
    "\n",
    "\n",
    "1. ~30 window, 210 days train    \n",
    "cv mean sharpe ratio score: 0.004027  \n",
    "cv STD sharpe ratio score: 0.116113   \n",
    "cv mean r2 score: -0.012231  \n",
    "\n",
    "2. ~30 window, total train (1112 days)  \n",
    "cv mean sharpe ratio score: -0.014474  \n",
    "cv STD sharpe ratio score: 0.000000  \n",
    "cv mean r2 score: -0.002762\n",
    "\n",
    "\n",
    "3. ~30 window, 130 days train (cv: 8)  \n",
    "cv mean sharpe ratio score: 0.060078  \n",
    "cv STD sharpe ratio score: 0.152693  \n",
    "cv mean r2 score: -0.003789  \n",
    "\n",
    "\n",
    "2. ~90 window, 210 days train  \n",
    "cv mean sharpe ratio score: 0.024323  \n",
    "mean R2-score: -0.012523      \n",
    " \n",
    "3. ~90 window, 350 days train  \n",
    "cv mean sharpe ratio score: 0.055422  \n",
    "cv STD sharpe ratio score: 0.033707  \n",
    "cv mean r2 score: -0.008804  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c8d1ab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:11:27.130233Z",
     "iopub.status.busy": "2022-07-03T12:11:27.129617Z",
     "iopub.status.idle": "2022-07-03T12:11:27.134070Z",
     "shell.execute_reply": "2022-07-03T12:11:27.133363Z"
    },
    "papermill": {
     "duration": 0.064116,
     "end_time": "2022-07-03T12:11:27.136654",
     "exception": false,
     "start_time": "2022-07-03T12:11:27.072538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # feat importance plot\n",
    "# feat_importance['avg'] = feat_importance.mean(axis=1)\n",
    "# feat_importance = feat_importance.sort_values(by='avg',ascending=True)\n",
    "# pal=sns.color_palette(\"plasma_r\", 40).as_hex()[2:]\n",
    "\n",
    "\n",
    "# fig=go.Figure()\n",
    "# for i in range(len(feat_importance.index)):\n",
    "#     fig.add_shape(dict(type=\"line\", y0=i, y1=i, x0=0, x1=feat_importance['avg'][i], \n",
    "#                        line_color=pal[::-1][i],opacity=0.7,line_width=4))\n",
    "# fig.add_trace(go.Scatter(x=feat_importance['avg'], y=feat_importance.index, mode='markers', \n",
    "#                          marker_color=pal[::-1], marker_size=8,\n",
    "#                          hovertemplate='%{y} Importance = %{x:.0f}<extra></extra>'))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd474d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:11:27.243853Z",
     "iopub.status.busy": "2022-07-03T12:11:27.243212Z",
     "iopub.status.idle": "2022-07-03T12:11:27.252587Z",
     "shell.execute_reply": "2022-07-03T12:11:27.251864Z"
    },
    "papermill": {
     "duration": 0.065912,
     "end_time": "2022-07-03T12:11:27.254919",
     "exception": false,
     "start_time": "2022-07-03T12:11:27.189007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # only using average cv score\n",
    "\n",
    "# def objective(trial, df, y, train_size, test_size, fold):  # X_scaled, X_scaled_val, y, y_val\n",
    "    \n",
    "#     global count\n",
    "#     print('')\n",
    "#     print('========= Optuna HyperParameter Tuning trial {} =========='.format(count))\n",
    "#     print('')\n",
    "#     param_grid = {\n",
    "#         # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "#         'objective': 'regression',\n",
    "#         \"seed\": 2022,\n",
    "\n",
    "#         \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [500, 1000, 2000, 4000, 5000, 10000]),\n",
    "#         \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 5e-4, 0.5),\n",
    "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n",
    "#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "# #         \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "#         \"bagging_fraction\": trial.suggest_float(\n",
    "#             \"bagging_fraction\", 0.2, 0.9, step=0.1\n",
    "#         ),\n",
    "# #         \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "#         \"feature_fraction\": trial.suggest_float(\n",
    "#             \"feature_fraction\", 0.2, 0.9, step=0.1\n",
    "#         ),\n",
    "        \n",
    "#         'max_bin': trial.suggest_int('max_bin', 15, 256)\n",
    "        \n",
    "#     }\n",
    "\n",
    "#     # tscv = TimeSeriesSplit(n_splits=k, test_size=60)\n",
    "\n",
    "#     cv_score = []\n",
    "#     cv_sharpe = []\n",
    "#     grid = df.index.unique() \n",
    "#     feat_importance = pd.DataFrame([])\n",
    "    \n",
    "#     for i, (train_idx, eval_idx) in enumerate(group_timeseriesCV(grid, train_size, test_size, fold)):\n",
    "#     # for i, (train_idx, eval_idx) in enumerate(tscv.split(grid)):\n",
    "#     #     train_idx = grid[train_idx]\n",
    "#     #     eval_idx = grid[eval_idx]\n",
    "\n",
    "        \n",
    "#         t0 = time.time()\n",
    "#         print(f'========Training fold {i+1}========')\n",
    "\n",
    "#         print('training size:', len(train_idx), '  test size:', len(eval_idx))\n",
    "\n",
    "#         print(\"Train Date range: {} to {}\".format(train_idx.min(),train_idx.max()))\n",
    "#         print(\"Valid Date range: {} to {}\".format(eval_idx.min(),eval_idx.max()))\n",
    "\n",
    "\n",
    "#         X, y = df_added_all_scaled.copy(), y_all.copy()\n",
    "#         # X, y = X_lgb.copy(), y.copy()\n",
    "\n",
    "#         X_train, X_test = X.loc[train_idx].reset_index(drop=True), X.loc[eval_idx].reset_index(drop=True)\n",
    "#         y_train, y_test = y.loc[train_idx].reset_index(drop=True), y.loc[eval_idx].reset_index(drop=True)\n",
    "\n",
    "        \n",
    "#         lgb = LGBMRegressor(**param_grid)\n",
    "#         lgb.fit(\n",
    "#             X_train, \n",
    "#             y_train,\n",
    "#             eval_set = [(X_test, y_test)],\n",
    "#             eval_metric='rmse',\n",
    "#             callbacks=[\n",
    "#                     early_stopping(stopping_rounds=20)\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "#         feat_importance[\"Importance_Fold\"+str(i+1)]=lgb.feature_importances_\n",
    "#         feat_importance.set_index(X_train.columns, inplace=True)\n",
    "\n",
    "#         y_pred = lgb.predict(X_test)\n",
    "\n",
    "        \n",
    "#         # Sharpe Ratio \n",
    "#         df_sharpe = pd.DataFrame([])\n",
    "#         df_sharpe['Date'] = X.loc[eval_idx].index\n",
    "#         df_sharpe['predict'] = y_pred\n",
    "#         df_sharpe['Target'] = y_test\n",
    "#         df_sharpe['Rank'] = (df_sharpe.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n",
    "#     #         display(X_test.groupby('Date')['Rank'].min())\n",
    "#     #         display(X_test.groupby('Date')['Rank'].max())\n",
    "#     #         display(X_test)\n",
    "#         sharpe = calc_spread_return_sharpe(df_sharpe)\n",
    "        \n",
    "#         cv_sharpe.append(sharpe)\n",
    "\n",
    "#         print(f'Fold {i+1} evaluated in {time.time() - t0 :.3f}s')\n",
    "#         print(f'Fold {i+1} sharpe ratio score: {sharpe :.6f}')\n",
    "\n",
    "\n",
    "        \n",
    "#         assert len(y_test) == len(y_pred)\n",
    "#         assert y_test.isna().any() == False\n",
    "        \n",
    "#         # MSE as objective \n",
    "#         cv_score.append(mean_squared_error(y_test.values, y_pred))\n",
    "        \n",
    "#     print('cv completed with mse scores:', cv_score)\n",
    "#     print('cv mean mse score: {:.6f}'.format(np.mean(cv_score)))\n",
    "#     print('cv mean sharp score: {:.6f}'.format(np.mean(cv_sharpe)))\n",
    "#     print('cv STD sharp score: {:.6f}'.format(np.std(cv_sharpe)))\n",
    "#     count += 1\n",
    "\n",
    "#     feat_importance['avg'] = feat_importance.mean(axis=1)\n",
    "#     feat_importance = feat_importance.sort_values(by='avg',ascending=True)\n",
    "#     display(feat_importance)\n",
    "    \n",
    "#     return np.mean(cv_score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9af29a37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:11:27.361411Z",
     "iopub.status.busy": "2022-07-03T12:11:27.360686Z",
     "iopub.status.idle": "2022-07-03T12:11:27.365695Z",
     "shell.execute_reply": "2022-07-03T12:11:27.364695Z"
    },
    "papermill": {
     "duration": 0.061284,
     "end_time": "2022-07-03T12:11:27.368121",
     "exception": false,
     "start_time": "2022-07-03T12:11:27.306837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count = 1\n",
    "\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Regressor\")\n",
    "# func = lambda trial: objective(trial, df_added, y, train_size=130, test_size=60, fold=8)\n",
    "# study.optimize(func, n_trials=10)\n",
    "\n",
    "# joblib.dump(study, '../models/lgbm_fin_10trial_stationary_scaled_train130_test60_fold8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc7fb944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:11:27.473417Z",
     "iopub.status.busy": "2022-07-03T12:11:27.473041Z",
     "iopub.status.idle": "2022-07-03T12:11:27.479037Z",
     "shell.execute_reply": "2022-07-03T12:11:27.477990Z"
    },
    "papermill": {
     "duration": 0.061338,
     "end_time": "2022-07-03T12:11:27.481221",
     "exception": false,
     "start_time": "2022-07-03T12:11:27.419883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lgb = LGBMRegressor(**maxsr_params, seed=2022)\n",
    "# lgb.fit(\n",
    "#     df_added.loc[grid],   #only recent 130 days fit to predict next 60 days\n",
    "#     y.loc[grid],\n",
    "#     # eval_set = [(X_test, y_test)],\n",
    "#     # callbacks=[\n",
    "#             # early_stopping(stopping_rounds=20)\n",
    "#         # ]\n",
    "#     )\n",
    "\n",
    "\n",
    "# evaluate 3-month  12-06 ~ 3월\n",
    "# sgrid = supp_data['Date'].unique()[:2]\n",
    "# print(min(sgrid), max(sgrid))\n",
    "\n",
    "# biggrid = train_with_supp['Date'].unique()[-300:] #last 300 days\n",
    "# print(min(biggrid), max(biggrid))\n",
    "\n",
    "# # train_with_supp\n",
    "# ts_divsec = divideSecurities(train_with_supp.set_index('Date').loc[biggrid].reset_index())\n",
    "# feat_supp, _ = add_features_train(ts_divsec)\n",
    "# X_supp = feat_supp.loc[sgrid] #preprocess_inference(feat_supp.set_index('Date').loc[sgrid].reset_index(), trained_scalers)\n",
    "\n",
    "# y_test = train_with_supp.set_index('Date').loc[sgrid, ['Target']].reset_index(drop=True)\n",
    "# y_pred = ensemble.predict(X_supp)\n",
    "\n",
    "# print('supp data evaluation with r2-score : {:.5f}'.format(r2_score(y_test.values, y_pred)))\n",
    "\n",
    "# df_sharpe = pd.DataFrame([])\n",
    "# df_sharpe['Date'] = X_supp.index\n",
    "# df_sharpe['SecuritiesCode'] = X_supp.SecuritiesCode.reset_index(drop=True)\n",
    "# df_sharpe['predict'] = y_pred\n",
    "# df_sharpe['Target'] = y_test\n",
    "# df_sharpe['Rank'] = (df_sharpe.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n",
    "# eval_score = calc_spread_return_sharpe(df_sharpe)\n",
    "\n",
    "# print('sharpe ratio score : {}'.format(eval_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6e865",
   "metadata": {
    "papermill": {
     "duration": 0.052535,
     "end_time": "2022-07-03T12:11:27.585582",
     "exception": false,
     "start_time": "2022-07-03T12:11:27.533047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### score board\n",
    "| number | features used | len_train, len_eval, cv | trial | train_score | test_score | remarks |  \n",
    "| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n",
    "| 1 | rolling(not scaled) | 210, 60, 5 | 50 | R2:-0.001238 SR:0.110312 | R2: -0.01851 SR: -0.182878 |  |\n",
    "| 2 | rolling(not scaled)- 30 rolling | 130, 60, 8 (also 130 only for test) | 10 | R2:-0.003075 SR:0.127686 | R2: -0.01352 SR: -0.216620 |  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5342dd9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:11:27.693376Z",
     "iopub.status.busy": "2022-07-03T12:11:27.692403Z",
     "iopub.status.idle": "2022-07-03T12:11:27.701753Z",
     "shell.execute_reply": "2022-07-03T12:11:27.700704Z"
    },
    "papermill": {
     "duration": 0.066378,
     "end_time": "2022-07-03T12:11:27.704624",
     "exception": false,
     "start_time": "2022-07-03T12:11:27.638246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # local-api\n",
    "# sys.path.insert(0, '../input/jpx-local-api')\n",
    "# from local_api import local_api\n",
    "\n",
    "# seccodes = np.sort(train['SecuritiesCode'].unique())\n",
    "# main_list = stock_list.set_index('SecuritiesCode').loc[seccodes, ['33SectorCode']]\n",
    "\n",
    "# myapi = local_api('../input/jpx-tokyo-stock-exchange-prediction/supplemental_files', \n",
    "#                  start_date='2021-12-06', end_date='2021-12-07') # 3-month\n",
    "# env = myapi.make_env()\n",
    "# iter_test = env.iter_test()\n",
    "# for i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(tqdm(iter_test)):\n",
    "# #     t0 = time.time()\n",
    "    \n",
    "#     # 이전 데이터와 합치고 최근 140일치만 이용한다\n",
    "#     today = prices.iloc[0]['Date']\n",
    "#     lastday = str(pd.to_datetime(today) - pd.DateOffset(200))\n",
    "    \n",
    "#     # add 33SectorCode\n",
    "#     prices = prices.set_index('SecuritiesCode').join(main_list).reset_index().astype({\n",
    "#         'SecuritiesCode':'category',\n",
    "#         '33SectorCode':'category'})\n",
    "\n",
    "#     if i == 0:\n",
    "#         close_df = pd.concat([\n",
    "#             train_with_supp.loc[\n",
    "#                 (train_with_supp['Date'] > lastday) & (train_with_supp['Date'] < today)],\n",
    "#             prices\n",
    "#             ]).reset_index(drop=True)\n",
    "#     else:\n",
    "#         close_df = pd.concat([\n",
    "#             close_df.loc[\n",
    "#                 (close_df['Date'] > lastday) & (close_df['Date'] < today)],\n",
    "#             prices\n",
    "#             ]).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "\n",
    "#     sec_list_infer = divideSecurities(close_df)\n",
    "#     X, _ = add_features_train(sec_list_infer)\n",
    "#     # display(X)\n",
    "#     X = X[X.index == today]\n",
    "#     # display(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # X, y\n",
    "#     X['predict'] = ensemble.predict(X)\n",
    "#     X['Rank'] = (X['predict'].rank(method='first', ascending=False)-1).astype(int)\n",
    "#     sample_prediction['Rank'] = X['Rank'].values\n",
    "#     # display(sample_prediction)\n",
    "#     # display(myapi.gt_prices)\n",
    "#     # display(gt_prices)\n",
    "#     # check Rank\n",
    "#     assert sample_prediction[\"Rank\"].notna().all()\n",
    "#     assert sample_prediction[\"Rank\"].min() == 0\n",
    "#     assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n",
    "    \n",
    "# #     display(sample_prediction)\n",
    "#     env.predict(sample_prediction)\n",
    "# print(env.score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea5d23",
   "metadata": {
    "papermill": {
     "duration": 0.052655,
     "end_time": "2022-07-03T12:11:27.815273",
     "exception": false,
     "start_time": "2022-07-03T12:11:27.762618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "pd.to_datetime 최소한도로 쓸것. 연산 cost가 너무 큼. 특히 inference 단계에서는 쓰지 않기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93bb160e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:11:27.922205Z",
     "iopub.status.busy": "2022-07-03T12:11:27.921605Z",
     "iopub.status.idle": "2022-07-03T12:11:27.950732Z",
     "shell.execute_reply": "2022-07-03T12:11:27.949974Z"
    },
    "papermill": {
     "duration": 0.084992,
     "end_time": "2022-07-03T12:11:27.953062",
     "exception": false,
     "start_time": "2022-07-03T12:11:27.868070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jpx_tokyo_market_prediction\n",
    "env = jpx_tokyo_market_prediction.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a11ad26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T12:11:28.059342Z",
     "iopub.status.busy": "2022-07-03T12:11:28.058726Z",
     "iopub.status.idle": "2022-07-03T12:12:54.616942Z",
     "shell.execute_reply": "2022-07-03T12:12:54.615902Z"
    },
    "papermill": {
     "duration": 86.61457,
     "end_time": "2022-07-03T12:12:54.619723",
     "exception": false,
     "start_time": "2022-07-03T12:11:28.005153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:38<00:00, 51.76it/s]\n",
      "100%|██████████| 2000/2000 [00:41<00:00, 48.04it/s]\n"
     ]
    }
   ],
   "source": [
    "seccodes = np.sort(train['SecuritiesCode'].unique())\n",
    "main_list = stock_list.set_index('SecuritiesCode').loc[seccodes, ['33SectorCode']]\n",
    "\n",
    "for i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n",
    "#     t0 = time.time()\n",
    "    \n",
    "    # 이전 데이터와 합치고 최근 140일치만 이용한다\n",
    "    today = prices.iloc[0]['Date']\n",
    "    lastday = str(pd.to_datetime(today) - pd.DateOffset(200))\n",
    "    \n",
    "    # add 33SectorCode\n",
    "    prices = prices.set_index('SecuritiesCode').join(main_list).reset_index().astype({\n",
    "        'SecuritiesCode':'category',\n",
    "        '33SectorCode':'category'})\n",
    "\n",
    "    if i == 0:\n",
    "        close_df = pd.concat([\n",
    "            train_with_supp.loc[\n",
    "                (train_with_supp['Date'] > lastday) & (train_with_supp['Date'] < today)],\n",
    "            prices\n",
    "            ]).reset_index(drop=True)\n",
    "    else:\n",
    "        close_df = pd.concat([\n",
    "            close_df.loc[\n",
    "                (close_df['Date'] > lastday) & (close_df['Date'] < today)],\n",
    "            prices\n",
    "            ]).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "\n",
    "    sec_list_infer = divideSecurities(close_df)\n",
    "    X, _ = add_features_train(sec_list_infer)\n",
    "    # display(X)\n",
    "    X = X[X.index == today]\n",
    "    # display(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # X, y\n",
    "    X['predict'] = ensemble.predict(X)\n",
    "    X['Rank'] = (X['predict'].rank(method='first', ascending=False)-1).astype(int)\n",
    "    sample_prediction['Rank'] = X['Rank'].values\n",
    "    # display(sample_prediction)\n",
    "    # display(myapi.gt_prices)\n",
    "    # display(gt_prices)\n",
    "    # check Rank\n",
    "    assert sample_prediction[\"Rank\"].notna().all()\n",
    "    assert sample_prediction[\"Rank\"].min() == 0\n",
    "    assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n",
    "    \n",
    "#     display(sample_prediction)\n",
    "    env.predict(sample_prediction)\n",
    "#     print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26e7a6",
   "metadata": {
    "papermill": {
     "duration": 0.067877,
     "end_time": "2022-07-03T12:12:54.803988",
     "exception": false,
     "start_time": "2022-07-03T12:12:54.736111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 262.560288,
   "end_time": "2022-07-03T12:12:56.197485",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-03T12:08:33.637197",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
