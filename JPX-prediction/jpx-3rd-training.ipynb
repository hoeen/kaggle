{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.026599,"end_time":"2022-06-10T07:18:07.088733","exception":false,"start_time":"2022-06-10T07:18:07.062134","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:03.398551Z","iopub.execute_input":"2022-06-11T07:53:03.399149Z","iopub.status.idle":"2022-06-11T07:53:03.425692Z","shell.execute_reply.started":"2022-06-11T07:53:03.399004Z","shell.execute_reply":"2022-06-11T07:53:03.424679Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\nimport talib as ta ","metadata":{"papermill":{"duration":32.577331,"end_time":"2022-06-10T07:18:39.672443","exception":false,"start_time":"2022-06-10T07:18:07.095112","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:06.052208Z","iopub.execute_input":"2022-06-11T07:53:06.052578Z","iopub.status.idle":"2022-06-11T07:53:16.931292Z","shell.execute_reply.started":"2022-06-11T07:53:06.052549Z","shell.execute_reply":"2022-06-11T07:53:16.930112Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from talib-binary==0.4.19) (1.21.6)\nInstalling collected packages: talib-binary\nSuccessfully installed talib-binary-0.4.19\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler\nimport missingno as msno\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, mean_squared_error\n\n\nimport statsmodels.api as sm\nfrom pylab import rcParams\n\nfrom tqdm import tqdm\n\n# import ta\nfrom talib import abstract\n\n\nimport time\nimport gc\nimport sys\n\n# sys.path.insert(0, '../input/jpx-local-api')\n# from local_api import local_api","metadata":{"papermill":{"duration":3.040282,"end_time":"2022-06-10T07:18:42.722968","exception":false,"start_time":"2022-06-10T07:18:39.682686","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:16.933243Z","iopub.execute_input":"2022-06-11T07:53:16.933587Z","iopub.status.idle":"2022-06-11T07:53:19.216070Z","shell.execute_reply.started":"2022-06-11T07:53:16.933553Z","shell.execute_reply":"2022-06-11T07:53:19.215146Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')\n# display(data)\ntrain = data.copy()\n\n# using supplement data as test data\nsupp_data = pd.read_csv('/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv')\n\n# train_with_supp = pd.concat([train, supp_data]).reset_index(drop=True)\n# train_with_supp = train.copy()\n# train_with_supp\n","metadata":{"papermill":{"duration":8.985423,"end_time":"2022-06-10T07:18:51.715669","exception":false,"start_time":"2022-06-10T07:18:42.730246","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:19.217065Z","iopub.execute_input":"2022-06-11T07:53:19.217360Z","iopub.status.idle":"2022-06-11T07:53:25.528047Z","shell.execute_reply.started":"2022-06-11T07:53:19.217334Z","shell.execute_reply":"2022-06-11T07:53:25.527180Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"supplemental data 는 21/12/06 ~ 22/05/27까지 현재 115일로 이루어져 있다 (갱신될 수 있음)","metadata":{}},{"cell_type":"code","source":"supp_data.groupby('Date').count()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T07:53:25.530177Z","iopub.execute_input":"2022-06-11T07:53:25.531005Z","iopub.status.idle":"2022-06-11T07:53:25.604466Z","shell.execute_reply.started":"2022-06-11T07:53:25.530961Z","shell.execute_reply":"2022-06-11T07:53:25.603374Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"            RowId  SecuritiesCode  Open  High   Low  Close  Volume  \\\nDate                                                                 \n2021-12-06   2000            2000  1994  1994  1994   1994    2000   \n2021-12-07   2000            2000  1996  1996  1996   1996    2000   \n2021-12-08   2000            2000  1992  1992  1992   1992    2000   \n2021-12-09   2000            2000  1994  1994  1994   1994    2000   \n2021-12-10   2000            2000  1994  1994  1994   1994    2000   \n...           ...             ...   ...   ...   ...    ...     ...   \n2022-05-23   1998            1998  1995  1995  1995   1995    1998   \n2022-05-24   1998            1998  1989  1989  1989   1989    1998   \n2022-05-25   1998            1998  1994  1994  1994   1994    1998   \n2022-05-26   1998            1998  1990  1990  1990   1990    1998   \n2022-05-27   1997            1997  1993  1993  1993   1993    1997   \n\n            AdjustmentFactor  ExpectedDividend  SupervisionFlag  Target  \nDate                                                                     \n2021-12-06              2000                 0             2000    2000  \n2021-12-07              2000                 0             2000    2000  \n2021-12-08              2000                 0             2000    2000  \n2021-12-09              2000                 0             2000    2000  \n2021-12-10              2000                 0             2000    2000  \n...                      ...               ...              ...     ...  \n2022-05-23              1998                 0             1998    1998  \n2022-05-24              1998                 0             1998    1998  \n2022-05-25              1998                 0             1998    1997  \n2022-05-26              1998                48             1998    1997  \n2022-05-27              1997                 0             1997    1997  \n\n[115 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowId</th>\n      <th>SecuritiesCode</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>AdjustmentFactor</th>\n      <th>ExpectedDividend</th>\n      <th>SupervisionFlag</th>\n      <th>Target</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-12-06</th>\n      <td>2000</td>\n      <td>2000</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>2000</td>\n    </tr>\n    <tr>\n      <th>2021-12-07</th>\n      <td>2000</td>\n      <td>2000</td>\n      <td>1996</td>\n      <td>1996</td>\n      <td>1996</td>\n      <td>1996</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>2000</td>\n    </tr>\n    <tr>\n      <th>2021-12-08</th>\n      <td>2000</td>\n      <td>2000</td>\n      <td>1992</td>\n      <td>1992</td>\n      <td>1992</td>\n      <td>1992</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>2000</td>\n    </tr>\n    <tr>\n      <th>2021-12-09</th>\n      <td>2000</td>\n      <td>2000</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>2000</td>\n    </tr>\n    <tr>\n      <th>2021-12-10</th>\n      <td>2000</td>\n      <td>2000</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>2000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-05-23</th>\n      <td>1998</td>\n      <td>1998</td>\n      <td>1995</td>\n      <td>1995</td>\n      <td>1995</td>\n      <td>1995</td>\n      <td>1998</td>\n      <td>1998</td>\n      <td>0</td>\n      <td>1998</td>\n      <td>1998</td>\n    </tr>\n    <tr>\n      <th>2022-05-24</th>\n      <td>1998</td>\n      <td>1998</td>\n      <td>1989</td>\n      <td>1989</td>\n      <td>1989</td>\n      <td>1989</td>\n      <td>1998</td>\n      <td>1998</td>\n      <td>0</td>\n      <td>1998</td>\n      <td>1998</td>\n    </tr>\n    <tr>\n      <th>2022-05-25</th>\n      <td>1998</td>\n      <td>1998</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>1994</td>\n      <td>1998</td>\n      <td>1998</td>\n      <td>0</td>\n      <td>1998</td>\n      <td>1997</td>\n    </tr>\n    <tr>\n      <th>2022-05-26</th>\n      <td>1998</td>\n      <td>1998</td>\n      <td>1990</td>\n      <td>1990</td>\n      <td>1990</td>\n      <td>1990</td>\n      <td>1998</td>\n      <td>1998</td>\n      <td>48</td>\n      <td>1998</td>\n      <td>1997</td>\n    </tr>\n    <tr>\n      <th>2022-05-27</th>\n      <td>1997</td>\n      <td>1997</td>\n      <td>1993</td>\n      <td>1993</td>\n      <td>1993</td>\n      <td>1993</td>\n      <td>1997</td>\n      <td>1997</td>\n      <td>0</td>\n      <td>1997</td>\n      <td>1997</td>\n    </tr>\n  </tbody>\n</table>\n<p>115 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_scaled","metadata":{"execution":{"iopub.status.busy":"2022-06-11T06:00:44.783836Z","iopub.execute_input":"2022-06-11T06:00:44.784431Z","iopub.status.idle":"2022-06-11T06:00:44.815122Z","shell.execute_reply.started":"2022-06-11T06:00:44.784391Z","shell.execute_reply":"2022-06-11T06:00:44.813504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2021-10-20 이후로 설정하는 이유에 대해 EDA로 알아보기.\n\n# df = df[df.Date>\"2021-10-20\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-11T04:50:35.028628Z","iopub.execute_input":"2022-06-11T04:50:35.029709Z","iopub.status.idle":"2022-06-11T04:50:35.034453Z","shell.execute_reply.started":"2022-06-11T04:50:35.02967Z","shell.execute_reply":"2022-06-11T04:50:35.032979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def time_elapsed(t0):\n    t1 = time.time()\n    print(time.time() - t0)\n    return t1","metadata":{"papermill":{"duration":0.014964,"end_time":"2022-06-10T07:18:51.737733","exception":false,"start_time":"2022-06-10T07:18:51.722769","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:25.605814Z","iopub.execute_input":"2022-06-11T07:53:25.606259Z","iopub.status.idle":"2022-06-11T07:53:25.611717Z","shell.execute_reply.started":"2022-06-11T07:53:25.606218Z","shell.execute_reply":"2022-06-11T07:53:25.610737Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"SecuritiesCode 에 따라 인덱싱하는 것이 시간 소요가 너무 크다.  \n미리 Data를 종목마다 쪼개서 넣어 놓는게 좋겠다. - > 리스트 활용","metadata":{"papermill":{"duration":0.006467,"end_time":"2022-06-10T07:18:51.75129","exception":false,"start_time":"2022-06-10T07:18:51.744823","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def divideSecurities(df):\n    sec_list = []\n#     print('Divide securities individually..')\n    for code in np.sort(df.SecuritiesCode.unique()):\n        sec_list.append(df.loc[df.SecuritiesCode == code, :].reset_index(drop=True))\n    return sec_list\n\nsec_list = divideSecurities(train)","metadata":{"papermill":{"duration":10.220028,"end_time":"2022-06-10T07:19:01.978063","exception":false,"start_time":"2022-06-10T07:18:51.758035","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:25.613042Z","iopub.execute_input":"2022-06-11T07:53:25.613634Z","iopub.status.idle":"2022-06-11T07:53:33.726685Z","shell.execute_reply.started":"2022-06-11T07:53:25.613591Z","shell.execute_reply":"2022-06-11T07:53:33.725552Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def add_features_train(input_df, sec_list):\n    df_list = []\n    for df in tqdm(sec_list):\n        \n        # shadows\n        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n        \n\n        \n        # lagged features\n        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n        # lagged close, target (target 은 정확히 무엇? return인가)\n        \n        # lagged feature 계산하기 전 결측치 채워넣기\n        df = df.fillna(method='ffill')\n        \n\n        \n        # TA-lib features - RSI, EMA 7-90\n        df['RSI'] = ta.RSI(df['Close'])\n        df['EMA7'] = ta.EMA(df['Close'], 7)\n        df['EMA15'] = ta.EMA(df['Close'], 15)\n        df['EMA30'] = ta.EMA(df['Close'], 30)\n        df['EMA90'] = ta.EMA(df['Close'], 90)\n        \n\n        \n#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n\n\n        # fill ema features by backward -- 이렇게 채워진 것은 false data 이므로 일단 test 해보고 없애는 것을 검토하자.\n        df = df.fillna(method='bfill')\n\n    \n        # volatility\n        \n        df_list.append(df)\n\n        \n    df_feature_added = pd.concat(df_list).sort_values(['Date','SecuritiesCode'])\n    \n    return df_feature_added\n","metadata":{"papermill":{"duration":0.020518,"end_time":"2022-06-10T07:19:02.005975","exception":false,"start_time":"2022-06-10T07:19:01.985457","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:33.727851Z","iopub.execute_input":"2022-06-11T07:53:33.728191Z","iopub.status.idle":"2022-06-11T07:53:33.736123Z","shell.execute_reply.started":"2022-06-11T07:53:33.728154Z","shell.execute_reply":"2022-06-11T07:53:33.735393Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# add feature to data\n\ndf_added = add_features_train(train, sec_list)\n# df_added.to_csv('train_with_supp_feature_added.csv', index=False)\n\n# load data\n# df_added = pd.read_csv('/kaggle/input/train-with-supp-feature-added-v1-all-cdl/train_with_supp_feature_added.csv')\n\n# df_added","metadata":{"papermill":{"duration":17.085412,"end_time":"2022-06-10T07:19:19.098368","exception":false,"start_time":"2022-06-10T07:19:02.012956","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:33.736934Z","iopub.execute_input":"2022-06-11T07:53:33.737408Z","iopub.status.idle":"2022-06-11T07:53:48.826070Z","shell.execute_reply.started":"2022-06-11T07:53:33.737374Z","shell.execute_reply":"2022-06-11T07:53:48.825270Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 2000/2000 [00:11<00:00, 169.47it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_added.Date.unique()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T07:53:48.829632Z","iopub.execute_input":"2022-06-11T07:53:48.830130Z","iopub.status.idle":"2022-06-11T07:53:48.968490Z","shell.execute_reply.started":"2022-06-11T07:53:48.830074Z","shell.execute_reply":"2022-06-11T07:53:48.967519Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array(['2017-01-04', '2017-01-05', '2017-01-06', ..., '2021-12-01',\n       '2021-12-02', '2021-12-03'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"# 1202 개의 시간 간격이 모든 상품에 대해서 존재하는지 알아보기. ","metadata":{"execution":{"iopub.status.busy":"2022-06-11T07:53:48.970938Z","iopub.execute_input":"2022-06-11T07:53:48.971288Z","iopub.status.idle":"2022-06-11T07:53:48.975443Z","shell.execute_reply.started":"2022-06-11T07:53:48.971257Z","shell.execute_reply":"2022-06-11T07:53:48.974409Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def add_features_infer(input_df, close_df): #input df 는 price 데이터\n    \n    df_list = []\n    sec_list = divideSecurities(input_df)\n    \n    close_list = divideSecurities(close_df) # for rolling features\n    \n    for i in range(len(sec_list)):\n        \n        \n        close = close_list[i] #.loc[close_df.SecuritiesCode == code, :].fillna(method='ffill')\n        df = sec_list[i]\n        \n        # test data의 open, high, low, close 중 nan 있으면 이전 값에서 가져와 채움\n        if df.loc[:, ['Open', 'High', 'Low', 'Close']].isna().any().any():\n            df.loc[:, ['Open', 'High', 'Low', 'Close']] = close.loc[close['Date'] == close.iloc[-1]['Date'], ['Open', 'High', 'Low', 'Close']].values \n        \n        # shadows\n        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n\n        # lagged features\n        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n        # lagged close, target (target 은 정확히 무엇? return인가)\n        \n        ## Rolling features ##\n        # TA-lib features - RSI, EMA 7-90\n        df['RSI'] = ta.RSI(close['Close']).iloc[-1]\n        df['EMA7'] = ta.EMA(close['Close'], 7).iloc[-1]\n        df['EMA15'] = ta.EMA(close['Close'], 15).iloc[-1]\n        df['EMA30'] = ta.EMA(close['Close'], 30).iloc[-1]\n        df['EMA90'] = ta.EMA(close['Close'], 90).iloc[-1]\n        \n        \n#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n\n\n        # volatility\n        \n        df_list.append(df)\n    \n    df_feature_added = pd.concat(df_list)\n    \n    return df_feature_added\n\n","metadata":{"papermill":{"duration":0.02422,"end_time":"2022-06-10T07:19:19.12975","exception":false,"start_time":"2022-06-10T07:19:19.10553","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:48.976702Z","iopub.execute_input":"2022-06-11T07:53:48.977018Z","iopub.status.idle":"2022-06-11T07:53:48.988623Z","shell.execute_reply.started":"2022-06-11T07:53:48.976991Z","shell.execute_reply":"2022-06-11T07:53:48.987982Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def preprocess_train(df):\n    \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n#     minmax = MinMaxScaler()\n    stdsc = StandardScaler()\n    ordinal = OrdinalEncoder()\n\n    target = ['Target']\n#     minmax_features = ['Date']\n    ord_features = ['SecuritiesCode'] \n    scaled_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'upper_shadow', 'lower_shadow',\n                      'RSI', 'EMA7', 'EMA15', 'EMA30', 'EMA90'] #+ [c for c in df.columns if c.startswith('CDL')] # pattern recognition features\n    \n#     date_scaled = minmax.fit_transform(dfc.loc[:,minmax_features])\n    date_code_ord = ordinal.fit_transform(dfc.loc[:,ord_features])\n    scaled = stdsc.fit_transform(dfc.loc[:,scaled_features])\n    \n#     display(pd.DataFrame(date_code_ord, columns=ord_features))\n#     display(pd.DataFrame(scaled, columns=scaled_features))\n    \n    \n    dfc_scaled = pd.concat([# pd.DataFrame(date_scaled, columns=minmax_features),\n                            pd.DataFrame(date_code_ord, columns=ord_features),\n                            pd.DataFrame(scaled, columns=scaled_features)], \n                            axis=1)\n    dfc_scaled = pd.concat([df['Date'].reset_index(drop=True), dfc_scaled],\n                           axis=1)\n    dfc_scaled = dfc_scaled.set_index(['Date'])\n\n    y = dfc.set_index(['Date']).loc[:, ['Target']]\n    \n    \n    return dfc_scaled, y, [ordinal, stdsc]\n    \n\nX_scaled, y, trained_scalers = preprocess_train(df_added)  # 2021-12-06부터 test 시작이므로 그 전까지만 이용한다.\n\n# X_scaled\ny","metadata":{"papermill":{"duration":3.306171,"end_time":"2022-06-10T07:19:22.442841","exception":false,"start_time":"2022-06-10T07:19:19.13667","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T08:28:51.735764Z","iopub.execute_input":"2022-06-11T08:28:51.736165Z","iopub.status.idle":"2022-06-11T08:28:53.631361Z","shell.execute_reply.started":"2022-06-11T08:28:51.736122Z","shell.execute_reply":"2022-06-11T08:28:53.630377Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"              Target\nDate                \n2017-01-04  0.000730\n2017-01-04  0.012324\n2017-01-04  0.006154\n2017-01-04  0.011053\n2017-01-04  0.003026\n...              ...\n2021-12-03  0.034816\n2021-12-03  0.025478\n2021-12-03 -0.004302\n2021-12-03  0.009098\n2021-12-03  0.018414\n\n[2332531 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Target</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-01-04</th>\n      <td>0.000730</td>\n    </tr>\n    <tr>\n      <th>2017-01-04</th>\n      <td>0.012324</td>\n    </tr>\n    <tr>\n      <th>2017-01-04</th>\n      <td>0.006154</td>\n    </tr>\n    <tr>\n      <th>2017-01-04</th>\n      <td>0.011053</td>\n    </tr>\n    <tr>\n      <th>2017-01-04</th>\n      <td>0.003026</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-12-03</th>\n      <td>0.034816</td>\n    </tr>\n    <tr>\n      <th>2021-12-03</th>\n      <td>0.025478</td>\n    </tr>\n    <tr>\n      <th>2021-12-03</th>\n      <td>-0.004302</td>\n    </tr>\n    <tr>\n      <th>2021-12-03</th>\n      <td>0.009098</td>\n    </tr>\n    <tr>\n      <th>2021-12-03</th>\n      <td>0.018414</td>\n    </tr>\n  </tbody>\n</table>\n<p>2332531 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_inference(df, trained_scalers: list):\n    ordinal = trained_scalers[0]\n    stdsc = trained_scalers[1]\n    \n      \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n    target = ['Target']\n    ord_features = ['SecuritiesCode'] \n    scaled_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'upper_shadow', 'lower_shadow',\n                      'RSI', 'EMA7', 'EMA15', 'EMA30', 'EMA90'] #+ [c for c in df.columns if c.startswith('CDL')] # pattern recognition features\n    \n    \n    date_code_ord = ordinal.transform(dfc.loc[:,ord_features])\n    scaled = stdsc.transform(dfc.loc[:,scaled_features])\n    dfc_scaled = pd.concat([pd.DataFrame(date_code_ord, columns=ord_features),\n                            pd.DataFrame(scaled, columns=scaled_features)], axis=1)\n\n    \n    return dfc_scaled\n    \n\n# X_test_scaled = preprocess_train(df_added, trained_scalers)\n\n# X_test_scaled","metadata":{"papermill":{"duration":0.019575,"end_time":"2022-06-10T07:19:22.469389","exception":false,"start_time":"2022-06-10T07:19:22.449814","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T07:53:51.050125Z","iopub.execute_input":"2022-06-11T07:53:51.050712Z","iopub.status.idle":"2022-06-11T07:53:51.058170Z","shell.execute_reply.started":"2022-06-11T07:53:51.050665Z","shell.execute_reply":"2022-06-11T07:53:51.057168Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# base model - lgbm \nlgb = LGBMRegressor().fit(X_scaled, y)\n\n","metadata":{"papermill":{"duration":7.219516,"end_time":"2022-06-10T07:19:29.695617","exception":false,"start_time":"2022-06-10T07:19:22.476101","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-11T04:51:08.329144Z","iopub.execute_input":"2022-06-11T04:51:08.330498Z","iopub.status.idle":"2022-06-11T04:51:16.565909Z","shell.execute_reply.started":"2022-06-11T04:51:08.330436Z","shell.execute_reply":"2022-06-11T04:51:16.565026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configure test set and evaluate the model\n\nuse **TimeseriesCV**  \nreference: [the notebook about time-series basics](https://www.kaggle.com/code/kashnitsky/topic-9-part-1-time-series-analysis-in-python)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit ","metadata":{"execution":{"iopub.status.busy":"2022-06-11T07:59:41.080020Z","iopub.execute_input":"2022-06-11T07:59:41.080488Z","iopub.status.idle":"2022-06-11T07:59:41.085418Z","shell.execute_reply.started":"2022-06-11T07:59:41.080449Z","shell.execute_reply":"2022-06-11T07:59:41.084168Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Date당 주식(증권?) 수가 2000개에서 점점 줄어들어 마지막엔 1860개 정도 된다.","metadata":{}},{"cell_type":"code","source":"df_added.groupby(df_added.index).count().Date.plot()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T05:31:15.91456Z","iopub.execute_input":"2022-06-11T05:31:15.915077Z","iopub.status.idle":"2022-06-11T05:31:16.834824Z","shell.execute_reply.started":"2022-06-11T05:31:15.915034Z","shell.execute_reply":"2022-06-11T05:31:16.833598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train 의 전체 기간 1202일을 모두 커버하는 주식의 수는 1865개. 아닌 것이 135개이다.   \n\n135개의 주식의 정보","metadata":{}},{"cell_type":"code","source":"train.groupby('SecuritiesCode').count().loc[train.groupby('SecuritiesCode').count().RowId != 1202, :]","metadata":{"execution":{"iopub.status.busy":"2022-06-11T05:31:16.837868Z","iopub.execute_input":"2022-06-11T05:31:16.838625Z","iopub.status.idle":"2022-06-11T05:31:18.418089Z","shell.execute_reply.started":"2022-06-11T05:31:16.838573Z","shell.execute_reply":"2022-06-11T05:31:18.416724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time-series cv 를 어떻게 나누어야 할까?  \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-06-11T05:51:05.98678Z","iopub.execute_input":"2022-06-11T05:51:05.988304Z","iopub.status.idle":"2022-06-11T05:51:05.997422Z","shell.execute_reply.started":"2022-06-11T05:51:05.988213Z","shell.execute_reply":"2022-06-11T05:51:05.996489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-06-11T05:54:49.322464Z","iopub.execute_input":"2022-06-11T05:54:49.323935Z","iopub.status.idle":"2022-06-11T05:54:49.620135Z","shell.execute_reply.started":"2022-06-11T05:54:49.323868Z","shell.execute_reply":"2022-06-11T05:54:49.618414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scaled.columns","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:01:52.471533Z","iopub.execute_input":"2022-06-11T08:01:52.471979Z","iopub.status.idle":"2022-06-11T08:01:52.478963Z","shell.execute_reply.started":"2022-06-11T08:01:52.471943Z","shell.execute_reply":"2022-06-11T08:01:52.478223Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Index(['SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume',\n       'upper_shadow', 'lower_shadow', 'RSI', 'EMA7', 'EMA15', 'EMA30',\n       'EMA90'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"X_scaled.loc[X_scaled.index.unique()[:10], :]","metadata":{"execution":{"iopub.status.busy":"2022-06-11T05:58:05.311882Z","iopub.execute_input":"2022-06-11T05:58:05.312382Z","iopub.status.idle":"2022-06-11T05:58:05.588152Z","shell.execute_reply.started":"2022-06-11T05:58:05.312344Z","shell.execute_reply":"2022-06-11T05:58:05.587093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scaled.index.unique()[:5]","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:21:07.131029Z","iopub.execute_input":"2022-06-11T08:21:07.131516Z","iopub.status.idle":"2022-06-11T08:21:07.267060Z","shell.execute_reply.started":"2022-06-11T08:21:07.131478Z","shell.execute_reply":"2022-06-11T08:21:07.266120Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Index(['2017-01-04', '2017-01-05', '2017-01-06', '2017-01-10', '2017-01-11'], dtype='object', name='Date')"},"metadata":{}}]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:27:00.279893Z","iopub.execute_input":"2022-06-11T08:27:00.280324Z","iopub.status.idle":"2022-06-11T08:27:00.333732Z","shell.execute_reply.started":"2022-06-11T08:27:00.280286Z","shell.execute_reply":"2022-06-11T08:27:00.332712Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"Date\n2017-01-04    0.000730\n2017-01-04    0.012324\n2017-01-04    0.006154\n2017-01-04    0.011053\n2017-01-04    0.003026\n                ...   \n2021-12-03    0.034816\n2021-12-03    0.025478\n2021-12-03   -0.004302\n2021-12-03    0.009098\n2021-12-03    0.018414\nName: Target, Length: 2332531, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"grid[train_idx]","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:28:22.164700Z","iopub.execute_input":"2022-06-11T08:28:22.165283Z","iopub.status.idle":"2022-06-11T08:28:22.173021Z","shell.execute_reply.started":"2022-06-11T08:28:22.165231Z","shell.execute_reply":"2022-06-11T08:28:22.172111Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"Index(['2017-01-04', '2017-01-05', '2017-01-06', '2017-01-10', '2017-01-11',\n       '2017-01-12', '2017-01-13', '2017-01-16', '2017-01-17', '2017-01-18',\n       ...\n       '2017-06-02', '2017-06-05', '2017-06-06', '2017-06-07', '2017-06-08',\n       '2017-06-09', '2017-06-12', '2017-06-13', '2017-06-14', '2017-06-15'],\n      dtype='object', name='Date', length=112)"},"metadata":{}}]},{"cell_type":"code","source":"# debugging\ngrid = X_scaled.index.unique()\ntscv = TimeSeriesSplit(n_splits=10)\nfor train_idx, test_idx in tscv.split(grid):\n    X_train = X_scaled.loc[grid[train_idx], :]\n    y_train = y.loc[grid[train_idx], :]","metadata":{"execution":{"iopub.status.busy":"2022-06-11T08:28:13.622132Z","iopub.execute_input":"2022-06-11T08:28:13.622564Z","iopub.status.idle":"2022-06-11T08:28:13.844136Z","shell.execute_reply.started":"2022-06-11T08:28:13.622523Z","shell.execute_reply":"2022-06-11T08:28:13.843144Z"},"trusted":true},"execution_count":46,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/244310838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtscv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0;31m# ugly hack for GH #836\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mCheck\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0macross\u001b[0m \u001b[0mmy\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \"\"\"\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key_length\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexingError\u001b[0m: Too many indexers"],"ename":"IndexingError","evalue":"Too many indexers","output_type":"error"}]},{"cell_type":"markdown","source":"### TimeseriesCV + Optuna 로 LGBM hyperparameter 튜닝하기","metadata":{}},{"cell_type":"code","source":"def timeseriesCVerror(X, y, fold, loss, Model):#model, loss_function):\n    errors = []\n    tscv = TimeSeriesSplit(n_splits=fold)\n    # split input : index date\n    grid = X.index.unique()\n    \n    k = 0\n    for train_idx, test_idx in tscv.split(grid):\n        t0 = time.time()\n        k += 1\n        print(f'train for fold {k}...')\n        \n#         print(train_idx, test_idx)\n#         print(grid[train_idx])\n        X_train, y_train = X.loc[grid[train_idx], :], y.loc[grid[train_idx], :]\n        X_test, y_test = X.loc[grid[test_idx], :], y.loc[grid[test_idx], :]\n        \n        mod = Model.fit(X_train, y_train)\n        y_pred = mod.predict(X_test)\n        errors.append(loss(y_pred, y_test))\n        \n        print(f'time elapsed for fold {k} : {time.time() - t0:.4f} s')\n        \n    print(errors)\n    print(np.mean(errors))\n    return\n\n    \ntimeseriesCVerror(X_scaled, y, 10, mean_squared_error, \n                  LGBMRegressor(\n                      num_leaves=4000, learning_rate=0.5, \n                      n_estimators=2500, max_bin=88,\n                      seed=33\n                      )\n                 )","metadata":{"execution":{"iopub.status.busy":"2022-06-11T09:15:11.574914Z","iopub.execute_input":"2022-06-11T09:15:11.575290Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"train for fold 1...\n","output_type":"stream"}]},{"cell_type":"code","source":"?LGBMRegressor","metadata":{"execution":{"iopub.status.busy":"2022-06-11T09:04:35.979948Z","iopub.execute_input":"2022-06-11T09:04:35.980534Z","iopub.status.idle":"2022-06-11T09:04:36.059627Z","shell.execute_reply.started":"2022-06-11T09:04:35.980493Z","shell.execute_reply":"2022-06-11T09:04:36.058359Z"},"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mboosting_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gbdt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnum_leaves\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msubsample_for_bin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmin_split_gain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmin_child_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msubsample_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msilent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'warn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'split'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m      LightGBM regressor.\n\u001b[0;31mInit docstring:\u001b[0m\nConstruct a gradient boosting model.\n\nParameters\n----------\nboosting_type : str, optional (default='gbdt')\n    'gbdt', traditional Gradient Boosting Decision Tree.\n    'dart', Dropouts meet Multiple Additive Regression Trees.\n    'goss', Gradient-based One-Side Sampling.\n    'rf', Random Forest.\nnum_leaves : int, optional (default=31)\n    Maximum tree leaves for base learners.\nmax_depth : int, optional (default=-1)\n    Maximum tree depth for base learners, <=0 means no limit.\nlearning_rate : float, optional (default=0.1)\n    Boosting learning rate.\n    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n    in training using ``reset_parameter`` callback.\n    Note, that this will ignore the ``learning_rate`` argument in training.\nn_estimators : int, optional (default=100)\n    Number of boosted trees to fit.\nsubsample_for_bin : int, optional (default=200000)\n    Number of samples for constructing bins.\nobjective : str, callable or None, optional (default=None)\n    Specify the learning task and the corresponding learning objective or\n    a custom objective function to be used (see note below).\n    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\nclass_weight : dict, 'balanced' or None, optional (default=None)\n    Weights associated with classes in the form ``{class_label: weight}``.\n    Use this parameter only for multi-class classification task;\n    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n    You may want to consider performing probability calibration\n    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n    The 'balanced' mode uses the values of y to automatically adjust weights\n    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n    If None, all classes are supposed to have weight one.\n    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n    if ``sample_weight`` is specified.\nmin_split_gain : float, optional (default=0.)\n    Minimum loss reduction required to make a further partition on a leaf node of the tree.\nmin_child_weight : float, optional (default=1e-3)\n    Minimum sum of instance weight (hessian) needed in a child (leaf).\nmin_child_samples : int, optional (default=20)\n    Minimum number of data needed in a child (leaf).\nsubsample : float, optional (default=1.)\n    Subsample ratio of the training instance.\nsubsample_freq : int, optional (default=0)\n    Frequency of subsample, <=0 means no enable.\ncolsample_bytree : float, optional (default=1.)\n    Subsample ratio of columns when constructing each tree.\nreg_alpha : float, optional (default=0.)\n    L1 regularization term on weights.\nreg_lambda : float, optional (default=0.)\n    L2 regularization term on weights.\nrandom_state : int, RandomState object or None, optional (default=None)\n    Random number seed.\n    If int, this number is used to seed the C++ code.\n    If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n    If None, default seeds in C++ code are used.\nn_jobs : int, optional (default=-1)\n    Number of parallel threads.\nsilent : bool, optional (default=True)\n    Whether to print messages while running boosting.\nimportance_type : str, optional (default='split')\n    The type of feature importance to be filled into ``feature_importances_``.\n    If 'split', result contains numbers of times the feature is used in a model.\n    If 'gain', result contains total gains of splits which use the feature.\n**kwargs\n    Other parameters for the model.\n    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n\n    .. warning::\n\n        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n\nNote\n----\nA custom objective function can be provided for the ``objective`` parameter.\nIn this case, it should have the signature\n``objective(y_true, y_pred) -> grad, hess`` or\n``objective(y_true, y_pred, group) -> grad, hess``:\n\n    y_true : array-like of shape = [n_samples]\n        The target values.\n    y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n        The predicted values.\n        Predicted values are returned before any transformation,\n        e.g. they are raw margin instead of probability of positive class for binary task.\n    group : array-like\n        Group/query data.\n        Only used in the learning-to-rank task.\n        sum(group) = n_samples.\n        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n    grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n        The value of the first order derivative (gradient) of the loss\n        with respect to the elements of y_pred for each sample point.\n    hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n        The value of the second order derivative (Hessian) of the loss\n        with respect to the elements of y_pred for each sample point.\n\nFor multi-class task, the y_pred is group by class_id first, then group by row_id.\nIf you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\nand you should group grad and hess in this way as well.\n\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     DaskLGBMRegressor\n"},"metadata":{}}]},{"cell_type":"code","source":"tscv = TimeSeriesSplit(n_splits=10) #약 120일로 test를 쪼개는 것이 supp_data와 비슷한 크기이므로 10으로 쪼갬.\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-11T04:51:58.103581Z","iopub.execute_input":"2022-06-11T04:51:58.103962Z","iopub.status.idle":"2022-06-11T04:51:58.13252Z","shell.execute_reply.started":"2022-06-11T04:51:58.103932Z","shell.execute_reply":"2022-06-11T04:51:58.131535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_added.iloc[1749399:2040964]","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:58:41.872841Z","iopub.execute_input":"2022-06-10T13:58:41.873331Z","iopub.status.idle":"2022-06-10T13:58:41.915364Z","shell.execute_reply.started":"2022-06-10T13:58:41.873295Z","shell.execute_reply":"2022-06-10T13:58:41.914212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"evaluating metric : Sharpe ratio","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def timeseriesCVscore(params, model, series, loss_function=mean_squared_error, slen=24):\n    \"\"\"\n        Returns error on CV  \n        \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        slen - season length for Holt-Winters model\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=3) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        predictions = model.predict[-len(test):]\n        actual = values[test]\n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del data, train, supp_data, y, X_scaled\n# gc.collect()","metadata":{"execution":{"iopub.execute_input":"2022-06-10T07:19:29.712142Z","iopub.status.busy":"2022-06-10T07:19:29.711729Z","iopub.status.idle":"2022-06-10T07:19:29.969642Z","shell.execute_reply":"2022-06-10T07:19:29.968645Z"},"papermill":{"duration":0.268312,"end_time":"2022-06-10T07:19:29.971797","exception":false,"start_time":"2022-06-10T07:19:29.703485","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # local-api\n# myapi = local_api('../input/jpx-tokyo-stock-exchange-prediction/supplemental_files')\n# env = myapi.make_env()\n# iter_test = env.iter_test()\n# for i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(tqdm(iter_test)):\n# #     t0 = time.time()\n    \n#     # 이전 데이터와 합치고 최근 140일치만 이용한다\n#     today = prices.iloc[0]['Date']\n#     lastday = str(pd.to_datetime(today) - pd.DateOffset(140))\n    \n#     if i == 0:\n#         close_df = pd.concat([\n#             train_with_supp.loc[\n#                 (train_with_supp['Date'] > lastday) & (train_with_supp['Date'] < today), \n#                     ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']],\n#                 prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']]\n#             ]).reset_index(drop=True)\n        \n#     else:\n#         close_df = pd.concat([\n#             close_df.loc[\n#                 (close_df['Date'] > lastday) & (close_df['Date'] < today), \n#                     ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']],\n#                 prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']]\n#             ]).reset_index(drop=True)\n        \n\n#     feat = add_features_infer(prices, close_df)\n#     X = preprocess_inference(feat, trained_scalers)\n\n#     # X, y\n#     X['Target'] = lgb.predict(X)\n#     X['Rank'] = (X['Target'].rank(method='average', ascending=False)-1).astype(int)\n#     sample_prediction['Rank'] = X['Rank'].values\n    \n#     # check Rank\n#     assert sample_prediction[\"Rank\"].notna().all()\n#     assert sample_prediction[\"Rank\"].min() == 0\n#     assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n    \n# #     display(sample_prediction)\n#     env.predict(sample_prediction)\n# #     print(time.time() - t0)","metadata":{"execution":{"iopub.execute_input":"2022-06-10T07:19:29.987277Z","iopub.status.busy":"2022-06-10T07:19:29.986906Z","iopub.status.idle":"2022-06-10T07:19:29.993318Z","shell.execute_reply":"2022-06-10T07:19:29.992119Z"},"papermill":{"duration":0.016479,"end_time":"2022-06-10T07:19:29.995421","exception":false,"start_time":"2022-06-10T07:19:29.978942","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import jpx_tokyo_market_prediction\n# env = jpx_tokyo_market_prediction.make_env()\n# iter_test = env.iter_test()","metadata":{"execution":{"iopub.execute_input":"2022-06-10T07:19:30.010377Z","iopub.status.busy":"2022-06-10T07:19:30.009997Z","iopub.status.idle":"2022-06-10T07:19:30.039298Z","shell.execute_reply":"2022-06-10T07:19:30.038042Z"},"papermill":{"duration":0.039592,"end_time":"2022-06-10T07:19:30.041738","exception":false,"start_time":"2022-06-10T07:19:30.002146","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pd.to_datetime 최소한도로 쓸것. 연산 cost가 너무 큼. 특히 inference 단계에서는 쓰지 않기.","metadata":{"papermill":{"duration":0.006955,"end_time":"2022-06-10T07:19:30.055904","exception":false,"start_time":"2022-06-10T07:19:30.048949","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 2000개 종목 리스트에서 날짜 하나씩 빼고 뒷날짜 추가 구현?\n","metadata":{"execution":{"iopub.execute_input":"2022-06-10T07:19:30.072338Z","iopub.status.busy":"2022-06-10T07:19:30.071552Z","iopub.status.idle":"2022-06-10T07:19:30.075935Z","shell.execute_reply":"2022-06-10T07:19:30.075001Z"},"papermill":{"duration":0.014919,"end_time":"2022-06-10T07:19:30.077723","exception":false,"start_time":"2022-06-10T07:19:30.062804","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n#     t0 = time.time()\n    \n    # 이전 데이터와 합치고 최근 140일치만 이용한다\n    today = prices.iloc[0]['Date']\n    lastday = str(pd.to_datetime(today) - pd.DateOffset(140))\n    \n    if i == 0:\n        close_df = pd.concat([\n            train_with_supp.loc[\n                (train_with_supp['Date'] > lastday) & (train_with_supp['Date'] < today), \n                    ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']],\n                prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']]\n            ]).reset_index(drop=True)\n        \n    else:\n        close_df = pd.concat([\n            close_df.loc[\n                (close_df['Date'] > lastday) & (close_df['Date'] < today), \n                    ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']],\n                prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']]\n            ]).reset_index(drop=True)\n        \n\n    feat = add_features_infer(prices, close_df)\n    X = preprocess_inference(feat, trained_scalers)\n\n    # X, y\n    X['Target'] = lgb.predict(X)\n    X['Rank'] = (X['Target'].rank(method='first', ascending=False)-1).astype(int)\n    sample_prediction['Rank'] = X['Rank'].values\n    # check Rank\n    assert sample_prediction[\"Rank\"].notna().all()\n    assert sample_prediction[\"Rank\"].min() == 0\n    assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n#     display(sample_prediction)\n    env.predict(sample_prediction)\n#     print(time.time() - t0)","metadata":{"execution":{"iopub.execute_input":"2022-06-10T07:19:30.094129Z","iopub.status.busy":"2022-06-10T07:19:30.093457Z","iopub.status.idle":"2022-06-10T07:20:00.064786Z","shell.execute_reply":"2022-06-10T07:20:00.063803Z"},"papermill":{"duration":29.982802,"end_time":"2022-06-10T07:20:00.067238","exception":false,"start_time":"2022-06-10T07:19:30.084436","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.006632,"end_time":"2022-06-10T07:20:00.080839","exception":false,"start_time":"2022-06-10T07:20:00.074207","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}