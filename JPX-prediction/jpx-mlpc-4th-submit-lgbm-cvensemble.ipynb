{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-06-28T18:52:32.997797Z","iopub.status.busy":"2022-06-28T18:52:32.997353Z","iopub.status.idle":"2022-06-28T18:52:33.010810Z","shell.execute_reply":"2022-06-28T18:52:33.009804Z"},"papermill":{"duration":0.026696,"end_time":"2022-06-28T18:52:33.013483","exception":false,"start_time":"2022-06-28T18:52:32.986787","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install ../input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\nimport talib as ta ","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:52:33.030945Z","iopub.status.busy":"2022-06-28T18:52:33.030181Z","iopub.status.idle":"2022-06-28T18:53:05.715762Z","shell.execute_reply":"2022-06-28T18:53:05.714459Z"},"papermill":{"duration":32.697433,"end_time":"2022-06-28T18:53:05.718764","exception":false,"start_time":"2022-06-28T18:52:33.021331","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import early_stopping\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler\nimport missingno as msno\n\nfrom pylab import rcParams\n\nfrom tqdm import tqdm\n\n# import ta\nfrom talib import abstract\n\n\nimport time\nimport gc\nimport sys\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import GroupKFold\n\nimport seaborn as sns\nimport plotly.graph_objects as go\n\nfrom decimal import ROUND_HALF_UP, Decimal\n\n# sys.path.insert(0, '../input/jpx-local-api')\n# from local_api import local_api","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:05.737890Z","iopub.status.busy":"2022-06-28T18:53:05.737227Z","iopub.status.idle":"2022-06-28T18:53:08.894763Z","shell.execute_reply":"2022-06-28T18:53:08.893670Z"},"papermill":{"duration":3.170787,"end_time":"2022-06-28T18:53:08.897607","exception":false,"start_time":"2022-06-28T18:53:05.726820","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')\n# display(data)\ntrain = data.copy()\n\n# using supplement data as test data\nsupp_data = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv')\n\ntrain_with_supp = pd.concat([train, supp_data]).reset_index(drop=True)\n# train_with_supp = train.copy()\n# train_with_supp\n","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:08.917470Z","iopub.status.busy":"2022-06-28T18:53:08.915989Z","iopub.status.idle":"2022-06-28T18:53:16.720457Z","shell.execute_reply":"2022-06-28T18:53:16.719121Z"},"papermill":{"duration":7.817207,"end_time":"2022-06-28T18:53:16.723339","exception":false,"start_time":"2022-06-28T18:53:08.906132","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def time_elapsed(t0):\n#     t1 = time.time()\n#     print(time.time() - t0)\n#     return t1","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:16.741465Z","iopub.status.busy":"2022-06-28T18:53:16.741021Z","iopub.status.idle":"2022-06-28T18:53:16.746106Z","shell.execute_reply":"2022-06-28T18:53:16.744858Z"},"papermill":{"duration":0.017052,"end_time":"2022-06-28T18:53:16.748597","exception":false,"start_time":"2022-06-28T18:53:16.731545","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SecuritiesCode 에 따라 인덱싱하는 것이 시간 소요가 너무 크다.  \n미리 Data를 종목마다 쪼개서 넣어 놓는게 좋겠다. - > 리스트 활용","metadata":{"papermill":{"duration":0.008076,"end_time":"2022-06-28T18:53:16.765513","exception":false,"start_time":"2022-06-28T18:53:16.757437","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:16.783586Z","iopub.status.busy":"2022-06-28T18:53:16.783177Z","iopub.status.idle":"2022-06-28T18:53:16.794621Z","shell.execute_reply":"2022-06-28T18:53:16.793361Z"},"papermill":{"duration":0.023675,"end_time":"2022-06-28T18:53:16.797341","exception":false,"start_time":"2022-06-28T18:53:16.773666","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_adjusted_close(df):\n    \"\"\"\n    Args:\n        df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n    Returns:\n        df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n    \"\"\"\n    # sort data to generate CumulativeAdjustmentFactor\n    df = df.sort_values(\"Date\", ascending=False)\n    # generate CumulativeAdjustmentFactor\n    df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n    # generate AdjustedClose\n    df.loc[:, \"AdjustedClose\"] = (\n        df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n    ).map(lambda x: float(\n        Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n    ))\n    # reverse order\n    df = df.sort_values(\"Date\")\n    # to fill AdjustedClose, replace 0 into np.nan\n    df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n    # forward fill AdjustedClose\n    df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n\n    df = df.drop(columns=['CumulativeAdjustmentFactor'])\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ta_features(df, inf=None, train=True):\n    \"\"\"\n    Get technical features from TA-Lib\n    ref : https://www.kaggle.com/code/daosword/jpx-pytorch-neural-network-with-ta-lib-features\n    \"\"\"\n    if train:\n        op = df['Open']\n        hi = df['High']\n        lo = df['Low']\n        cl = df['AdjustedClose']  #df['Close']\n        vo = df['Volume']\n\n    #     # Overlap Studies\n    #     df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n    #     df['DEMA'] = ta.DEMA(cl, timeperiod=30)\n        for period in [7,15,30,90]:\n            df['EMA{}'.format(period)] = cl.ewm(span=period,  adjust=False).mean().iloc[-1]/cl\n            df['return{}'.format(period)] = cl.pct_change(period)\n            df['volatility{}'.format(period)] = np.log(cl).diff().rolling(period).std()\n            \n            \n\n\n        # df['EMA7'] = ta.EMA(cl, 7)/cl\n        # df['EMA15'] = ta.EMA(cl, 15)/cl\n        # df['EMA30'] = ta.EMA(cl, 30)/cl\n        # df['EMA90'] = ta.EMA(cl, 90)/cl\n\n    #     df['HT_TRENDLINE'] = ta.HT_TRENDLINE(cl)\n    #     df['KAMA'] = ta.KAMA(cl, timeperiod=30)\n    #     df['MA'] = ta.MA(cl, timeperiod=30, matype=0)\n    #     df['MIDPOINT'] = ta.MIDPOINT(cl, timeperiod=14)\n    #     df['SAR'] = ta.SAR(hi, lo, acceleration=0, maximum=0)\n    #     df['SAREXT'] = ta.SAREXT(hi, lo, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n    #     df['SMA'] = ta.SMA(cl, timeperiod=30)\n    #     df['T3'] = ta.T3(df['Close'], timeperiod=5, vfactor=0)\n    #     df['TEMA'] = ta.TEMA(df['Close'], timeperiod=30)\n    #     df['TRIMA'] = ta.TRIMA(df['Close'], timeperiod=30)\n    #     df['WMA'] = ta.WMA(df['Close'], timeperiod=30)\n\n        # Momentum Indicators\n    #     df['ADX'] = ta.ADX(hi, lo, cl, timeperiod=14)\n    #     df['ADXR'] = ta.ADXR(hi, lo, cl, timeperiod=14)\n    #     df['APO'] = ta.APO(cl, fastperiod=12, slowperiod=26, matype=0)\n    #     df['AROON_down'], df['AROON_up'] = ta.AROON(hi, lo, timeperiod=14)\n    #     df['AROONOSC'] = ta.AROONOSC(hi, lo, timeperiod=14)\n    #     df['BOP'] = ta.BOP(op, hi, lo, cl)\n    #     df['CCI'] = ta.CCI(hi, lo, cl, timeperiod=14)\n    #     df['DX'] = ta.DX(hi, lo, cl, timeperiod=14)\n    #     df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = ta.MACD(cl, fastperiod=12, slowperiod=26, signalperiod=9)\n    #     df['MFI'] = ta.MFI(hi, lo, cl, vo, timeperiod=14)\n    #     df['MINUS_DI'] = ta.MINUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['MINUS_DM'] = ta.MINUS_DM(hi, lo, timeperiod=14)\n        df['MOM'] = ta.MOM(cl, timeperiod=10)\n    #     df['PLUS_DI'] = ta.PLUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['PLUS_DM'] = ta.PLUS_DM(hi, lo, timeperiod=14)\n        df['RSI'] = ta.RSI(cl, timeperiod=14)\n        df['STOCH_slowk'], df['STOCH_slowd'] = ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n        df['STOCHF_fastk'], df['STOCHF_fastd'] = ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)\n        df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\n    #     df['TRIX'] = ta.TRIX(cl, timeperiod=30)\n    #     df['ULTOSC'] = ta.ULTOSC(hi, lo, cl, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n    #     df['WILLR'] = ta.WILLR(hi, lo, cl, timeperiod=14)\n\n        # Volume Indicators\n    #     df['AD'] = ta.AD(hi, lo, cl, vo)\n    #     df['ADOSC'] = ta.ADOSC(hi, lo, cl, vo, fastperiod=3, slowperiod=10)\n    #     df['OBV'] = ta.OBV(cl, vo)\n\n        # Volatility Indicators\n        df['ATR'] = ta.ATR(hi, lo, cl, timeperiod=14)\n        df['NATR'] = ta.NATR(hi, lo, cl, timeperiod=14)\n        df['TRANGE'] = ta.TRANGE(hi, lo, cl)\n\n        # Cycle Indicators\n    #     df['HT_DCPERIOD'] = ta.HT_DCPERIOD(cl)\n    #     df['HT_DCPHASE'] = ta.HT_DCPHASE(cl)\n    #     df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = ta.HT_PHASOR(cl)\n    #     df['HT_SINE_sine'], df['HT_SINE_leadsine'] = ta.HT_SINE(cl)\n    #     df['HT_TRENDMODE'] = ta.HT_TRENDMODE(cl)\n\n        # Statistic Functions\n    #     df['BETA'] = ta.BETA(hi, lo, timeperiod=5)\n    #     df['CORREL'] = ta.CORREL(hi, lo, timeperiod=30)\n    #     df['LINEARREG'] = ta.LINEARREG(cl, timeperiod=14) - cl\n    #     df['LINEARREG_ANGLE'] = ta.LINEARREG_ANGLE(cl, timeperiod=14)\n    #     df['LINEARREG_INTERCEPT'] = ta.LINEARREG_INTERCEPT(cl, timeperiod=14) - cl\n    #     df['LINEARREG_SLOPE'] = ta.LINEARREG_SLOPE(cl, timeperiod=14)\n        df['STDDEV'] = ta.STDDEV(cl, timeperiod=5, nbdev=1)   \n    \n    \n    else: #inference \n        op = inf['Open']\n        hi = inf['High']\n        lo = inf['Low']\n        cl = inf['Close']\n        vo = inf['Volume']\n        \n            #     # Overlap Studies\n    #     df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n    #     df['DEMA'] = ta.DEMA(cl, timeperiod=30)\n\n        # df['EMA7'] = cl.ewm(span=7, min_periods=3, adjust=False).mean().iloc[-1]/cl\n        # df['EMA15'] = cl.ewm(span=15, min_periods=7, adjust=False).mean().iloc[-1]/cl\n        # df['EMA30'] = cl.ewm(span=30, min_periods=15, adjust=False).mean().iloc[-1]/cl\n        # df['EMA90'] = cl.ewm(span=90, min_periods=30, adjust=False).mean().iloc[-1]/cl\n\n        df['EMA7'] = (ta.EMA(cl, 7)/cl).iloc[-1]\n        df['EMA15'] = (ta.EMA(cl, 15)/cl).iloc[-1]\n        df['EMA30'] = (ta.EMA(cl, 30)/cl).iloc[-1]\n        df['EMA90'] = (ta.EMA(cl, 90)/cl).iloc[-1]\n\n    #     df['HT_TRENDLINE'] = ta.HT_TRENDLINE(cl)\n    #     df['KAMA'] = ta.KAMA(cl, timeperiod=30)\n    #     df['MA'] = ta.MA(cl, timeperiod=30, matype=0)\n    #     df['MIDPOINT'] = ta.MIDPOINT(cl, timeperiod=14)\n    #     df['SAR'] = ta.SAR(hi, lo, acceleration=0, maximum=0)\n    #     df['SAREXT'] = ta.SAREXT(hi, lo, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n    #     df['SMA'] = ta.SMA(cl, timeperiod=30)\n    #     df['T3'] = ta.T3(df['Close'], timeperiod=5, vfactor=0)\n    #     df['TEMA'] = ta.TEMA(df['Close'], timeperiod=30)\n    #     df['TRIMA'] = ta.TRIMA(df['Close'], timeperiod=30)\n    #     df['WMA'] = ta.WMA(df['Close'], timeperiod=30)\n\n        # Momentum Indicators\n    #     df['ADX'] = ta.ADX(hi, lo, cl, timeperiod=14)\n    #     df['ADXR'] = ta.ADXR(hi, lo, cl, timeperiod=14)\n    #     df['APO'] = ta.APO(cl, fastperiod=12, slowperiod=26, matype=0)\n    #     df['AROON_down'], df['AROON_up'] = ta.AROON(hi, lo, timeperiod=14)\n    #     df['AROONOSC'] = ta.AROONOSC(hi, lo, timeperiod=14)\n    #     df['BOP'] = ta.BOP(op, hi, lo, cl)\n    #     df['CCI'] = ta.CCI(hi, lo, cl, timeperiod=14)\n    #     df['DX'] = ta.DX(hi, lo, cl, timeperiod=14)\n    #     df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = ta.MACD(cl, fastperiod=12, slowperiod=26, signalperiod=9)\n    #     df['MFI'] = ta.MFI(hi, lo, cl, vo, timeperiod=14)\n    #     df['MINUS_DI'] = ta.MINUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['MINUS_DM'] = ta.MINUS_DM(hi, lo, timeperiod=14)\n        df['MOM'] = ta.MOM(cl, timeperiod=10).iloc[-1]\n    #     df['PLUS_DI'] = ta.PLUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['PLUS_DM'] = ta.PLUS_DM(hi, lo, timeperiod=14)\n        df['RSI'] = ta.RSI(cl, timeperiod=14).iloc[-1]\n        df['STOCH_slowk'], df['STOCH_slowd'] = (ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)[0].iloc[-1],\n                                                ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)[1].iloc[-1])\n        df['STOCHF_fastk'], df['STOCHF_fastd'] = (ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)[0].iloc[-1],\n                                                  ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)[1].iloc[-1])\n        df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = (ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[0].iloc[-1],\n                                                      ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[1].iloc[-1])\n    #     df['TRIX'] = ta.TRIX(cl, timeperiod=30)\n    #     df['ULTOSC'] = ta.ULTOSC(hi, lo, cl, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n    #     df['WILLR'] = ta.WILLR(hi, lo, cl, timeperiod=14)\n\n        # Volume Indicators\n    #     df['AD'] = ta.AD(hi, lo, cl, vo)\n    #     df['ADOSC'] = ta.ADOSC(hi, lo, cl, vo, fastperiod=3, slowperiod=10)\n    #     df['OBV'] = ta.OBV(cl, vo)\n\n        # Volatility Indicators\n        df['ATR'] = ta.ATR(hi, lo, cl, timeperiod=14).iloc[-1]\n        df['NATR'] = ta.NATR(hi, lo, cl, timeperiod=14).iloc[-1]\n        df['TRANGE'] = ta.TRANGE(hi, lo, cl).iloc[-1]\n\n        # Cycle Indicators\n    #     df['HT_DCPERIOD'] = ta.HT_DCPERIOD(cl)\n    #     df['HT_DCPHASE'] = ta.HT_DCPHASE(cl)\n    #     df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = ta.HT_PHASOR(cl)\n    #     df['HT_SINE_sine'], df['HT_SINE_leadsine'] = ta.HT_SINE(cl)\n    #     df['HT_TRENDMODE'] = ta.HT_TRENDMODE(cl)\n\n        # Statistic Functions\n    #     df['BETA'] = ta.BETA(hi, lo, timeperiod=5)\n    #     df['CORREL'] = ta.CORREL(hi, lo, timeperiod=30)\n    #     df['LINEARREG'] = ta.LINEARREG(cl, timeperiod=14) - cl\n    #     df['LINEARREG_ANGLE'] = ta.LINEARREG_ANGLE(cl, timeperiod=14)\n    #     df['LINEARREG_INTERCEPT'] = ta.LINEARREG_INTERCEPT(cl, timeperiod=14) - cl\n    #     df['LINEARREG_SLOPE'] = ta.LINEARREG_SLOPE(cl, timeperiod=14)\n        df['STDDEV'] = ta.STDDEV(cl, timeperiod=5, nbdev=1).iloc[-1]   \n    \n    return df","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:16.816718Z","iopub.status.busy":"2022-06-28T18:53:16.816322Z","iopub.status.idle":"2022-06-28T18:53:16.848509Z","shell.execute_reply":"2022-06-28T18:53:16.847171Z"},"papermill":{"duration":0.045125,"end_time":"2022-06-28T18:53:16.850841","exception":false,"start_time":"2022-06-28T18:53:16.805716","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def divideSecurities(df):\n    sec_list = []\n#     print('Divide securities individually..')\n    for code in np.sort(df.SecuritiesCode.unique()):\n        sec_list.append(df.loc[df.SecuritiesCode == code, :].reset_index(drop=True))\n    return sec_list\n\nsec_list = divideSecurities(train)","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:16.868985Z","iopub.status.busy":"2022-06-28T18:53:16.868613Z","iopub.status.idle":"2022-06-28T18:53:26.155148Z","shell.execute_reply":"2022-06-28T18:53:26.153932Z"},"papermill":{"duration":9.298566,"end_time":"2022-06-28T18:53:26.157748","exception":false,"start_time":"2022-06-28T18:53:16.859182","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features_train(sec_list):\n    df_list = []\n    for df in tqdm(sec_list):\n        \n        # shadows\n        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n        \n        # adjusted close\n        df = generate_adjusted_close(df)\n\n        ## Rolling features ##\n        \n        # lagged features\n        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n        # lagged close, target (target 은 정확히 무엇? return인가)\n        \n        # lagged feature 계산하기 전 결측치 채워넣기\n        df = df.fillna(method='ffill')\n        # df = df.interpolate()\n        \n        \n        \n        # All indicators in ta\n        df = get_ta_features(df.copy())\n        # try:\n        #     df = get_ta_features(df.copy())\n        # except:\n        #     print(f\"error in SecuritiesCode: {df['SecuritiesCode'][0]}\")\n        #     display(df)\n        #     continue\n                \n        # not add pattern recognition - feature importance 가 거의 0임. \n#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n\n\n        ####  For test!! df['STOCHF_fastd'] df['STOCHRSI_fastk'] df['ATR']\n        ####  open high low close 등 제거\n        df = df.drop(columns=['STOCHF_fastd', 'STOCHRSI_fastk', 'STOCH_slowk', 'NATR', 'ATR'])\n    #     df = df.drop(columns=['upper_shadow', 'Low', 'TRANGE', 'High',\n    #    'STOCHRSI_fastk', 'STOCHRSI_fastd', 'Open'])\n        # fill ema features by backward -- 이렇게 채워진 것은 false data 이므로 일단 test 해보고 없애는 것을 검토하자.\n        # df = df.fillna(method='ffill')\n        # df = df.interpolate()\n        \n        # volatility\n        \n        df_list.append(df)\n        \n        del df\n        \n    gc.collect()\n    df_feature_added = pd.concat(df_list).sort_values(['Date','SecuritiesCode'])\n    \n    return df_feature_added\n\n\n","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:26.176550Z","iopub.status.busy":"2022-06-28T18:53:26.176097Z","iopub.status.idle":"2022-06-28T18:53:26.187451Z","shell.execute_reply":"2022-06-28T18:53:26.186113Z"},"papermill":{"duration":0.024417,"end_time":"2022-06-28T18:53:26.190377","exception":false,"start_time":"2022-06-28T18:53:26.165960","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def add_features_train(input_df, sec_list):\n#     df_list = []\n#     for df in sec_list:\n        \n#         # shadows\n#         df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n#         df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n        \n\n        \n#         # lagged features\n#         # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n#         # lagged close, target (target 은 정확히 무엇? return인가)\n        \n#         # lagged feature 계산하기 전 결측치 채워넣기\n#         df = df.fillna(method='ffill')\n        \n\n        \n#         # TA-lib features - RSI, EMA 7-90\n#         df['RSI'] = ta.RSI(df['Close'])\n#         df['EMA7'] = ta.EMA(df['Close'], 7)\n#         df['EMA15'] = ta.EMA(df['Close'], 15)\n#         df['EMA30'] = ta.EMA(df['Close'], 30)\n#         df['EMA90'] = ta.EMA(df['Close'], 90)\n        \n\n        \n# #         for indicator in ta.get_function_groups()['Pattern Recognition']:\n# #             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n\n\n#         # fill ema features by backward -- 이렇게 채워진 것은 false data 이므로 일단 test 해보고 없애는 것을 검토하자.\n#         df = df.fillna(method='bfill')\n\n    \n#         # volatility\n        \n#         df_list.append(df)\n\n        \n#     df_feature_added = pd.concat(df_list)\n    \n#     return df_feature_added\n","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:26.210358Z","iopub.status.busy":"2022-06-28T18:53:26.209758Z","iopub.status.idle":"2022-06-28T18:53:26.216067Z","shell.execute_reply":"2022-06-28T18:53:26.215230Z"},"papermill":{"duration":0.019438,"end_time":"2022-06-28T18:53:26.218606","exception":false,"start_time":"2022-06-28T18:53:26.199168","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add feature to data\n\ndf_added = add_features_train(sec_list)\n# df_added = df_added[(df_added['Date'] >= '2021-06-03') & (df_added['Date'] <= '2021-12-03')]\n# df_added = df_added[df_added['Date'] >= '2021-05-27']\n# df_added.to_csv('train_with_supp_feature_added.csv', index=False)\n\n# load data\n# df_added = pd.read_csv('/kaggle/input/train-with-supp-feature-added-v1-all-cdl/train_with_supp_feature_added.csv')\n\n# df_added","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:26.239226Z","iopub.status.busy":"2022-06-28T18:53:26.238094Z","iopub.status.idle":"2022-06-28T18:53:58.575323Z","shell.execute_reply":"2022-06-28T18:53:58.573985Z"},"papermill":{"duration":32.350483,"end_time":"2022-06-28T18:53:58.578083","exception":false,"start_time":"2022-06-28T18:53:26.227600","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features_infer(input_df, close_df): #input df 는 price 데이터\n    \n    df_list = []\n    \n    print('Divide input securities...')\n    sec_list = divideSecurities(input_df)\n    print('Divide close_df securities...')\n    close_list = divideSecurities(close_df) # for rolling features\n    print('='*10 + 'feature adding' + '='*10)\n    \n    for i in range(len(sec_list)):\n        \n        \n        close = close_list[i] #.loc[close_df.SecuritiesCode == code, :].fillna(method='ffill')\n        df = sec_list[i]\n#         # test data의 open, high, low, close 중 nan 있으면 이전 값에서 가져와 채움\n        if df.loc[:, ['Open', 'High', 'Low', 'Close']].isna().any().any():\n#             display(df)\n#             display(close)\n# #             print(close.iloc[-2]['Open', 'High', 'Low', 'Close'].values())\n            df.loc[:, ['Open', 'High', 'Low', 'Close']] = close.loc[close['Date'] == close.iloc[-1]['Date'], ['Open', 'High', 'Low', 'Close']].values \n        \n        \n\n        \n        # shadows\n        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n\n        # lagged features\n        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n        # lagged close, target (target 은 정확히 무엇? return인가)\n        \n        ## Rolling features ##\n        \n        \n        # TA-lib features\n        df = get_ta_features(df, close, train=False)\n        \n        ####  open high low close 등 제거\n        df = df.drop(columns=['STOCHF_fastd', 'STOCHRSI_fastk', 'ATR', 'Open', 'High', 'Low', 'Close'])\n        \n#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n\n\n        # volatility\n        \n        df_list.append(df)\n        del df\n    \n    df_feature_added = pd.concat(df_list)\n    \n    return df_feature_added\n\n","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:58.632204Z","iopub.status.busy":"2022-06-28T18:53:58.631814Z","iopub.status.idle":"2022-06-28T18:53:58.646481Z","shell.execute_reply":"2022-06-28T18:53:58.645227Z"},"papermill":{"duration":0.044338,"end_time":"2022-06-28T18:53:58.648874","exception":false,"start_time":"2022-06-28T18:53:58.604536","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_train(df):\n    \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n#     minmax = MinMaxScaler()\n    stdsc = StandardScaler()\n    ordinal = OrdinalEncoder()\n\n    target = ['Target']\n#     minmax_features = ['Date']\n    ord_features = ['SecuritiesCode'] \n    scaled_features = [i for i in df.columns if i not in ['Date', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag',\n                                                         'Target', 'SecuritiesCode']]\n    \n#     date_scaled = minmax.fit_transform(dfc.loc[:,minmax_features])\n    date_code_ord = ordinal.fit_transform(dfc.loc[:,ord_features])\n    scaled = stdsc.fit_transform(dfc.loc[:,scaled_features])\n    \n#     display(pd.DataFrame(date_code_ord, columns=ord_features))\n#     display(pd.DataFrame(scaled, columns=scaled_features))\n    \n    \n    dfc_scaled = pd.concat([df['Date'].reset_index(drop=True),\n                            pd.DataFrame(date_code_ord, columns=ord_features),\n                            pd.DataFrame(scaled, columns=scaled_features)], axis=1)\n\n    dfc_scaled = dfc_scaled.set_index(['Date']).drop(columns=['Close'])\n\n    y = dfc.set_index(['Date']).loc[:, 'Target']\n    \n    return dfc_scaled, y, [ordinal, stdsc]\n    \n\nX_scaled, y, trained_scalers = preprocess_train(df_added)  # 2021-12-06부터 test 시작이므로 그 전까지만 이용한다.\n\n# X_scaled","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:58.701210Z","iopub.status.busy":"2022-06-28T18:53:58.700837Z","iopub.status.idle":"2022-06-28T18:53:58.914322Z","shell.execute_reply":"2022-06-28T18:53:58.913199Z"},"papermill":{"duration":0.243211,"end_time":"2022-06-28T18:53:58.917252","exception":false,"start_time":"2022-06-28T18:53:58.674041","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scaled","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_inference(df, trained_scalers: list):\n    ordinal = trained_scalers[0]\n    stdsc = trained_scalers[1]\n    \n      \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n    target = ['Target']\n    ord_features = ['SecuritiesCode'] \n    scaled_features = [i for i in df.columns if i not in ['Date', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag',\n                                                         'Target', 'SecuritiesCode']]\n#     print('scaled_features:', scaled_features)\n    date_code_ord = ordinal.transform(dfc.loc[:,ord_features])\n    scaled = stdsc.transform(dfc.loc[:,scaled_features])\n    \n\n\n    dfc_scaled = pd.concat([df['Date'].reset_index(drop=True),\n                            pd.DataFrame(date_code_ord, columns=ord_features),\n                            pd.DataFrame(scaled, columns=scaled_features)], axis=1)\n\n    dfc_scaled = dfc_scaled.set_index(['Date'])\n\n    \n\n#     y = dfc.loc[:, ['Target']]\n    \n    return dfc_scaled\n\n\n# X_test_scaled = preprocess_train(df_added, trained_scalers)\n\n# X_test_scaled","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:58.970628Z","iopub.status.busy":"2022-06-28T18:53:58.970202Z","iopub.status.idle":"2022-06-28T18:53:58.980321Z","shell.execute_reply":"2022-06-28T18:53:58.979144Z"},"papermill":{"duration":0.039544,"end_time":"2022-06-28T18:53:58.982753","exception":false,"start_time":"2022-06-28T18:53:58.943209","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n 'n_estimators': 1000,\n 'learning_rate': 0.0497652883951982,\n 'num_leaves': 227,\n 'max_depth': 12,\n 'min_data_in_leaf': 500,\n 'lambda_l1': 3.060184062092395e-08,\n 'lambda_l2': 0.22368531324354038,\n 'bagging_fraction': 0.4,\n 'feature_fraction': 0.30000000000000004,\n 'seed': 2022\n}","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:59.035616Z","iopub.status.busy":"2022-06-28T18:53:59.034968Z","iopub.status.idle":"2022-06-28T18:53:59.041424Z","shell.execute_reply":"2022-06-28T18:53:59.040221Z"},"papermill":{"duration":0.035255,"end_time":"2022-06-28T18:53:59.043717","exception":false,"start_time":"2022-06-28T18:53:59.008462","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"130일 훈련 60일 테스트 로 6-fold 진행  \n총 1202일 - 190일로 6개로 나누고, 5개로 앙상블, 1개로 예측\n","metadata":{}},{"cell_type":"code","source":"def preprocess_train_lgb(df):\n    \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag']).set_index(['Date']).drop(columns=['Target'])\n\n    y = df.set_index('Date').loc[:, ['Target']]\n    \n    return dfc, y\n    \n\nX_lgb, y = preprocess_train_lgb(df_added)  # 2021-12-06부터 test 시작이므로 그 전까지만 이용한다.\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_lgb.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_board = dict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base model - lgbm - only 130 days for training\n# test - time-series cv 5-fold ensemble\nclass EnsembleModel:\n    def __init__(self, models: list):\n        self.models = models\n    \n    def predict(self, x):\n        predicted = np.zeros((len(x), len(self.models)))\n        for i, model in enumerate(self.models):\n            predicted[:, i] = model.predict(x)\n        return np.sum(predicted, axis=1) / len(self.models)\n        \ndef group_timeseriesCV(grid, train_size=200, test_size=60):\n    # 6-fold\n    cv_size = train_size + test_size\n    for i in range(4):\n        cv_train = grid[cv_size*i:cv_size*(i+1)][:train_size]\n        cv_test = grid[cv_size*i:cv_size*(i+1)][-test_size:]\n        yield cv_train, cv_test\n\n\ngrid = train['Date'].unique()[90:] #90일의 false data 고려하지 않기\ntscv = TimeSeriesSplit(n_splits=10, test_size=60)\n\nk = 0\nfeat_importance = pd.DataFrame()\nmodels = []\ncv_score = []\n# for train_idx, eval_idx in group_timeseriesCV(grid): #  #\n\nfor train_idx, eval_idx in tscv.split(grid):\n    train_idx = grid[train_idx]\n    eval_idx = grid[eval_idx]\n\n    k += 1\n    t0 = time.time()\n    print(f'========Training fold {k}========')\n   \n    print('training size:', len(train_idx), '  test size:', len(eval_idx))\n    \n    print(\"Train Date range: {} to {}\".format(train_idx.min(),train_idx.max()))\n    print(\"Valid Date range: {} to {}\".format(eval_idx.min(),eval_idx.max()))\n    \n\n    X, y = X_scaled.copy(), y.copy()\n    # X, y = X_lgb.copy(), y.copy()\n\n    X_train, X_test = X.loc[train_idx].reset_index(drop=True), X.loc[eval_idx].reset_index(drop=True)\n    y_train, y_test = y.loc[train_idx].reset_index(drop=True), y.loc[eval_idx].reset_index(drop=True)\n\n    \n    lgb = LGBMRegressor(device='GPU')\n    lgb.fit(\n        X_train, \n        y_train,\n        eval_set = [(X_test, y_test)],\n        callbacks=[\n                early_stopping(stopping_rounds=20)\n            ]\n        )\n    \n    models.append(lgb)\n\n    feat_importance[\"Importance_Fold\"+str(k)]=lgb.feature_importances_\n    feat_importance.set_index(X_train.columns, inplace=True)\n\n    y_pred = lgb.predict(X_test)\n   \n    \n    # Sharpe Ratio \n    df_sharpe = pd.DataFrame([])\n    df_sharpe['Date'] = X.loc[eval_idx].index\n    df_sharpe['predict'] = y_pred\n    df_sharpe['Target'] = y_test\n    df_sharpe['Rank'] = (df_sharpe.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n#         display(X_test.groupby('Date')['Rank'].min())\n#         display(X_test.groupby('Date')['Rank'].max())\n#         display(X_test)\n    eval_score = calc_spread_return_sharpe(df_sharpe)\n    cv_score.append(eval_score)\n\n    print(f'Fold {k} evaluated in {time.time() - t0 :.3f}s')\n    print(f'Fold {k} sharpe ratio score: {eval_score :.6f}')\n    \nprint(f'cv mean sharpe ratio score: {np.mean(cv_score) :.6f}')\n\n# score_type = 'scaled_6fold_basicfeatonly'\n# if score_type in score_board.keys():\n#     raise ValueError('key already exists')\n# else:\n#     score_board[score_type] = np.mean(cv_score)\n\n# lgb = LGBMRegressor(device='GPU',**params).fit(X_scaled, y)\n\n\n# feat importance plot\nfeat_importance['avg'] = feat_importance.mean(axis=1)\nfeat_importance = feat_importance.sort_values(by='avg',ascending=True)\npal=sns.color_palette(\"plasma_r\", 40).as_hex()[2:]\n\n\nfig=go.Figure()\nfor i in range(len(feat_importance.index)):\n    fig.add_shape(dict(type=\"line\", y0=i, y1=i, x0=0, x1=feat_importance['avg'][i], \n                       line_color=pal[::-1][i],opacity=0.7,line_width=4))\nfig.add_trace(go.Scatter(x=feat_importance['avg'], y=feat_importance.index, mode='markers', \n                         marker_color=pal[::-1], marker_size=8,\n                         hovertemplate='%{y} Importance = %{x:.0f}<extra></extra>'))\nfig.show()\n\n \n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble = EnsembleModel(models)\nensemble.predict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base model - lgbm - only 130 days for training\n# test - time-series cv 5-fold ensemble\nclass EnsembleModel:\n    def __init__(self, models: list):\n        self.models = models\n    \n    def predict(self, x):\n        predicted = np.zeros((len(x), len(self.models)))\n        for i, model in enumerate(self.models):\n            predicted[:, i] = model.predict(x)\n        return np.sum(predicted, axis=1) / len(self.models)\n        \ndef group_timeseriesCV(grid, train_size=200, test_size=60):\n    # 6-fold\n    cv_size = train_size + test_size\n    for i in range(4):\n        cv_train = grid[cv_size*i:cv_size*(i+1)][:train_size]\n        cv_test = grid[cv_size*i:cv_size*(i+1)][-test_size:]\n        yield cv_train, cv_test\n\n\ngrid = train['Date'].unique()[90:] #90일의 false data 고려하지 않기\ntscv = TimeSeriesSplit(n_splits=10, test_size=60)\n\nk = 0\nfeat_importance = pd.DataFrame()\n\ncv_score = []\n# for train_idx, eval_idx in group_timeseriesCV(grid): #  #\n\nfor train_idx, eval_idx in tscv.split(grid):\n    train_idx = grid[train_idx]\n    eval_idx = grid[eval_idx]\n\n    k += 1\n    t0 = time.time()\n    print(f'========Training fold {k}========')\n   \n    print('training size:', len(train_idx), '  test size:', len(eval_idx))\n    \n    print(\"Train Date range: {} to {}\".format(train_idx.min(),train_idx.max()))\n    print(\"Valid Date range: {} to {}\".format(eval_idx.min(),eval_idx.max()))\n    \n\n    X, y = X_scaled.copy(), y.copy()\n    # X, y = X_lgb.copy(), y.copy()\n\n    X_train, X_test = X.loc[train_idx].reset_index(drop=True), X.loc[eval_idx].reset_index(drop=True)\n    y_train, y_test = y.loc[train_idx].reset_index(drop=True), y.loc[eval_idx].reset_index(drop=True)\n\n    \n    # lgb = LGBMRegressor(device='GPU')\n    # lgb.fit(\n    #     X_train, \n    #     y_train,\n    #     eval_set = [(X_test, y_test)],\n    #     callbacks=[\n    #             early_stopping(stopping_rounds=20)\n    #         ]\n    #     )\n    \n    \n\n    # feat_importance[\"Importance_Fold\"+str(k)]=lgb.feature_importances_\n    # feat_importance.set_index(X_train.columns, inplace=True)\n\n    y_pred = ensemble.predict(X_test)\n   \n    \n    # Sharpe Ratio \n    df_sharpe = pd.DataFrame([])\n    df_sharpe['Date'] = X.loc[eval_idx].index\n    df_sharpe['predict'] = y_pred\n    df_sharpe['Target'] = y_test\n    df_sharpe['Rank'] = (df_sharpe.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n#         display(X_test.groupby('Date')['Rank'].min())\n#         display(X_test.groupby('Date')['Rank'].max())\n#         display(X_test)\n    eval_score = calc_spread_return_sharpe(df_sharpe)\n    cv_score.append(eval_score)\n\n    print(f'Fold {k} evaluated in {time.time() - t0 :.3f}s')\n    print(f'Fold {k} sharpe ratio score: {eval_score :.6f}')\n    \nprint(f'cv mean sharpe ratio score: {np.mean(cv_score) :.6f}')\n\n# score_type = 'scaled_6fold_basicfeatonly'\n# if score_type in score_board.keys():\n#     raise ValueError('key already exists')\n# else:\n#     score_board[score_type] = np.mean(cv_score)\n\n# lgb = LGBMRegressor(device='GPU',**params).fit(X_scaled, y)\n\n\n# feat importance plot\n# feat_importance['avg'] = feat_importance.mean(axis=1)\n# feat_importance = feat_importance.sort_values(by='avg',ascending=True)\n# pal=sns.color_palette(\"plasma_r\", 40).as_hex()[2:]\n\n\n# fig=go.Figure()\n# for i in range(len(feat_importance.index)):\n#     fig.add_shape(dict(type=\"line\", y0=i, y1=i, x0=0, x1=feat_importance['avg'][i], \n#                        line_color=pal[::-1][i],opacity=0.7,line_width=4))\n# fig.add_trace(go.Scatter(x=feat_importance['avg'], y=feat_importance.index, mode='markers', \n#                          marker_color=pal[::-1], marker_size=8,\n#                          hovertemplate='%{y} Importance = %{x:.0f}<extra></extra>'))\n# fig.show()\n\n \n\n\n","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:53:59.096807Z","iopub.status.busy":"2022-06-28T18:53:59.096419Z","iopub.status.idle":"2022-06-28T18:54:09.325796Z","shell.execute_reply":"2022-06-28T18:54:09.324793Z"},"papermill":{"duration":10.258961,"end_time":"2022-06-28T18:54:09.328611","exception":false,"start_time":"2022-06-28T18:53:59.069650","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance.index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_board","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del data, train, supp_data, y, X_scaled\n# gc.collect()","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:54:09.382108Z","iopub.status.busy":"2022-06-28T18:54:09.381679Z","iopub.status.idle":"2022-06-28T18:54:09.386180Z","shell.execute_reply":"2022-06-28T18:54:09.384990Z"},"papermill":{"duration":0.034025,"end_time":"2022-06-28T18:54:09.388527","exception":false,"start_time":"2022-06-28T18:54:09.354502","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# local-api\nsys.path.insert(0, '../input/jpx-local-api')\nfrom local_api import local_api\n\nmyapi = local_api('../input/jpx-tokyo-stock-exchange-prediction/supplemental_files', \n                 start_date='2021-11-29', end_date='2022-02-25')\nenv = myapi.make_env()\niter_test = env.iter_test()\nfor i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(tqdm(iter_test)):\n#     t0 = time.time()\n    \n    # 이전 데이터와 합치고 최근 140일치만 이용한다\n    today = prices.iloc[0]['Date']\n    lastday = str(pd.to_datetime(today) - pd.DateOffset(140))\n    \n    if i == 0:\n        close_df = pd.concat([\n            train_with_supp.loc[\n                (train_with_supp['Date'] > lastday) & (train_with_supp['Date'] < today), \n                    ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume']],\n                prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume']]\n            ]).reset_index(drop=True)\n        \n    else:\n        close_df = pd.concat([\n            close_df.loc[\n                (close_df['Date'] > lastday) & (close_df['Date'] < today), \n                    ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume']],\n                prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume']]\n            ]).reset_index(drop=True)\n        \n    # 최근 130개 데이터만 가지고 train 한다. \n    feat = add_features_infer(prices, close_df)\n    X = preprocess_inference(feat, trained_scalers)\n    \n    # display(X)\n    # X, y\n    X['predict'] = lgb.predict(X)\n    X['Rank'] = (X['predict'].rank(method='average', ascending=False)-1).astype(int)\n    sample_prediction['Rank'] = X['Rank'].values\n    \n    # check Rank\n    assert sample_prediction[\"Rank\"].notna().all()\n    assert sample_prediction[\"Rank\"].min() == 0\n    assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n    \n#     display(sample_prediction)\n    env.predict(sample_prediction)\n#     print(time.time() - t0)","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:54:09.442461Z","iopub.status.busy":"2022-06-28T18:54:09.441294Z","iopub.status.idle":"2022-06-28T18:54:09.448945Z","shell.execute_reply":"2022-06-28T18:54:09.447723Z"},"papermill":{"duration":0.03748,"end_time":"2022-06-28T18:54:09.451387","exception":false,"start_time":"2022-06-28T18:54:09.413907","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.score()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# test_data = train_with_supp.copy()\n\n# test_sec_list = divideSecurities(test_data)\n# test_df_added = add_features_train(test_sec_list)\n\n# test_X = preprocess_inference(test_df_added, trained_scalers)\n# test_X = test_X[(test_X.index <= '2022-02-25') & (test_X.index >= '2021-11-29')]\n# test_y = test_df_added.loc[(test_df_added['Date'] <= '2022-02-25') &\n#                            (test_df_added['Date'] >= '2021-11-29'), 'Target']\n\n\n# test_X['predict'] = lgb.predict(test_X)\n# test_X['Target'] = test_y.values\n# test_X['Rank'] = (test_X.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n# calc_spread_return_sharpe(test_X)","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:54:09.504529Z","iopub.status.busy":"2022-06-28T18:54:09.503839Z","iopub.status.idle":"2022-06-28T18:54:09.508266Z","shell.execute_reply":"2022-06-28T18:54:09.507425Z"},"papermill":{"duration":0.033305,"end_time":"2022-06-28T18:54:09.510448","exception":false,"start_time":"2022-06-28T18:54:09.477143","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pd.to_datetime 최소한도로 쓸것. 연산 cost가 너무 큼. 특히 inference 단계에서는 쓰지 않기.","metadata":{"papermill":{"duration":0.025011,"end_time":"2022-06-28T18:54:09.561001","exception":false,"start_time":"2022-06-28T18:54:09.535990","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# import jpx_tokyo_market_prediction\n# env = jpx_tokyo_market_prediction.make_env()\n# iter_test = env.iter_test()","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:54:09.614073Z","iopub.status.busy":"2022-06-28T18:54:09.613407Z","iopub.status.idle":"2022-06-28T18:54:09.641687Z","shell.execute_reply":"2022-06-28T18:54:09.639591Z"},"papermill":{"duration":0.058964,"end_time":"2022-06-28T18:54:09.645352","exception":false,"start_time":"2022-06-28T18:54:09.586388","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n# #     t0 = time.time()\n    \n#     # 이전 데이터와 합치고 최근 140일치만 이용한다\n#     today = prices.iloc[0]['Date']\n#     lastday = str(pd.to_datetime(today) - pd.DateOffset(140))\n    \n#     if i == 0:\n#         close_df = pd.concat([\n#             train_with_supp.loc[\n#                 (train_with_supp['Date'] > lastday) & (train_with_supp['Date'] < today), \n#                     ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume']],\n#                 prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume']]\n#             ]).reset_index(drop=True)\n        \n#     else:\n#         close_df = pd.concat([\n#             close_df.loc[\n#                 (close_df['Date'] > lastday) & (close_df['Date'] < today), \n#                     ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume']],\n#                 prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume']]\n#             ]).reset_index(drop=True)\n        \n\n#     feat = add_features_infer(prices, close_df)\n#     X = preprocess_inference(feat, trained_scalers)\n\n#     # X, y\n#     X['predict'] = lgb.predict(X)\n#     X['Rank'] = (X['predict'].rank(method='first', ascending=False)-1).astype(int)\n#     sample_prediction['Rank'] = X['Rank'].values\n    \n#     # check Rank\n#     assert sample_prediction[\"Rank\"].notna().all()\n#     assert sample_prediction[\"Rank\"].min() == 0\n#     assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n    \n# #     display(sample_prediction)\n#     env.predict(sample_prediction)\n# #     print(time.time() - t0)","metadata":{"execution":{"iopub.execute_input":"2022-06-28T18:54:09.704301Z","iopub.status.busy":"2022-06-28T18:54:09.703201Z","iopub.status.idle":"2022-06-28T18:55:05.921119Z","shell.execute_reply":"2022-06-28T18:55:05.919762Z"},"papermill":{"duration":56.248233,"end_time":"2022-06-28T18:55:05.924392","exception":false,"start_time":"2022-06-28T18:54:09.676159","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.027529,"end_time":"2022-06-28T18:55:05.978468","exception":false,"start_time":"2022-06-28T18:55:05.950939","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}