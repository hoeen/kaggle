{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-28T17:26:35.919390Z","iopub.execute_input":"2022-06-28T17:26:35.919786Z","iopub.status.idle":"2022-06-28T17:26:35.947048Z","shell.execute_reply.started":"2022-06-28T17:26:35.919756Z","shell.execute_reply":"2022-06-28T17:26:35.945924Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/sample_submission.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/options.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/financials.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/secondary_stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/trades.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/jpx_tokyo_market_prediction/competition.cpython-37m-x86_64-linux-gnu.so\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/jpx_tokyo_market_prediction/__init__.py\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/stock_fin_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/trades_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/stock_price_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/options_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/stock_list_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/options.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/financials.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/secondary_stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/trades.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/options.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/financials.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/secondary_stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/trades.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# %cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:36.030998Z","iopub.execute_input":"2022-06-28T17:26:36.031824Z","iopub.status.idle":"2022-06-28T17:26:36.036826Z","shell.execute_reply.started":"2022-06-28T17:26:36.031777Z","shell.execute_reply":"2022-06-28T17:26:36.035901Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'train_alot_of_features.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:36.170624Z","iopub.execute_input":"2022-06-28T17:26:36.172818Z","iopub.status.idle":"2022-06-28T17:26:36.177680Z","shell.execute_reply.started":"2022-06-28T17:26:36.172761Z","shell.execute_reply":"2022-06-28T17:26:36.176408Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\nimport talib as ta ","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:36.304841Z","iopub.execute_input":"2022-06-28T17:26:36.306791Z","iopub.status.idle":"2022-06-28T17:26:47.486616Z","shell.execute_reply.started":"2022-06-28T17:26:36.306730Z","shell.execute_reply":"2022-06-28T17:26:47.485259Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from talib-binary==0.4.19) (1.21.6)\nInstalling collected packages: talib-binary\nSuccessfully installed talib-binary-0.4.19\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import early_stopping\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler\nimport missingno as msno\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit \n\nimport statsmodels.api as sm\nfrom pylab import rcParams\n\nfrom tqdm import tqdm\n\nimport optuna\nfrom optuna.integration import LightGBMPruningCallback\n\nfrom talib import abstract\n\nimport plotly.graph_objects as go\n\nimport time\nimport gc\nimport sys\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n# sys.path.insert(0, '../input/jpx-local-api')\n# from local_api import local_api","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:47.488733Z","iopub.execute_input":"2022-06-28T17:26:47.489102Z","iopub.status.idle":"2022-06-28T17:26:47.503372Z","shell.execute_reply.started":"2022-06-28T17:26:47.489068Z","shell.execute_reply":"2022-06-28T17:26:47.502489Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_ta_features(df, inf=None, train=True):\n    \"\"\"\n    Get technical features from TA-Lib\n    ref : https://www.kaggle.com/code/daosword/jpx-pytorch-neural-network-with-ta-lib-features\n    \"\"\"\n    if train:\n        op = df['Open']\n        hi = df['High']\n        lo = df['Low']\n        cl = df['Close']\n        vo = df['Volume']\n\n    #     # Overlap Studies\n    #     df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n    #     df['DEMA'] = ta.DEMA(cl, timeperiod=30)\n        \n        df['EMA7'] = ta.EMA(cl, 7)/cl\n        df['EMA15'] = ta.EMA(cl, 15)/cl\n        df['EMA30'] = ta.EMA(cl, 30)/cl\n        df['EMA90'] = ta.EMA(cl, 90)/cl\n\n    #     df['HT_TRENDLINE'] = ta.HT_TRENDLINE(cl)\n    #     df['KAMA'] = ta.KAMA(cl, timeperiod=30)\n    #     df['MA'] = ta.MA(cl, timeperiod=30, matype=0)\n    #     df['MIDPOINT'] = ta.MIDPOINT(cl, timeperiod=14)\n    #     df['SAR'] = ta.SAR(hi, lo, acceleration=0, maximum=0)\n    #     df['SAREXT'] = ta.SAREXT(hi, lo, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n    #     df['SMA'] = ta.SMA(cl, timeperiod=30)\n    #     df['T3'] = ta.T3(df['Close'], timeperiod=5, vfactor=0)\n    #     df['TEMA'] = ta.TEMA(df['Close'], timeperiod=30)\n    #     df['TRIMA'] = ta.TRIMA(df['Close'], timeperiod=30)\n    #     df['WMA'] = ta.WMA(df['Close'], timeperiod=30)\n\n        # Momentum Indicators\n    #     df['ADX'] = ta.ADX(hi, lo, cl, timeperiod=14)\n    #     df['ADXR'] = ta.ADXR(hi, lo, cl, timeperiod=14)\n    #     df['APO'] = ta.APO(cl, fastperiod=12, slowperiod=26, matype=0)\n    #     df['AROON_down'], df['AROON_up'] = ta.AROON(hi, lo, timeperiod=14)\n    #     df['AROONOSC'] = ta.AROONOSC(hi, lo, timeperiod=14)\n    #     df['BOP'] = ta.BOP(op, hi, lo, cl)\n    #     df['CCI'] = ta.CCI(hi, lo, cl, timeperiod=14)\n    #     df['DX'] = ta.DX(hi, lo, cl, timeperiod=14)\n    #     df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = ta.MACD(cl, fastperiod=12, slowperiod=26, signalperiod=9)\n    #     df['MFI'] = ta.MFI(hi, lo, cl, vo, timeperiod=14)\n    #     df['MINUS_DI'] = ta.MINUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['MINUS_DM'] = ta.MINUS_DM(hi, lo, timeperiod=14)\n        df['MOM'] = ta.MOM(cl, timeperiod=10)\n    #     df['PLUS_DI'] = ta.PLUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['PLUS_DM'] = ta.PLUS_DM(hi, lo, timeperiod=14)\n        df['RSI'] = ta.RSI(cl, timeperiod=14)\n        df['STOCH_slowk'], df['STOCH_slowd'] = ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n        df['STOCHF_fastk'], df['STOCHF_fastd'] = ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)\n        df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\n    #     df['TRIX'] = ta.TRIX(cl, timeperiod=30)\n    #     df['ULTOSC'] = ta.ULTOSC(hi, lo, cl, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n    #     df['WILLR'] = ta.WILLR(hi, lo, cl, timeperiod=14)\n\n        # Volume Indicators\n    #     df['AD'] = ta.AD(hi, lo, cl, vo)\n    #     df['ADOSC'] = ta.ADOSC(hi, lo, cl, vo, fastperiod=3, slowperiod=10)\n    #     df['OBV'] = ta.OBV(cl, vo)\n\n        # Volatility Indicators\n        df['ATR'] = ta.ATR(hi, lo, cl, timeperiod=14)\n        df['NATR'] = ta.NATR(hi, lo, cl, timeperiod=14)\n        df['TRANGE'] = ta.TRANGE(hi, lo, cl)\n\n        # Cycle Indicators\n    #     df['HT_DCPERIOD'] = ta.HT_DCPERIOD(cl)\n    #     df['HT_DCPHASE'] = ta.HT_DCPHASE(cl)\n    #     df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = ta.HT_PHASOR(cl)\n    #     df['HT_SINE_sine'], df['HT_SINE_leadsine'] = ta.HT_SINE(cl)\n    #     df['HT_TRENDMODE'] = ta.HT_TRENDMODE(cl)\n\n        # Statistic Functions\n    #     df['BETA'] = ta.BETA(hi, lo, timeperiod=5)\n    #     df['CORREL'] = ta.CORREL(hi, lo, timeperiod=30)\n    #     df['LINEARREG'] = ta.LINEARREG(cl, timeperiod=14) - cl\n    #     df['LINEARREG_ANGLE'] = ta.LINEARREG_ANGLE(cl, timeperiod=14)\n    #     df['LINEARREG_INTERCEPT'] = ta.LINEARREG_INTERCEPT(cl, timeperiod=14) - cl\n    #     df['LINEARREG_SLOPE'] = ta.LINEARREG_SLOPE(cl, timeperiod=14)\n        df['STDDEV'] = ta.STDDEV(cl, timeperiod=5, nbdev=1)   \n    \n    \n    else: #inference \n        op = inf['Open']\n        hi = inf['High']\n        lo = inf['Low']\n        cl = inf['Close']\n        vo = inf['Volume']\n        \n            #     # Overlap Studies\n    #     df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n    #     df['DEMA'] = ta.DEMA(cl, timeperiod=30)\n\n        \n        df['EMA7'] = (ta.EMA(cl, 7)/cl).iloc[-1]\n        df['EMA15'] = (ta.EMA(cl, 15)/cl).iloc[-1]\n        df['EMA30'] = (ta.EMA(cl, 30)/cl).iloc[-1]\n        df['EMA90'] = (ta.EMA(cl, 90)/cl).iloc[-1]\n\n    #     df['HT_TRENDLINE'] = ta.HT_TRENDLINE(cl)\n    #     df['KAMA'] = ta.KAMA(cl, timeperiod=30)\n    #     df['MA'] = ta.MA(cl, timeperiod=30, matype=0)\n    #     df['MIDPOINT'] = ta.MIDPOINT(cl, timeperiod=14)\n    #     df['SAR'] = ta.SAR(hi, lo, acceleration=0, maximum=0)\n    #     df['SAREXT'] = ta.SAREXT(hi, lo, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n    #     df['SMA'] = ta.SMA(cl, timeperiod=30)\n    #     df['T3'] = ta.T3(df['Close'], timeperiod=5, vfactor=0)\n    #     df['TEMA'] = ta.TEMA(df['Close'], timeperiod=30)\n    #     df['TRIMA'] = ta.TRIMA(df['Close'], timeperiod=30)\n    #     df['WMA'] = ta.WMA(df['Close'], timeperiod=30)\n\n        # Momentum Indicators\n    #     df['ADX'] = ta.ADX(hi, lo, cl, timeperiod=14)\n    #     df['ADXR'] = ta.ADXR(hi, lo, cl, timeperiod=14)\n    #     df['APO'] = ta.APO(cl, fastperiod=12, slowperiod=26, matype=0)\n    #     df['AROON_down'], df['AROON_up'] = ta.AROON(hi, lo, timeperiod=14)\n    #     df['AROONOSC'] = ta.AROONOSC(hi, lo, timeperiod=14)\n    #     df['BOP'] = ta.BOP(op, hi, lo, cl)\n    #     df['CCI'] = ta.CCI(hi, lo, cl, timeperiod=14)\n    #     df['DX'] = ta.DX(hi, lo, cl, timeperiod=14)\n    #     df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = ta.MACD(cl, fastperiod=12, slowperiod=26, signalperiod=9)\n    #     df['MFI'] = ta.MFI(hi, lo, cl, vo, timeperiod=14)\n    #     df['MINUS_DI'] = ta.MINUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['MINUS_DM'] = ta.MINUS_DM(hi, lo, timeperiod=14)\n        df['MOM'] = ta.MOM(cl, timeperiod=10).iloc[-1]\n    #     df['PLUS_DI'] = ta.PLUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['PLUS_DM'] = ta.PLUS_DM(hi, lo, timeperiod=14)\n        df['RSI'] = ta.RSI(cl, timeperiod=14).iloc[-1]\n        df['STOCH_slowk'], df['STOCH_slowd'] = (ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)[0].iloc[-1],\n                                                ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)[1].iloc[-1])\n        df['STOCHF_fastk'], df['STOCHF_fastd'] = (ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)[0].iloc[-1],\n                                                  ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)[1].iloc[-1])\n        df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = (ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[0].iloc[-1],\n                                                      ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[1].iloc[-1])\n    #     df['TRIX'] = ta.TRIX(cl, timeperiod=30)\n    #     df['ULTOSC'] = ta.ULTOSC(hi, lo, cl, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n    #     df['WILLR'] = ta.WILLR(hi, lo, cl, timeperiod=14)\n\n        # Volume Indicators\n    #     df['AD'] = ta.AD(hi, lo, cl, vo)\n    #     df['ADOSC'] = ta.ADOSC(hi, lo, cl, vo, fastperiod=3, slowperiod=10)\n    #     df['OBV'] = ta.OBV(cl, vo)\n\n        # Volatility Indicators\n        df['ATR'] = ta.ATR(hi, lo, cl, timeperiod=14).iloc[-1]\n        df['NATR'] = ta.NATR(hi, lo, cl, timeperiod=14).iloc[-1]\n        df['TRANGE'] = ta.TRANGE(hi, lo, cl).iloc[-1]\n\n        # Cycle Indicators\n    #     df['HT_DCPERIOD'] = ta.HT_DCPERIOD(cl)\n    #     df['HT_DCPHASE'] = ta.HT_DCPHASE(cl)\n    #     df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = ta.HT_PHASOR(cl)\n    #     df['HT_SINE_sine'], df['HT_SINE_leadsine'] = ta.HT_SINE(cl)\n    #     df['HT_TRENDMODE'] = ta.HT_TRENDMODE(cl)\n\n        # Statistic Functions\n    #     df['BETA'] = ta.BETA(hi, lo, timeperiod=5)\n    #     df['CORREL'] = ta.CORREL(hi, lo, timeperiod=30)\n    #     df['LINEARREG'] = ta.LINEARREG(cl, timeperiod=14) - cl\n    #     df['LINEARREG_ANGLE'] = ta.LINEARREG_ANGLE(cl, timeperiod=14)\n    #     df['LINEARREG_INTERCEPT'] = ta.LINEARREG_INTERCEPT(cl, timeperiod=14) - cl\n    #     df['LINEARREG_SLOPE'] = ta.LINEARREG_SLOPE(cl, timeperiod=14)\n        df['STDDEV'] = ta.STDDEV(cl, timeperiod=5, nbdev=1).iloc[-1]   \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:47.505357Z","iopub.execute_input":"2022-06-28T17:26:47.505830Z","iopub.status.idle":"2022-06-28T17:26:47.531540Z","shell.execute_reply.started":"2022-06-28T17:26:47.505787Z","shell.execute_reply":"2022-06-28T17:26:47.530112Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')\n# display(data)\n\n\n# using supplement data as test data\nsupp_data = pd.read_csv('/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv')\n\n# train_with_supp = pd.concat([train, supp_data]).reset_index(drop=True)\n# train_with_supp = train.copy()\n# train_with_supp\n\n# TA Features\n# train","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:47.534801Z","iopub.execute_input":"2022-06-28T17:26:47.535158Z","iopub.status.idle":"2022-06-28T17:26:53.894322Z","shell.execute_reply.started":"2022-06-28T17:26:47.535127Z","shell.execute_reply":"2022-06-28T17:26:53.893087Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def divideSecurities(df):\n    sec_list = []\n    print('Divide securities individually..')\n    for code in np.sort(df.SecuritiesCode.unique()):\n        sec_list.append(df.loc[df.SecuritiesCode == code, :].reset_index(drop=True))\n    return sec_list\n\n# sec_list = divideSecurities(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:53.895678Z","iopub.execute_input":"2022-06-28T17:26:53.895997Z","iopub.status.idle":"2022-06-28T17:26:53.902224Z","shell.execute_reply.started":"2022-06-28T17:26:53.895968Z","shell.execute_reply":"2022-06-28T17:26:53.901158Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def add_features_train(sec_list):\n    df_list = []\n    for df in tqdm(sec_list):\n        \n        # shadows\n        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n        \n\n        ## Rolling features ##\n        \n        # lagged features\n        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n        # lagged close, target (target 은 정확히 무엇? return인가)\n        \n        # lagged feature 계산하기 전 결측치 채워넣기\n        df = df.fillna(method='ffill')\n        \n        \n        \n        # All indicators in ta\n        try:\n            df = get_ta_features(df.copy())\n        except:\n            print(f\"error in SecuritiesCode: {df['SecuritiesCode'][0]}\")\n            display(df)\n            continue\n                \n        # not add pattern recognition - feature importance 가 거의 0임. \n#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n\n\n        ####  For test!! df['STOCHF_fastd'] df['STOCHRSI_fastk'] df['ATR']\n        ####  open high low close 등 제거\n        df = df.drop(columns=['STOCHF_fastd', 'STOCHRSI_fastk', 'ATR', 'Open', 'High', 'Low', 'Close'])\n\n        # fill ema features by backward -- 이렇게 채워진 것은 false data 이므로 일단 test 해보고 없애는 것을 검토하자.\n        df = df.fillna(method='bfill')\n\n    \n        # volatility\n        \n        df_list.append(df)\n        \n        del df\n        \n    gc.collect()\n    df_feature_added = pd.concat(df_list).sort_values(['Date','SecuritiesCode']).set_index('Date')\n    \n    return df_feature_added\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:53.903517Z","iopub.execute_input":"2022-06-28T17:26:53.903823Z","iopub.status.idle":"2022-06-28T17:26:53.914846Z","shell.execute_reply.started":"2022-06-28T17:26:53.903797Z","shell.execute_reply":"2022-06-28T17:26:53.913731Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#         else:\n#             df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:53.916377Z","iopub.execute_input":"2022-06-28T17:26:53.916742Z","iopub.status.idle":"2022-06-28T17:26:53.933256Z","shell.execute_reply.started":"2022-06-28T17:26:53.916712Z","shell.execute_reply":"2022-06-28T17:26:53.932102Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def add_features_infer(input_df, close_df): #input df 는 price 데이터\n    \n    df_list = []\n    \n    print('Divide input securities...')\n    sec_list = divideSecurities(input_df)\n    print('Divide close_df securities...')\n    close_list = divideSecurities(close_df) # for rolling features\n    print('='*10 + 'feature adding' + '='*10)\n    for i in range(len(sec_list)):\n        \n        if i == 0:\n            t0 = time.time()\n        close = close_list[i] #.loc[close_df.SecuritiesCode == code, :].fillna(method='ffill')\n        df = sec_list[i]\n#         display(df)\n        # test data의 open, high, low, close 중 nan 있으면 이전 값에서 가져와 채움\n        if df.loc[:, ['Open', 'High', 'Low', 'Close', 'Volume']].isna().any().any():\n            df.loc[:, ['Open', 'High', 'Low', 'Close', 'Volume']] = close.loc[close['Date'] == close.iloc[-1]['Date'], ['Open', 'High', 'Low', 'Close', 'Volume']].values \n        \n        if i == 0:\n            t1 = time.time()\n            print(t1 - t0, 's')\n        # shadows\n        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n       \n        if i == 0:\n            t2 = time.time()\n            print(t2 - t1, 's')\n            \n        # lagged features\n        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n        # lagged close, target (target 은 정확히 무엇? return인가)\n        \n        ## Rolling features ##\n        # TA-lib features - RSI, EMA 7-90\n        df = get_ta_features(df, close, train=False)\n\n        if i == 0:\n            t3 = time.time()\n            print(t3 - t2, 's')\n        \n#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n\n\n        # volatility\n        \n        df_list.append(df)\n    \n    df_feature_added = pd.concat(df_list)\n    print('='*10 + 'feature added' + '='*10)\n    return df_feature_added\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:53.934635Z","iopub.execute_input":"2022-06-28T17:26:53.935145Z","iopub.status.idle":"2022-06-28T17:26:53.949296Z","shell.execute_reply.started":"2022-06-28T17:26:53.935099Z","shell.execute_reply":"2022-06-28T17:26:53.948484Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def preprocess_train(df):\n    \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n#     minmax = MinMaxScaler()\n    stdsc = StandardScaler()\n    ordinal = OrdinalEncoder()\n\n    target = ['Target']\n#     minmax_features = ['Date']\n    ord_features = ['SecuritiesCode'] \n#     scaled_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'upper_shadow', 'lower_shadow',\n#                       'RSI', 'EMA7', 'EMA15', 'EMA30', 'EMA90'] + [c for c in df.columns if c.startswith('CDL')] # pattern recognition features\n    scaled_features = [i for i in df.columns if i not in ['Date', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag',\n                                                         'Target', 'SecuritiesCode']]\n    \n    \n#     date_scaled = minmax.fit_transform(dfc.loc[:,minmax_features])\n    date_code_ord = ordinal.fit_transform(dfc.loc[:,ord_features])\n    scaled = stdsc.fit_transform(dfc.loc[:,scaled_features])\n    \n#     display(pd.DataFrame(date_code_ord, columns=ord_features))\n#     display(pd.DataFrame(scaled, columns=scaled_features))\n    \n    \n    dfc_scaled = pd.concat([# pd.DataFrame(date_scaled, columns=minmax_features),\n                            pd.DataFrame(date_code_ord, columns=ord_features),\n                            pd.DataFrame(scaled, columns=scaled_features)], \n                            axis=1)\n#     dfc_scaled = pd.concat([df['Date'].reset_index(drop=True), dfc_scaled],\n#                            axis=1)\n#     dfc_scaled = dfc_scaled.set_index(['Date'])\n\n#     y = dfc.loc[:, ['Target']]\n    \n    \n    return dfc_scaled, [ordinal, stdsc]\n    \n\n# X_scaled, y, trained_scalers = preprocess_train(df_added)  # 2021-12-06부터 test 시작이므로 그 전까지만 이용한다.\n\n# X_scaled\n# y","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:53.950639Z","iopub.execute_input":"2022-06-28T17:26:53.951147Z","iopub.status.idle":"2022-06-28T17:26:53.966714Z","shell.execute_reply.started":"2022-06-28T17:26:53.951116Z","shell.execute_reply":"2022-06-28T17:26:53.965176Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def preprocess_inference(df, trained_scalers: list):\n    ordinal = trained_scalers[0]\n    stdsc = trained_scalers[1]\n    \n      \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n    target = ['Target']\n    ord_features = ['SecuritiesCode'] \n#     scaled_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'upper_shadow', 'lower_shadow',\n#                       'RSI', 'EMA7', 'EMA15', 'EMA30', 'EMA90'] + [c for c in df.columns if c.startswith('CDL')] # pattern recognition features\n    scaled_features = [i for i in df.columns if i not in ['Date', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag',\n                                                         'Target', 'SecuritiesCode']]\n    \n    date_code_ord = ordinal.transform(dfc.loc[:,ord_features])\n    scaled = stdsc.transform(dfc.loc[:,scaled_features])\n    dfc_scaled = pd.concat([pd.DataFrame(date_code_ord, columns=ord_features),\n                            pd.DataFrame(scaled, columns=scaled_features)], axis=1)\n    \n#     dfc_scaled = pd.concat([df['Date'].reset_index(drop=True), dfc_scaled],\n#                            axis=1)\n#     dfc_scaled = dfc_scaled.set_index(['Date'])\n\n    \n    \n    return dfc_scaled\n    \n\n# X_test_scaled = preprocess_train(df_added, trained_scalers)\n\n# X_test_scaled","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:53.971170Z","iopub.execute_input":"2022-06-28T17:26:53.972022Z","iopub.status.idle":"2022-06-28T17:26:53.981881Z","shell.execute_reply.started":"2022-06-28T17:26:53.971985Z","shell.execute_reply":"2022-06-28T17:26:53.980763Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation - Sharpe ratio\n\ntimeseriesCV - mean sharpe ratio 계산  ","metadata":{}},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:53.984254Z","iopub.execute_input":"2022-06-28T17:26:53.984588Z","iopub.status.idle":"2022-06-28T17:26:54.001075Z","shell.execute_reply.started":"2022-06-28T17:26:53.984559Z","shell.execute_reply":"2022-06-28T17:26:53.999842Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Check stationary features","metadata":{}},{"cell_type":"markdown","source":"### test2 - without columns['STOCHF_fastd', 'STOCHRSI_fastk', 'ATR', 'Open', 'High', 'Low', 'Close'] - 더 향상된 cv score\n\n- Stationary features만 남기는게 중요한 것 같다.\n\n","metadata":{}},{"cell_type":"code","source":"# train_cv = train.set_index('Date')\n# eval_cv = supp_data.set_index('Date')\ntrain_with_supp = pd.concat([train, supp_data]).reset_index(drop=True)\ngrid = train_with_supp['Date'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:26:54.002265Z","iopub.execute_input":"2022-06-28T17:26:54.002613Z","iopub.status.idle":"2022-06-28T17:26:54.643586Z","shell.execute_reply.started":"2022-06-28T17:26:54.002583Z","shell.execute_reply":"2022-06-28T17:26:54.642351Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# # cv - test 3 month period\n# # test2 - without columns=['STOCHF_fastd', 'STOCHRSI_fastk', 'ATR', 'Open', 'High', 'Low', 'Close']\n\n# # evaluate on each CV\n# def evaluate_test2(train, val):\n#     global feat_importance#, fold\n#     # train model\n#     sec_list = divideSecurities(train)\n#     df_added = add_features_train(sec_list)\n#     X_scaled, y, trained_scalers = preprocess_train(df_added)\n    \n#     # base model - lgbm \n#     lgb = LGBMRegressor()\n#     lgb.fit(X_scaled, y)\n    \n#     # predict evaluation data - same as train\n#     sec_list_val = divideSecurities(val)\n#     df_added_val = add_features_train(sec_list_val)\n#     X_scaled_val = preprocess_inference(df_added_val, trained_scalers)\n    \n#     y_pred = lgb.predict(X_scaled_val)\n#     val['predict'] = y_pred\n    \n#     feat_importance[\"Importance_Fold\"+str(k)]=lgb.feature_importances_\n#     feat_importance.set_index(X_scaled_val.columns, inplace=True)\n    \n#     # evaluate - eval set 일자별로 순위 계산하고, 이 결과를 바탕으로 sharpe ratio 계산\n#     eval_dates = val['Date'].unique()  # supp 일자 리스트\n\n#     predicted_df_list = []\n#     for i, date in enumerate(tqdm(eval_dates)):\n#         X = val[val['Date'] == date]\n#         # X, y\n#         X['Rank'] = (X['predict'].rank(method='first', ascending=False)-1).astype(int)\n\n#         # check Rank\n#         assert X[\"Rank\"].notna().all()\n#         assert X[\"Rank\"].min() == 0\n#         assert X[\"Rank\"].max() == len(X[\"Rank\"]) - 1\n#         predicted_df_list.append(X)\n\n#     predicted_df = pd.concat(predicted_df_list)\n# #     display(predicted_df)\n#     eval_score = calc_spread_return_sharpe(predicted_df)\n#     print('evaluated score:', eval_score)\n#     return eval_score\n\n\n# tscv = TimeSeriesSplit(n_splits=7, test_size=60)  # 2000개 주식이 모두 있는 기간이 300일 정도이다. 그래서 직전 기간까지 train하고 60일씩 5-fold로 쪼갬\n# # split input : index date\n# grid = np.concatenate([train['Date'].unique(), supp_data['Date'].unique()])\n\n\n\n# scores = []\n# k = 0\n# feat_importance = pd.DataFrame([])\n# for train_idx, eval_idx in tscv.split(grid):\n#     k += 1\n#     print(f'========Training fold {k}========')\n#     t0 = time.time()\n#     print('training size:', len(train_idx), '  test size:', len(eval_idx))\n    \n#     print(\"Train Date range: {} to {}\".format(grid[train_idx].min(),grid[train_idx].max()))\n#     print(\"Valid Date range: {} to {}\".format(grid[eval_idx].min(),grid[eval_idx].max()))\n    \n#     score = evaluate_test2(train_with_supp.loc[grid[train_idx]].reset_index(),\n#                      train_with_supp.loc[grid[eval_idx]].reset_index())\n#     print(f'Fold {k} evaluated in {time.time() - t0 :.3f}s')\n#     print(f'Fold {k} score: {score :.6f}')\n#     scores.append(score)\n    \n# print(scores)\n# print('cv mean score:', np.mean(scores), end='\\t')\n# print('cv std:', np.std(scores))\n\n\n# # test2 feature importances plot\n# feat_importance['avg'] = feat_importance.mean(axis=1)\n# feat_importance = feat_importance.sort_values(by='avg',ascending=True)\n# pal=sns.color_palette(\"plasma_r\", 29).as_hex()[2:]\n\n# fig=go.Figure()\n# for i in range(len(feat_importance.index)):\n#     fig.add_shape(dict(type=\"line\", y0=i, y1=i, x0=0, x1=feat_importance['avg'][i], \n#                        line_color=pal[::-1][i],opacity=0.7,line_width=4))\n# fig.add_trace(go.Scatter(x=feat_importance['avg'], y=feat_importance.index, mode='markers', \n#                          marker_color=pal[::-1], marker_size=8,))\n# #                          hovertemplate='%{y} Importance = %{x:.0f}<extra></extra>'))\n\n# fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T03:59:40.490114Z","iopub.execute_input":"2022-06-28T03:59:40.49047Z","iopub.status.idle":"2022-06-28T03:59:40.498349Z","shell.execute_reply.started":"2022-06-28T03:59:40.49043Z","shell.execute_reply":"2022-06-28T03:59:40.497542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature selection 결론?\n- case2 의 features 를 적극 이용\n- 나머지 case에서 상위 5개정도의 feature들을 이용\n- feature끼리의 상관계수 측정하여 너무 높은것은 뺌\n- target과의 상관계수도 살펴보기","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameter tuning using Optuna","metadata":{}},{"cell_type":"code","source":"# cv optimize 를 위해 train으로만 scaler 만든 후 전체 데이터에 적용\nsec_list = divideSecurities(train_with_supp)\ndf_added = add_features_train(sec_list)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:29:13.996389Z","iopub.execute_input":"2022-06-28T17:29:13.996836Z","iopub.status.idle":"2022-06-28T17:29:52.976053Z","shell.execute_reply.started":"2022-06-28T17:29:13.996799Z","shell.execute_reply":"2022-06-28T17:29:52.974706Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Divide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:25<00:00, 79.42it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def objective(trial, data, k=7):  # X_scaled, X_scaled_val, y, y_val\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        'objective': 'regression',\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [500, 1000, 2000, 4000, 5000, 10000]),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.5),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n#         \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.9, step=0.1\n        ),\n#         \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.9, step=0.1\n        ),\n        \"metric\": 'l2',\n        \"seed\": 2022\n    }\n\n    tscv = TimeSeriesSplit(n_splits=k, test_size=60)\n\n    cv_scores = []\n    for i, (train_idx, eval_idx) in enumerate(tscv.split(grid)):\n        print()\n        print(f'============== Fold {i+1} ================')\n        \n        X, y = data.drop(columns=['Target']), data[['Target']]\n        X_train, X_test = X.loc[grid[train_idx]].reset_index(drop=True), X.loc[grid[eval_idx]].reset_index(drop=True)\n        y_train, y_test = y.loc[grid[train_idx]].reset_index(drop=True), y.loc[grid[eval_idx]].reset_index(drop=True)\n        \n        print(\"Train Date range: {} to {}\".format(grid[train_idx].min(),grid[train_idx].max()))\n        print(\"Valid Date range: {} to {}\".format(grid[eval_idx].min(),grid[eval_idx].max()))\n        \n        X_train, trained_scalers = preprocess_train(X_train)\n        X_test = preprocess_inference(X_test, trained_scalers)\n        \n        \n        model = LGBMRegressor(**param_grid)\n        model.fit(\n            X_train,\n            y_train,\n        )\n        y_pred = model.predict(X_test)\n        \n#         cv_scores.append(mean_squared_error(y_test, y_pred))\n        \n        # test - optimize to maximize sharpe ratio\n        X_test['Date'] = X.loc[grid[eval_idx]].index\n        X_test['predict'] = y_pred\n        X_test['Target'] = y_test\n        X_test['Rank'] = (X_test.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n#         display(X_test.groupby('Date')['Rank'].min())\n#         display(X_test.groupby('Date')['Rank'].max())\n#         display(X_test)\n        eval_score = calc_spread_return_sharpe(X_test)\n        cv_scores.append(eval_score)\n    print('cv completed with scores:', cv_scores)\n    print('mean score:', np.mean(cv_scores))\n    return np.mean(cv_scores)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T12:31:15.869753Z","iopub.execute_input":"2022-06-28T12:31:15.870205Z","iopub.status.idle":"2022-06-28T12:31:15.890617Z","shell.execute_reply.started":"2022-06-28T12:31:15.870168Z","shell.execute_reply":"2022-06-28T12:31:15.889395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.logging.set_verbosity(optuna.logging.ERROR)\n\nstudy = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Regressor\")\nfunc = lambda trial: objective(trial, df_added, k=5)\nstudy.optimize(func, n_trials=20)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T12:31:17.085532Z","iopub.execute_input":"2022-06-28T12:31:17.086221Z","iopub.status.idle":"2022-06-28T12:48:02.057039Z","shell.execute_reply.started":"2022-06-28T12:31:17.086184Z","shell.execute_reply":"2022-06-28T12:48:02.05519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization = 30 trials\noptuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:32:32.955013Z","iopub.execute_input":"2022-06-28T04:32:32.95603Z","iopub.status.idle":"2022-06-28T04:32:32.978604Z","shell.execute_reply.started":"2022-06-28T04:32:32.955977Z","shell.execute_reply":"2022-06-28T04:32:32.977375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization - 20 trials\noptuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T12:52:41.866804Z","iopub.execute_input":"2022-06-28T12:52:41.869733Z","iopub.status.idle":"2022-06-28T12:52:42.076213Z","shell.execute_reply.started":"2022-06-28T12:52:41.869599Z","shell.execute_reply":"2022-06-28T12:52:42.075084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:18:32.377714Z","iopub.execute_input":"2022-06-28T04:18:32.378261Z","iopub.status.idle":"2022-06-28T04:18:32.388035Z","shell.execute_reply.started":"2022-06-28T04:18:32.378205Z","shell.execute_reply":"2022-06-28T04:18:32.386944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.trials_dataframe().sort_values('value', ascending=False).drop(\n        columns=['datetime_start', 'datetime_complete', 'duration'])","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:18:21.212608Z","iopub.execute_input":"2022-06-28T04:18:21.213071Z","iopub.status.idle":"2022-06-28T04:18:21.257288Z","shell.execute_reply.started":"2022-06-28T04:18:21.213022Z","shell.execute_reply":"2022-06-28T04:18:21.256333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.trials_dataframe().sort_values('value', ascending=False).drop(\n        columns=['datetime_start', 'datetime_complete', 'duration'])","metadata":{"execution":{"iopub.status.busy":"2022-06-28T04:32:32.981715Z","iopub.execute_input":"2022-06-28T04:32:32.98297Z","iopub.status.idle":"2022-06-28T04:32:33.019188Z","shell.execute_reply.started":"2022-06-28T04:32:32.982904Z","shell.execute_reply":"2022-06-28T04:32:33.018276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective_constant_period(trial, data, k=7):  # X_scaled, X_scaled_val, y, y_val\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        'objective': 'regression',\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [500, 1000, 2000, 4000, 5000, 10000]),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.5),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n#         \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.9, step=0.1\n        ),\n#         \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.9, step=0.1\n        ),\n        \"metric\": 'l2',\n        \"seed\": 2022\n    }\n\n    tscv = TimeSeriesSplit(n_splits=k, max_train_size=200, test_size=60)\n\n    cv_scores = []\n    for i, (train_idx, eval_idx) in enumerate(tscv.split(grid)):\n        print()\n        print(f'============== Fold {i+1} ================')\n        \n        X, y = data.drop(columns=['Target']), data[['Target']]\n        X_train, X_test = X.loc[grid[train_idx]].reset_index(drop=True), X.loc[grid[eval_idx]].reset_index(drop=True)\n        y_train, y_test = y.loc[grid[train_idx]].reset_index(drop=True), y.loc[grid[eval_idx]].reset_index(drop=True)\n        \n        print(\"Train Date range: {} to {}\".format(grid[train_idx].min(),grid[train_idx].max()))\n        print(\"Valid Date range: {} to {}\".format(grid[eval_idx].min(),grid[eval_idx].max()))\n        \n        X_train, trained_scalers = preprocess_train(X_train)\n        X_test = preprocess_inference(X_test, trained_scalers)\n        \n        \n        model = LGBMRegressor(**param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            callbacks=[\n#                 LightGBMPruningCallback(trial, \"l2\"),\n                early_stopping(stopping_rounds=20)\n            ],  # Add a pruning callback\n        )\n        y_pred = model.predict(X_test)\n        \n#         cv_scores.append(mean_squared_error(y_test, y_pred))\n        \n        # test - optimize to maximize sharpe ratio\n        X_test['Date'] = X.loc[grid[eval_idx]].index\n        X_test['predict'] = y_pred\n        X_test['Target'] = y_test\n        X_test['Rank'] = (X_test.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n#         display(X_test.groupby('Date')['Rank'].min())\n#         display(X_test.groupby('Date')['Rank'].max())\n#         display(X_test)\n        eval_score = calc_spread_return_sharpe(X_test)\n        cv_scores.append(eval_score)\n    print('cv completed with scores:', cv_scores)\n    print('mean score:', np.mean(cv_scores))\n    return np.mean(cv_scores)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T13:11:37.702044Z","iopub.execute_input":"2022-06-28T13:11:37.702444Z","iopub.status.idle":"2022-06-28T13:11:37.722766Z","shell.execute_reply.started":"2022-06-28T13:11:37.702413Z","shell.execute_reply":"2022-06-28T13:11:37.721598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.logging.set_verbosity(optuna.logging.ERROR)\n\nstudy = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Regressor\")\nfunc = lambda trial: objective_constant_period(trial, df_added, k=5)\nstudy.optimize(func, n_trials=20)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T13:11:38.091711Z","iopub.execute_input":"2022-06-28T13:11:38.092824Z","iopub.status.idle":"2022-06-28T13:22:17.006403Z","shell.execute_reply.started":"2022-06-28T13:11:38.092784Z","shell.execute_reply":"2022-06-28T13:22:17.005039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization - 20 trials\noptuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T13:23:45.671216Z","iopub.execute_input":"2022-06-28T13:23:45.671915Z","iopub.status.idle":"2022-06-28T13:23:45.703675Z","shell.execute_reply.started":"2022-06-28T13:23:45.67186Z","shell.execute_reply":"2022-06-28T13:23:45.702676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-06-28T13:29:21.772111Z","iopub.execute_input":"2022-06-28T13:29:21.772666Z","iopub.status.idle":"2022-06-28T13:29:21.784063Z","shell.execute_reply.started":"2022-06-28T13:29:21.772626Z","shell.execute_reply":"2022-06-28T13:29:21.782846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(train_size, data, k=7):  # X_scaled, X_scaled_val, y, y_val\n    \n\n    tscv = TimeSeriesSplit(n_splits=k, max_train_size=train_size, test_size=60)\n\n    cv_scores = []\n    for i, (train_idx, eval_idx) in enumerate(tscv.split(grid)):\n        print()\n        print(f'============== Fold {i+1} ================')\n        \n        X, y = data.drop(columns=['Target']), data[['Target']]\n        X_train, X_test = X.loc[grid[train_idx]].reset_index(drop=True), X.loc[grid[eval_idx]].reset_index(drop=True)\n        y_train, y_test = y.loc[grid[train_idx]].reset_index(drop=True), y.loc[grid[eval_idx]].reset_index(drop=True)\n        \n        print(\"Train Date range: {} to {}\".format(grid[train_idx].min(),grid[train_idx].max()))\n        print(\"Valid Date range: {} to {}\".format(grid[eval_idx].min(),grid[eval_idx].max()))\n        \n        X_train, trained_scalers = preprocess_train(X_train)\n        X_test = preprocess_inference(X_test, trained_scalers)\n        \n        \n        model = LGBMRegressor()\n        model.fit(\n            X_train,\n            y_train,\n            \n        )\n        y_pred = model.predict(X_test)\n        \n#         cv_scores.append(mean_squared_error(y_test, y_pred))\n        \n        # test - optimize to maximize sharpe ratio\n        X_test['Date'] = X.loc[grid[eval_idx]].index\n        X_test['predict'] = y_pred\n        X_test['Target'] = y_test\n        X_test['Rank'] = (X_test.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n#         display(X_test.groupby('Date')['Rank'].min())\n#         display(X_test.groupby('Date')['Rank'].max())\n#         display(X_test)\n        eval_score = calc_spread_return_sharpe(X_test)\n        cv_scores.append(eval_score)\n    print('cv completed with scores:', cv_scores)\n    print('mean score:', np.mean(cv_scores))\n    return np.mean(cv_scores)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:38:19.133287Z","iopub.execute_input":"2022-06-28T17:38:19.133688Z","iopub.status.idle":"2022-06-28T17:38:19.146934Z","shell.execute_reply.started":"2022-06-28T17:38:19.133649Z","shell.execute_reply":"2022-06-28T17:38:19.145975Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"xgrid = [30, 60, 90, 130, 180, 210, 300, 360, 500, 1000]\nmean_cv_scores = []\nfor train_time in xgrid:\n    mean_cv_scores.append(objective(train_time, df_added, k=5))\n    \nplt.plot(xgrid, mean_cv_scores)\nplt.xticks(xgrid)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:38:28.476636Z","iopub.execute_input":"2022-06-28T17:38:28.477046Z","iopub.status.idle":"2022-06-28T17:41:53.570507Z","shell.execute_reply.started":"2022-06-28T17:38:28.477012Z","shell.execute_reply":"2022-06-28T17:41:53.569194Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\n============== Fold 1 ================\nTrain Date range: 2021-01-20 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2021-04-16 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2021-07-15 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2021-10-14 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2022-01-13 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [-0.029390339449898963, 0.10967744218605219, -0.07246718524581296, 0.06283482527436286, 0.1172523613739899]\nmean score: 0.037581420827738604\n\n============== Fold 1 ================\nTrain Date range: 2020-12-04 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2021-03-05 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2021-06-03 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2021-08-31 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2021-11-29 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [0.022716190753770935, 0.07243368481534763, -0.023436623622257582, 0.2511433178679553, 0.14238921143347727]\nmean score: 0.0930491562496587\n\n============== Fold 1 ================\nTrain Date range: 2020-10-21 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2021-01-20 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2021-04-16 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2021-07-15 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2021-10-14 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [0.07532086475095322, -0.05079306694831718, 0.0420212347517397, 0.12000826808606617, 0.022405754055372918]\nmean score: 0.041792610939162964\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [0.31484296349768187, 0.008278534410239412, 0.008530025790924142, 0.09489519160099036, 0.06024650086271499]\nmean score: 0.09735864323251016\n\n============== Fold 1 ================\nTrain Date range: 2020-06-10 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2020-09-07 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2020-12-04 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2021-03-05 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2021-06-03 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [0.11325202385562202, -0.07937078937425257, 0.08353185642308134, -0.04090619395974517, 0.09924357372703106]\nmean score: 0.03515009413434733\n\n============== Fold 1 ================\nTrain Date range: 2020-04-23 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2020-07-22 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2020-10-21 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2021-01-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2021-04-16 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [-0.017660865535812423, -0.15990502088558647, 0.13875078598769647, 0.07278387916881617, 0.06350977077911434]\nmean score: 0.019495709902845614\n\n============== Fold 1 ================\nTrain Date range: 2019-12-09 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2020-03-11 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2020-06-10 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2020-09-07 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2020-12-04 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [0.09025070591309176, 0.055798832729678366, 0.00832175537554991, -0.08520124467387796, 0.20682186014942744]\nmean score: 0.05519838189877391\n\n============== Fold 1 ================\nTrain Date range: 2019-09-09 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2019-12-09 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2020-03-11 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2020-06-10 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2020-09-07 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [0.0023390882951008873, -0.030964479717787454, 0.03102283621161655, -0.13510732528442376, 0.2295010116910795]\nmean score: 0.019358226239117142\n\n============== Fold 1 ================\nTrain Date range: 2019-02-12 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2019-05-16 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2019-08-09 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2019-11-11 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2020-02-10 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [0.0628552297298809, -0.028922578942721273, -0.002736674622146028, -0.19085816325369104, -0.09014393719812623]\nmean score: -0.04996122485736073\n\n============== Fold 1 ================\nTrain Date range: 2017-01-30 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n\n============== Fold 2 ================\nTrain Date range: 2017-04-25 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n\n============== Fold 3 ================\nTrain Date range: 2017-07-24 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n\n============== Fold 4 ================\nTrain Date range: 2017-10-19 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n\n============== Fold 5 ================\nTrain Date range: 2018-01-19 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\ncv completed with scores: [0.1166708580365091, 0.017103201830398304, 0.013996220516682893, -0.22350019791241146, -0.05314924053891858]\nmean score: -0.02577583161354795\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"([<matplotlib.axis.XTick at 0x7f9034b9ec10>,\n  <matplotlib.axis.XTick at 0x7f9034b8db90>,\n  <matplotlib.axis.XTick at 0x7f9034274050>,\n  <matplotlib.axis.XTick at 0x7f90341e1dd0>,\n  <matplotlib.axis.XTick at 0x7f90341e1c50>,\n  <matplotlib.axis.XTick at 0x7f90341fb310>,\n  <matplotlib.axis.XTick at 0x7f9034b70210>,\n  <matplotlib.axis.XTick at 0x7f9034b70a10>,\n  <matplotlib.axis.XTick at 0x7f90341833d0>,\n  <matplotlib.axis.XTick at 0x7f9034183910>],\n [Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, '')])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.plot(xgrid, mean_cv_scores)\n\n# plt.xscale('log')\nplt.xticks(xgrid)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T13:45:10.917548Z","iopub.execute_input":"2022-06-28T13:45:10.917953Z","iopub.status.idle":"2022-06-28T13:45:11.111428Z","shell.execute_reply.started":"2022-06-28T13:45:10.917919Z","shell.execute_reply":"2022-06-28T13:45:11.110234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"130일에서 cv가 가장 높으니 130일로 설정해서 HP 튜닝 간다.","metadata":{}},{"cell_type":"code","source":"def objective(trial, data, k=7):  # X_scaled, X_scaled_val, y, y_val\n    param_grid = {\n        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        'objective': 'regression',\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [500, 1000, 2000, 4000, 5000, 10000]),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.5),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n#         \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.9, step=0.1\n        ),\n#         \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.9, step=0.1\n        ),\n        \"metric\": 'l2',\n        \"seed\": 2022\n    }\n\n    tscv = TimeSeriesSplit(n_splits=k, max_train_size=130, test_size=60)\n\n    cv_scores = []\n    for i, (train_idx, eval_idx) in enumerate(tscv.split(grid)):\n        print()\n        print(f'============== Fold {i+1} ================')\n        \n        X, y = data.drop(columns=['Target']), data[['Target']]\n        X_train, X_test = X.loc[grid[train_idx]].reset_index(drop=True), X.loc[grid[eval_idx]].reset_index(drop=True)\n        y_train, y_test = y.loc[grid[train_idx]].reset_index(drop=True), y.loc[grid[eval_idx]].reset_index(drop=True)\n        \n        print(\"Train Date range: {} to {}\".format(grid[train_idx].min(),grid[train_idx].max()))\n        print(\"Valid Date range: {} to {}\".format(grid[eval_idx].min(),grid[eval_idx].max()))\n        \n        X_train, trained_scalers = preprocess_train(X_train)\n        X_test = preprocess_inference(X_test, trained_scalers)\n        \n        \n        model = LGBMRegressor(**param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            verbose=True\n            \n        )\n        y_pred = model.predict(X_test)\n        \n#         cv_scores.append(mean_squared_error(y_test, y_pred))\n        \n        # test - optimize to maximize sharpe ratio\n        X_test['Date'] = X.loc[grid[eval_idx]].index\n        X_test['predict'] = y_pred\n        X_test['Target'] = y_test\n        X_test['Rank'] = (X_test.groupby('Date')['predict'].rank(method='first', ascending=False)-1).astype(int)\n#         display(X_test.groupby('Date')['Rank'].min())\n#         display(X_test.groupby('Date')['Rank'].max())\n#         display(X_test)\n        eval_score = calc_spread_return_sharpe(X_test)\n        cv_scores.append(eval_score)\n    print('cv completed with scores:', cv_scores)\n    print('mean score:', np.mean(cv_scores))\n    return np.mean(cv_scores)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:42:27.932984Z","iopub.execute_input":"2022-06-28T17:42:27.933431Z","iopub.status.idle":"2022-06-28T17:42:27.949336Z","shell.execute_reply.started":"2022-06-28T17:42:27.933379Z","shell.execute_reply":"2022-06-28T17:42:27.947665Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"optuna.logging.set_verbosity(optuna.logging.ERROR)\n\nstudy = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Regressor\")\nfunc = lambda trial: objective(trial, df_added, k=5)\nstudy.optimize(func, n_trials=10)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T17:42:38.636136Z","iopub.execute_input":"2022-06-28T17:42:38.636525Z","iopub.status.idle":"2022-06-28T18:05:09.750203Z","shell.execute_reply.started":"2022-06-28T17:42:38.636492Z","shell.execute_reply":"2022-06-28T18:05:09.748500Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=0.5919889619035699, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5919889619035699\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=9.570906874237072e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.570906874237072e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8100\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n[LightGBM] [Warning] lambda_l1 is set=0.5919889619035699, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5919889619035699\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=9.570906874237072e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.570906874237072e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8100\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n[LightGBM] [Warning] lambda_l1 is set=0.5919889619035699, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5919889619035699\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=9.570906874237072e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.570906874237072e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8100\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n[LightGBM] [Warning] lambda_l1 is set=0.5919889619035699, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5919889619035699\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=9.570906874237072e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.570906874237072e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8100\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\n[LightGBM] [Warning] lambda_l1 is set=0.5919889619035699, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5919889619035699\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=9.570906874237072e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.570906874237072e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8100\ncv completed with scores: [0.3315067202961218, 0.07401445430495623, 0.1276535778619834, -0.03191213021198674, 0.12031909974748459]\nmean score: 0.12431634439971187\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=3.508213002835734e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.508213002835734e-07\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=0.0007223379979171635, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0007223379979171635\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=8000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8000\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n[LightGBM] [Warning] lambda_l1 is set=3.508213002835734e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.508213002835734e-07\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=0.0007223379979171635, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0007223379979171635\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=8000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8000\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n[LightGBM] [Warning] lambda_l1 is set=3.508213002835734e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.508213002835734e-07\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=0.0007223379979171635, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0007223379979171635\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=8000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8000\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n[LightGBM] [Warning] lambda_l1 is set=3.508213002835734e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.508213002835734e-07\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=0.0007223379979171635, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0007223379979171635\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=8000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8000\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\n[LightGBM] [Warning] lambda_l1 is set=3.508213002835734e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.508213002835734e-07\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=0.0007223379979171635, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0007223379979171635\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=8000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8000\ncv completed with scores: [0.21000607085077908, 0.1459169498509996, 0.1270249410997218, 0.07058508903230559, 0.03751456172451697]\nmean score: 0.11820952251166461\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=6.155887155378549e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.155887155378549e-07\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] lambda_l2 is set=4.382896621581994, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.382896621581994\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=4700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4700\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n[LightGBM] [Warning] lambda_l1 is set=6.155887155378549e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.155887155378549e-07\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] lambda_l2 is set=4.382896621581994, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.382896621581994\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=4700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4700\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n[LightGBM] [Warning] lambda_l1 is set=6.155887155378549e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.155887155378549e-07\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] lambda_l2 is set=4.382896621581994, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.382896621581994\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=4700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4700\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n[LightGBM] [Warning] lambda_l1 is set=6.155887155378549e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.155887155378549e-07\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] lambda_l2 is set=4.382896621581994, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.382896621581994\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=4700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4700\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\n[LightGBM] [Warning] lambda_l1 is set=6.155887155378549e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.155887155378549e-07\n[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n[LightGBM] [Warning] lambda_l2 is set=4.382896621581994, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.382896621581994\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=4700, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4700\ncv completed with scores: [0.3163567793646173, 0.11953099025567836, 0.1216143319928097, 0.043562117553435045, 0.028095686667891924]\nmean score: 0.1258319811668865\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=0.01414221590554714, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01414221590554714\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=7.222249907865942e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.222249907865942e-07\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=5000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5000\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n[LightGBM] [Warning] lambda_l1 is set=0.01414221590554714, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01414221590554714\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=7.222249907865942e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.222249907865942e-07\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=5000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5000\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n[LightGBM] [Warning] lambda_l1 is set=0.01414221590554714, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01414221590554714\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=7.222249907865942e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.222249907865942e-07\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=5000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5000\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n[LightGBM] [Warning] lambda_l1 is set=0.01414221590554714, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01414221590554714\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=7.222249907865942e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.222249907865942e-07\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=5000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5000\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\n[LightGBM] [Warning] lambda_l1 is set=0.01414221590554714, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01414221590554714\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=7.222249907865942e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.222249907865942e-07\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=5000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5000\ncv completed with scores: [0.33251595606289663, 0.10886405278017676, 0.0926565088549256, 0.03057146630787637, 0.05553320236794474]\nmean score: 0.12402823727476402\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=1.4852091398067356e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4852091398067356e-06\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=1.43226239340328e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.43226239340328e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8800\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n[LightGBM] [Warning] lambda_l1 is set=1.4852091398067356e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4852091398067356e-06\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=1.43226239340328e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.43226239340328e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8800\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n[LightGBM] [Warning] lambda_l1 is set=1.4852091398067356e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4852091398067356e-06\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=1.43226239340328e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.43226239340328e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8800\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n[LightGBM] [Warning] lambda_l1 is set=1.4852091398067356e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4852091398067356e-06\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=1.43226239340328e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.43226239340328e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8800\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\n[LightGBM] [Warning] lambda_l1 is set=1.4852091398067356e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4852091398067356e-06\n[LightGBM] [Warning] bagging_fraction is set=0.30000000000000004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.30000000000000004\n[LightGBM] [Warning] lambda_l2 is set=1.43226239340328e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.43226239340328e-06\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=8800, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8800\ncv completed with scores: [0.2531232702727172, 0.11797147574979246, 0.11965667345414722, 0.035380424467422185, 0.08493414476228033]\nmean score: 0.12221319774127189\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=0.012904934713648564, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012904934713648564\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=0.08037525140652722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08037525140652722\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=3500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3500\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n[LightGBM] [Warning] lambda_l1 is set=0.012904934713648564, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012904934713648564\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=0.08037525140652722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08037525140652722\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=3500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3500\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n[LightGBM] [Warning] lambda_l1 is set=0.012904934713648564, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012904934713648564\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=0.08037525140652722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08037525140652722\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=3500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3500\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n[LightGBM] [Warning] lambda_l1 is set=0.012904934713648564, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012904934713648564\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=0.08037525140652722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08037525140652722\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=3500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3500\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\n[LightGBM] [Warning] lambda_l1 is set=0.012904934713648564, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012904934713648564\n[LightGBM] [Warning] bagging_fraction is set=0.2, subsample=1.0 will be ignored. Current value: bagging_fraction=0.2\n[LightGBM] [Warning] lambda_l2 is set=0.08037525140652722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08037525140652722\n[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n[LightGBM] [Warning] min_data_in_leaf is set=3500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3500\ncv completed with scores: [0.3518469583603313, 0.11975993736246056, 0.10558742295237825, 0.02206638227652524, 2.19550719137939e-05]\nmean score: 0.11985653120472184\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=3.060184062092395e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.060184062092395e-08\n[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n[LightGBM] [Warning] lambda_l2 is set=0.22368531324354038, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22368531324354038\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n[LightGBM] [Warning] lambda_l1 is set=3.060184062092395e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.060184062092395e-08\n[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n[LightGBM] [Warning] lambda_l2 is set=0.22368531324354038, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22368531324354038\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n[LightGBM] [Warning] lambda_l1 is set=3.060184062092395e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.060184062092395e-08\n[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n[LightGBM] [Warning] lambda_l2 is set=0.22368531324354038, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22368531324354038\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n[LightGBM] [Warning] lambda_l1 is set=3.060184062092395e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.060184062092395e-08\n[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n[LightGBM] [Warning] lambda_l2 is set=0.22368531324354038, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22368531324354038\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\n[LightGBM] [Warning] lambda_l1 is set=3.060184062092395e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.060184062092395e-08\n[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4\n[LightGBM] [Warning] lambda_l2 is set=0.22368531324354038, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.22368531324354038\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=500, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=500\ncv completed with scores: [0.2172642450383156, 0.13391860001672454, 0.13177071373103533, 0.17325765367338042, 0.044519885617551096]\nmean score: 0.14014621961540139\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=4.031471147672526e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.031471147672526e-06\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=0.5742659058240782, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5742659058240782\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1100\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n[LightGBM] [Warning] lambda_l1 is set=4.031471147672526e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.031471147672526e-06\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=0.5742659058240782, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5742659058240782\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1100\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n[LightGBM] [Warning] lambda_l1 is set=4.031471147672526e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.031471147672526e-06\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=0.5742659058240782, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5742659058240782\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1100\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n[LightGBM] [Warning] lambda_l1 is set=4.031471147672526e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.031471147672526e-06\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=0.5742659058240782, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5742659058240782\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1100\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\n[LightGBM] [Warning] lambda_l1 is set=4.031471147672526e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.031471147672526e-06\n[LightGBM] [Warning] bagging_fraction is set=0.6000000000000001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6000000000000001\n[LightGBM] [Warning] lambda_l2 is set=0.5742659058240782, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.5742659058240782\n[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n[LightGBM] [Warning] min_data_in_leaf is set=1100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1100\ncv completed with scores: [0.2783027264425126, 0.0999377768614141, 0.05684225137654354, 0.11173484799457968, 0.009645499633159449]\nmean score: 0.11129262046164186\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=0.04706039924333112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04706039924333112\n[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n[LightGBM] [Warning] lambda_l2 is set=7.592932942963475, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.592932942963475\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=3100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3100\n\n============== Fold 2 ================\nTrain Date range: 2020-11-19 to 2021-06-02\nValid Date range: 2021-06-03 to 2021-08-30\n[LightGBM] [Warning] lambda_l1 is set=0.04706039924333112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04706039924333112\n[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n[LightGBM] [Warning] lambda_l2 is set=7.592932942963475, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.592932942963475\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=3100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3100\n\n============== Fold 3 ================\nTrain Date range: 2021-02-18 to 2021-08-30\nValid Date range: 2021-08-31 to 2021-11-26\n[LightGBM] [Warning] lambda_l1 is set=0.04706039924333112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04706039924333112\n[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n[LightGBM] [Warning] lambda_l2 is set=7.592932942963475, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.592932942963475\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=3100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3100\n\n============== Fold 4 ================\nTrain Date range: 2021-05-20 to 2021-11-26\nValid Date range: 2021-11-29 to 2022-02-25\n[LightGBM] [Warning] lambda_l1 is set=0.04706039924333112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04706039924333112\n[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n[LightGBM] [Warning] lambda_l2 is set=7.592932942963475, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.592932942963475\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=3100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3100\n\n============== Fold 5 ================\nTrain Date range: 2021-08-17 to 2022-02-25\nValid Date range: 2022-02-28 to 2022-05-27\n[LightGBM] [Warning] lambda_l1 is set=0.04706039924333112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04706039924333112\n[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n[LightGBM] [Warning] lambda_l2 is set=7.592932942963475, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.592932942963475\n[LightGBM] [Warning] feature_fraction is set=0.30000000000000004, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.30000000000000004\n[LightGBM] [Warning] min_data_in_leaf is set=3100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=3100\ncv completed with scores: [0.308804924729888, 0.11849312567402422, 0.08957304379789258, 0.1344192324804199, -0.030912316961372664]\nmean score: 0.1240756019441704\n\n============== Fold 1 ================\nTrain Date range: 2020-08-24 to 2021-03-04\nValid Date range: 2021-03-05 to 2021-06-02\n[LightGBM] [Warning] lambda_l1 is set=3.1392708270739e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.1392708270739e-05\n[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n[LightGBM] [Warning] lambda_l2 is set=2.1324376567996228e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1324376567996228e-05\n[LightGBM] [Warning] feature_fraction is set=0.6000000000000001, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6000000000000001\n[LightGBM] [Warning] min_data_in_leaf is set=4400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4400\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/2265245936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LGBM Regressor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_added\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/2265245936.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LGBM Regressor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_added\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/2357933939.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, data, k)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     categorical_feature=categorical_feature, callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[1;32m    900\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0minit_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m         )\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3021\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3023\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   3024\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:05:18.429299Z","iopub.execute_input":"2022-06-28T18:05:18.429719Z","iopub.status.idle":"2022-06-28T18:05:18.444016Z","shell.execute_reply.started":"2022-06-28T18:05:18.429684Z","shell.execute_reply":"2022-06-28T18:05:18.442937Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"524a31df-ad0c-4358-a006-b8fe1a1d1b77\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"524a31df-ad0c-4358-a006-b8fe1a1d1b77\")) {                    Plotly.newPlot(                        \"524a31df-ad0c-4358-a006-b8fe1a1d1b77\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,5,6,7,8],\"y\":[0.12431634439971187,0.11820952251166461,0.1258319811668865,0.12402823727476402,0.12221319774127189,0.11985653120472184,0.14014621961540139,0.11129262046164186,0.1240756019441704],\"type\":\"scatter\"},{\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8],\"y\":[0.12431634439971187,0.12431634439971187,0.1258319811668865,0.1258319811668865,0.1258319811668865,0.1258319811668865,0.14014621961540139,0.14014621961540139,0.14014621961540139],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"#Trials\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('524a31df-ad0c-4358-a006-b8fe1a1d1b77');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:05:29.544236Z","iopub.execute_input":"2022-06-28T18:05:29.544643Z","iopub.status.idle":"2022-06-28T18:05:29.552242Z","shell.execute_reply.started":"2022-06-28T18:05:29.544609Z","shell.execute_reply":"2022-06-28T18:05:29.550968Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{'n_estimators': 1000,\n 'learning_rate': 0.0497652883951982,\n 'num_leaves': 227,\n 'max_depth': 12,\n 'min_data_in_leaf': 500,\n 'lambda_l1': 3.060184062092395e-08,\n 'lambda_l2': 0.22368531324354038,\n 'bagging_fraction': 0.4,\n 'feature_fraction': 0.30000000000000004}"},"metadata":{}}]},{"cell_type":"code","source":"study.trials_dataframe().sort_values('value', ascending=False).drop(\n        columns=['datetime_start', 'datetime_complete', 'duration'])","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:05:55.887187Z","iopub.execute_input":"2022-06-28T18:05:55.888009Z","iopub.status.idle":"2022-06-28T18:05:55.922294Z","shell.execute_reply.started":"2022-06-28T18:05:55.887965Z","shell.execute_reply":"2022-06-28T18:05:55.921194Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"   number     value  params_bagging_fraction  params_feature_fraction  \\\n6       6  0.140146                      0.4                      0.3   \n2       2  0.125832                      0.7                      0.8   \n0       0  0.124316                      0.6                      0.4   \n8       8  0.124076                      0.5                      0.3   \n3       3  0.124028                      0.2                      0.3   \n4       4  0.122213                      0.3                      0.4   \n5       5  0.119857                      0.2                      0.4   \n1       1  0.118210                      0.3                      0.3   \n7       7  0.111293                      0.6                      0.7   \n9       9       NaN                      0.5                      0.6   \n\n   params_lambda_l1  params_lambda_l2  params_learning_rate  params_max_depth  \\\n6      3.060184e-08      2.236853e-01              0.049765                12   \n2      6.155887e-07      4.382897e+00              0.018619                 6   \n0      5.919890e-01      9.570907e-06              0.055348                12   \n8      4.706040e-02      7.592933e+00              0.303983                10   \n3      1.414222e-02      7.222250e-07              0.047761                 7   \n4      1.485209e-06      1.432262e-06              0.025290                 8   \n5      1.290493e-02      8.037525e-02              0.006838                 7   \n1      3.508213e-07      7.223380e-04              0.119387                 3   \n7      4.031471e-06      5.742659e-01              0.003540                12   \n9      3.139271e-05      2.132438e-05              0.109554                 9   \n\n   params_min_data_in_leaf  params_n_estimators  params_num_leaves     state  \n6                      500                 1000                227  COMPLETE  \n2                     4700                 1000                236  COMPLETE  \n0                     8100                  500                165  COMPLETE  \n8                     3100                10000                 11  COMPLETE  \n3                     5000                  500                209  COMPLETE  \n4                     8800                 4000                240  COMPLETE  \n5                     3500                 5000                  6  COMPLETE  \n1                     8000                 2000                 77  COMPLETE  \n7                     1100                 2000                175  COMPLETE  \n9                     4400                 4000                 50   RUNNING  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number</th>\n      <th>value</th>\n      <th>params_bagging_fraction</th>\n      <th>params_feature_fraction</th>\n      <th>params_lambda_l1</th>\n      <th>params_lambda_l2</th>\n      <th>params_learning_rate</th>\n      <th>params_max_depth</th>\n      <th>params_min_data_in_leaf</th>\n      <th>params_n_estimators</th>\n      <th>params_num_leaves</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.140146</td>\n      <td>0.4</td>\n      <td>0.3</td>\n      <td>3.060184e-08</td>\n      <td>2.236853e-01</td>\n      <td>0.049765</td>\n      <td>12</td>\n      <td>500</td>\n      <td>1000</td>\n      <td>227</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.125832</td>\n      <td>0.7</td>\n      <td>0.8</td>\n      <td>6.155887e-07</td>\n      <td>4.382897e+00</td>\n      <td>0.018619</td>\n      <td>6</td>\n      <td>4700</td>\n      <td>1000</td>\n      <td>236</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.124316</td>\n      <td>0.6</td>\n      <td>0.4</td>\n      <td>5.919890e-01</td>\n      <td>9.570907e-06</td>\n      <td>0.055348</td>\n      <td>12</td>\n      <td>8100</td>\n      <td>500</td>\n      <td>165</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.124076</td>\n      <td>0.5</td>\n      <td>0.3</td>\n      <td>4.706040e-02</td>\n      <td>7.592933e+00</td>\n      <td>0.303983</td>\n      <td>10</td>\n      <td>3100</td>\n      <td>10000</td>\n      <td>11</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.124028</td>\n      <td>0.2</td>\n      <td>0.3</td>\n      <td>1.414222e-02</td>\n      <td>7.222250e-07</td>\n      <td>0.047761</td>\n      <td>7</td>\n      <td>5000</td>\n      <td>500</td>\n      <td>209</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.122213</td>\n      <td>0.3</td>\n      <td>0.4</td>\n      <td>1.485209e-06</td>\n      <td>1.432262e-06</td>\n      <td>0.025290</td>\n      <td>8</td>\n      <td>8800</td>\n      <td>4000</td>\n      <td>240</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.119857</td>\n      <td>0.2</td>\n      <td>0.4</td>\n      <td>1.290493e-02</td>\n      <td>8.037525e-02</td>\n      <td>0.006838</td>\n      <td>7</td>\n      <td>3500</td>\n      <td>5000</td>\n      <td>6</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.118210</td>\n      <td>0.3</td>\n      <td>0.3</td>\n      <td>3.508213e-07</td>\n      <td>7.223380e-04</td>\n      <td>0.119387</td>\n      <td>3</td>\n      <td>8000</td>\n      <td>2000</td>\n      <td>77</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.111293</td>\n      <td>0.6</td>\n      <td>0.7</td>\n      <td>4.031471e-06</td>\n      <td>5.742659e-01</td>\n      <td>0.003540</td>\n      <td>12</td>\n      <td>1100</td>\n      <td>2000</td>\n      <td>175</td>\n      <td>COMPLETE</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>0.5</td>\n      <td>0.6</td>\n      <td>3.139271e-05</td>\n      <td>2.132438e-05</td>\n      <td>0.109554</td>\n      <td>9</td>\n      <td>4400</td>\n      <td>4000</td>\n      <td>50</td>\n      <td>RUNNING</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}