{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-20T14:18:52.743308Z","iopub.execute_input":"2022-06-20T14:18:52.743734Z","iopub.status.idle":"2022-06-20T14:18:52.802088Z","shell.execute_reply.started":"2022-06-20T14:18:52.743648Z","shell.execute_reply":"2022-06-20T14:18:52.801027Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/jpx-train-lot-ta-features/train_lot_ta_features.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/sample_submission.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/options.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/financials.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/secondary_stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/trades.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/example_test_files/stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/jpx_tokyo_market_prediction/competition.cpython-37m-x86_64-linux-gnu.so\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/jpx_tokyo_market_prediction/__init__.py\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/stock_fin_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/trades_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/stock_price_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/options_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/data_specifications/stock_list_spec.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/options.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/financials.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/secondary_stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/trades.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/options.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/financials.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/secondary_stock_prices.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/trades.csv\n/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\n/kaggle/input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\n","output_type":"stream"}]},{"cell_type":"code","source":"# %cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-06-16T09:58:15.303526Z","iopub.execute_input":"2022-06-16T09:58:15.304004Z","iopub.status.idle":"2022-06-16T09:58:15.33622Z","shell.execute_reply.started":"2022-06-16T09:58:15.30397Z","shell.execute_reply":"2022-06-16T09:58:15.333762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'train_alot_of_features.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T12:46:07.622389Z","iopub.execute_input":"2022-06-15T12:46:07.622849Z","iopub.status.idle":"2022-06-15T12:46:07.630614Z","shell.execute_reply.started":"2022-06-15T12:46:07.622812Z","shell.execute_reply":"2022-06-15T12:46:07.629618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\nimport talib as ta ","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:18:52.803850Z","iopub.execute_input":"2022-06-20T14:18:52.804518Z","iopub.status.idle":"2022-06-20T14:19:26.306807Z","shell.execute_reply.started":"2022-06-20T14:18:52.804484Z","shell.execute_reply":"2022-06-20T14:19:26.305445Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/talib-package/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from talib-binary==0.4.19) (1.21.6)\nInstalling collected packages: talib-binary\nSuccessfully installed talib-binary-0.4.19\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import early_stopping\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler\nimport missingno as msno\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit \n\nimport statsmodels.api as sm\nfrom pylab import rcParams\n\nfrom tqdm import tqdm\n\n\nfrom talib import abstract\n\n\n\nimport time\nimport gc\nimport sys\n\n# sys.path.insert(0, '../input/jpx-local-api')\n# from local_api import local_api","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:19:26.308684Z","iopub.execute_input":"2022-06-20T14:19:26.309192Z","iopub.status.idle":"2022-06-20T14:19:28.986486Z","shell.execute_reply.started":"2022-06-20T14:19:26.309127Z","shell.execute_reply":"2022-06-20T14:19:28.984986Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"def get_ta_features(df, inf=None, train=True):\n    \"\"\"\n    Get technical features from TA-Lib\n    ref : https://www.kaggle.com/code/daosword/jpx-pytorch-neural-network-with-ta-lib-features\n    \"\"\"\n    if train:\n        op = df['Open']\n        hi = df['High']\n        lo = df['Low']\n        cl = df['Close']\n        vo = df['Volume']\n\n    #     # Overlap Studies\n    #     df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n    #     df['DEMA'] = ta.DEMA(cl, timeperiod=30)\n\n        df['EMA7'] = ta.EMA(cl, 7)\n        df['EMA15'] = ta.EMA(cl, 15)\n        df['EMA30'] = ta.EMA(cl, 30)\n        df['EMA90'] = ta.EMA(cl, 90)\n\n    #     df['HT_TRENDLINE'] = ta.HT_TRENDLINE(cl)\n    #     df['KAMA'] = ta.KAMA(cl, timeperiod=30)\n    #     df['MA'] = ta.MA(cl, timeperiod=30, matype=0)\n    #     df['MIDPOINT'] = ta.MIDPOINT(cl, timeperiod=14)\n    #     df['SAR'] = ta.SAR(hi, lo, acceleration=0, maximum=0)\n    #     df['SAREXT'] = ta.SAREXT(hi, lo, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n    #     df['SMA'] = ta.SMA(cl, timeperiod=30)\n    #     df['T3'] = ta.T3(df['Close'], timeperiod=5, vfactor=0)\n    #     df['TEMA'] = ta.TEMA(df['Close'], timeperiod=30)\n    #     df['TRIMA'] = ta.TRIMA(df['Close'], timeperiod=30)\n    #     df['WMA'] = ta.WMA(df['Close'], timeperiod=30)\n\n        # Momentum Indicators\n    #     df['ADX'] = ta.ADX(hi, lo, cl, timeperiod=14)\n    #     df['ADXR'] = ta.ADXR(hi, lo, cl, timeperiod=14)\n    #     df['APO'] = ta.APO(cl, fastperiod=12, slowperiod=26, matype=0)\n    #     df['AROON_down'], df['AROON_up'] = ta.AROON(hi, lo, timeperiod=14)\n    #     df['AROONOSC'] = ta.AROONOSC(hi, lo, timeperiod=14)\n    #     df['BOP'] = ta.BOP(op, hi, lo, cl)\n    #     df['CCI'] = ta.CCI(hi, lo, cl, timeperiod=14)\n    #     df['DX'] = ta.DX(hi, lo, cl, timeperiod=14)\n    #     df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = ta.MACD(cl, fastperiod=12, slowperiod=26, signalperiod=9)\n    #     df['MFI'] = ta.MFI(hi, lo, cl, vo, timeperiod=14)\n    #     df['MINUS_DI'] = ta.MINUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['MINUS_DM'] = ta.MINUS_DM(hi, lo, timeperiod=14)\n        df['MOM'] = ta.MOM(cl, timeperiod=10)\n    #     df['PLUS_DI'] = ta.PLUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['PLUS_DM'] = ta.PLUS_DM(hi, lo, timeperiod=14)\n        df['RSI'] = ta.RSI(cl, timeperiod=14)\n        df['STOCH_slowk'], df['STOCH_slowd'] = ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n        df['STOCHF_fastk'], df['STOCHF_fastd'] = ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)\n        df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\n    #     df['TRIX'] = ta.TRIX(cl, timeperiod=30)\n    #     df['ULTOSC'] = ta.ULTOSC(hi, lo, cl, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n    #     df['WILLR'] = ta.WILLR(hi, lo, cl, timeperiod=14)\n\n        # Volume Indicators\n    #     df['AD'] = ta.AD(hi, lo, cl, vo)\n    #     df['ADOSC'] = ta.ADOSC(hi, lo, cl, vo, fastperiod=3, slowperiod=10)\n    #     df['OBV'] = ta.OBV(cl, vo)\n\n        # Volatility Indicators\n        df['ATR'] = ta.ATR(hi, lo, cl, timeperiod=14)\n        df['NATR'] = ta.NATR(hi, lo, cl, timeperiod=14)\n        df['TRANGE'] = ta.TRANGE(hi, lo, cl)\n\n        # Cycle Indicators\n    #     df['HT_DCPERIOD'] = ta.HT_DCPERIOD(cl)\n    #     df['HT_DCPHASE'] = ta.HT_DCPHASE(cl)\n    #     df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = ta.HT_PHASOR(cl)\n    #     df['HT_SINE_sine'], df['HT_SINE_leadsine'] = ta.HT_SINE(cl)\n    #     df['HT_TRENDMODE'] = ta.HT_TRENDMODE(cl)\n\n        # Statistic Functions\n    #     df['BETA'] = ta.BETA(hi, lo, timeperiod=5)\n    #     df['CORREL'] = ta.CORREL(hi, lo, timeperiod=30)\n    #     df['LINEARREG'] = ta.LINEARREG(cl, timeperiod=14) - cl\n    #     df['LINEARREG_ANGLE'] = ta.LINEARREG_ANGLE(cl, timeperiod=14)\n    #     df['LINEARREG_INTERCEPT'] = ta.LINEARREG_INTERCEPT(cl, timeperiod=14) - cl\n    #     df['LINEARREG_SLOPE'] = ta.LINEARREG_SLOPE(cl, timeperiod=14)\n        df['STDDEV'] = ta.STDDEV(cl, timeperiod=5, nbdev=1)   \n    \n    \n    else: #inference \n        op = inf['Open']\n        hi = inf['High']\n        lo = inf['Low']\n        cl = inf['Close']\n        vo = inf['Volume']\n        \n            #     # Overlap Studies\n    #     df['BBANDS_upper'], df['BBANDS_middle'], df['BBANDS_lower'] = ta.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n    #     df['DEMA'] = ta.DEMA(cl, timeperiod=30)\n\n        df['EMA7'] = ta.EMA(cl, 7).iloc[-1]\n        df['EMA15'] = ta.EMA(cl, 15).iloc[-1]\n        df['EMA30'] = ta.EMA(cl, 30).iloc[-1]\n        df['EMA90'] = ta.EMA(cl, 90).iloc[-1]\n\n    #     df['HT_TRENDLINE'] = ta.HT_TRENDLINE(cl)\n    #     df['KAMA'] = ta.KAMA(cl, timeperiod=30)\n    #     df['MA'] = ta.MA(cl, timeperiod=30, matype=0)\n    #     df['MIDPOINT'] = ta.MIDPOINT(cl, timeperiod=14)\n    #     df['SAR'] = ta.SAR(hi, lo, acceleration=0, maximum=0)\n    #     df['SAREXT'] = ta.SAREXT(hi, lo, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n    #     df['SMA'] = ta.SMA(cl, timeperiod=30)\n    #     df['T3'] = ta.T3(df['Close'], timeperiod=5, vfactor=0)\n    #     df['TEMA'] = ta.TEMA(df['Close'], timeperiod=30)\n    #     df['TRIMA'] = ta.TRIMA(df['Close'], timeperiod=30)\n    #     df['WMA'] = ta.WMA(df['Close'], timeperiod=30)\n\n        # Momentum Indicators\n    #     df['ADX'] = ta.ADX(hi, lo, cl, timeperiod=14)\n    #     df['ADXR'] = ta.ADXR(hi, lo, cl, timeperiod=14)\n    #     df['APO'] = ta.APO(cl, fastperiod=12, slowperiod=26, matype=0)\n    #     df['AROON_down'], df['AROON_up'] = ta.AROON(hi, lo, timeperiod=14)\n    #     df['AROONOSC'] = ta.AROONOSC(hi, lo, timeperiod=14)\n    #     df['BOP'] = ta.BOP(op, hi, lo, cl)\n    #     df['CCI'] = ta.CCI(hi, lo, cl, timeperiod=14)\n    #     df['DX'] = ta.DX(hi, lo, cl, timeperiod=14)\n    #     df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = ta.MACD(cl, fastperiod=12, slowperiod=26, signalperiod=9)\n    #     df['MFI'] = ta.MFI(hi, lo, cl, vo, timeperiod=14)\n    #     df['MINUS_DI'] = ta.MINUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['MINUS_DM'] = ta.MINUS_DM(hi, lo, timeperiod=14)\n        df['MOM'] = ta.MOM(cl, timeperiod=10).iloc[-1]\n    #     df['PLUS_DI'] = ta.PLUS_DI(hi, lo, cl, timeperiod=14)\n    #     df['PLUS_DM'] = ta.PLUS_DM(hi, lo, timeperiod=14)\n        df['RSI'] = ta.RSI(cl, timeperiod=14).iloc[-1]\n        df['STOCH_slowk'], df['STOCH_slowd'] = (ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)[0].iloc[-1],\n                                                ta.STOCH(hi, lo, cl, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)[1].iloc[-1])\n        df['STOCHF_fastk'], df['STOCHF_fastd'] = (ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)[0].iloc[-1],\n                                                  ta.STOCHF(hi, lo, cl, fastk_period=5, fastd_period=3, fastd_matype=0)[1].iloc[-1])\n        df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = (ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[0].iloc[-1],\n                                                      ta.STOCHRSI(cl, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[1].iloc[-1])\n    #     df['TRIX'] = ta.TRIX(cl, timeperiod=30)\n    #     df['ULTOSC'] = ta.ULTOSC(hi, lo, cl, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n    #     df['WILLR'] = ta.WILLR(hi, lo, cl, timeperiod=14)\n\n        # Volume Indicators\n    #     df['AD'] = ta.AD(hi, lo, cl, vo)\n    #     df['ADOSC'] = ta.ADOSC(hi, lo, cl, vo, fastperiod=3, slowperiod=10)\n    #     df['OBV'] = ta.OBV(cl, vo)\n\n        # Volatility Indicators\n        df['ATR'] = ta.ATR(hi, lo, cl, timeperiod=14).iloc[-1]\n        df['NATR'] = ta.NATR(hi, lo, cl, timeperiod=14).iloc[-1]\n        df['TRANGE'] = ta.TRANGE(hi, lo, cl).iloc[-1]\n\n        # Cycle Indicators\n    #     df['HT_DCPERIOD'] = ta.HT_DCPERIOD(cl)\n    #     df['HT_DCPHASE'] = ta.HT_DCPHASE(cl)\n    #     df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = ta.HT_PHASOR(cl)\n    #     df['HT_SINE_sine'], df['HT_SINE_leadsine'] = ta.HT_SINE(cl)\n    #     df['HT_TRENDMODE'] = ta.HT_TRENDMODE(cl)\n\n        # Statistic Functions\n    #     df['BETA'] = ta.BETA(hi, lo, timeperiod=5)\n    #     df['CORREL'] = ta.CORREL(hi, lo, timeperiod=30)\n    #     df['LINEARREG'] = ta.LINEARREG(cl, timeperiod=14) - cl\n    #     df['LINEARREG_ANGLE'] = ta.LINEARREG_ANGLE(cl, timeperiod=14)\n    #     df['LINEARREG_INTERCEPT'] = ta.LINEARREG_INTERCEPT(cl, timeperiod=14) - cl\n    #     df['LINEARREG_SLOPE'] = ta.LINEARREG_SLOPE(cl, timeperiod=14)\n        df['STDDEV'] = ta.STDDEV(cl, timeperiod=5, nbdev=1).iloc[-1]   \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:19:28.988633Z","iopub.execute_input":"2022-06-20T14:19:28.988996Z","iopub.status.idle":"2022-06-20T14:19:29.020864Z","shell.execute_reply.started":"2022-06-20T14:19:28.988966Z","shell.execute_reply":"2022-06-20T14:19:29.019793Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')\n# display(data)\n\n\n# using supplement data as test data\nsupp_data = pd.read_csv('/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv')\n\n# train_with_supp = pd.concat([train, supp_data]).reset_index(drop=True)\n# train_with_supp = train.copy()\n# train_with_supp\n\n# TA Features\n# train","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:27:04.528069Z","iopub.execute_input":"2022-06-20T14:27:04.528877Z","iopub.status.idle":"2022-06-20T14:27:12.507950Z","shell.execute_reply.started":"2022-06-20T14:27:04.528839Z","shell.execute_reply":"2022-06-20T14:27:12.506491Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def divideSecurities(df):\n    sec_list = []\n    print('Divide securities individually..')\n    for code in np.sort(df.SecuritiesCode.unique()):\n        sec_list.append(df.loc[df.SecuritiesCode == code, :].reset_index(drop=True))\n    return sec_list\n\n# sec_list = divideSecurities(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:27:24.162105Z","iopub.execute_input":"2022-06-20T14:27:24.163506Z","iopub.status.idle":"2022-06-20T14:27:24.188983Z","shell.execute_reply.started":"2022-06-20T14:27:24.163458Z","shell.execute_reply":"2022-06-20T14:27:24.187043Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def add_features_test1(sec_list): # only basic features\n    df_list = []\n    for df in tqdm(sec_list):\n        \n        \n        \n        # lagged feature 계산하기 전 결측치 채워넣기\n        df = df.fillna(method='ffill')\n        \n        \n        \n        \n        df_list.append(df)\n        \n        del df\n        \n    gc.collect()\n    df_feature_added = pd.concat(df_list).sort_values(['Date','SecuritiesCode'])\n    \n    return df_feature_added\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:40:35.499514Z","iopub.execute_input":"2022-06-20T14:40:35.499985Z","iopub.status.idle":"2022-06-20T14:40:35.507218Z","shell.execute_reply.started":"2022-06-20T14:40:35.499948Z","shell.execute_reply":"2022-06-20T14:40:35.506200Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def add_features_train(sec_list):\n    df_list = []\n    for df in tqdm(sec_list):\n        \n        # shadows\n        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n        \n\n        ## Rolling features ##\n        \n        # lagged features\n        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n        # lagged close, target (target 은 정확히 무엇? return인가)\n        \n        # lagged feature 계산하기 전 결측치 채워넣기\n        df = df.fillna(method='ffill')\n        \n        \n        \n        # All indicators in ta\n        df = get_ta_features(df.copy())\n                \n        # not add pattern recognition - feature importance 가 거의 0임. \n#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n\n\n        # fill ema features by backward -- 이렇게 채워진 것은 false data 이므로 일단 test 해보고 없애는 것을 검토하자.\n        df = df.fillna(method='bfill')\n\n    \n        # volatility\n        \n        df_list.append(df)\n        \n        del df\n        \n    gc.collect()\n    df_feature_added = pd.concat(df_list).sort_values(['Date','SecuritiesCode'])\n    \n    return df_feature_added\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:08:35.434138Z","iopub.execute_input":"2022-06-20T06:08:35.434559Z","iopub.status.idle":"2022-06-20T06:08:35.442115Z","shell.execute_reply.started":"2022-06-20T06:08:35.434525Z","shell.execute_reply":"2022-06-20T06:08:35.441433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add feature to data\n\ndf_added = add_features_train(sec_list)\n# df_added.to_csv('train_alot_of_features.csv', index=False)\n\n# load data\n# df_added = pd.read_csv('/kaggle/input/train-with-supp-feature-added-v1-all-cdl/train_with_supp_feature_added.csv')\n\n# df_added","metadata":{"execution":{"iopub.status.busy":"2022-06-16T09:08:36.088712Z","iopub.execute_input":"2022-06-16T09:08:36.08928Z","iopub.status.idle":"2022-06-16T09:09:08.893328Z","shell.execute_reply.started":"2022-06-16T09:08:36.089244Z","shell.execute_reply":"2022-06-16T09:09:08.89219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#         else:\n#             df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:08:44.243843Z","iopub.execute_input":"2022-06-20T06:08:44.244223Z","iopub.status.idle":"2022-06-20T06:08:44.257103Z","shell.execute_reply.started":"2022-06-20T06:08:44.244167Z","shell.execute_reply":"2022-06-20T06:08:44.256431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ex = reduce_mem_usage(df_added).loc[df_added.SecuritiesCode == 1377, :]\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features_infer(input_df, close_df): #input df 는 price 데이터\n    \n    df_list = []\n    \n    print('Divide input securities...')\n    sec_list = divideSecurities(input_df)\n    print('Divide close_df securities...')\n    close_list = divideSecurities(close_df) # for rolling features\n    print('='*10 + 'feature adding' + '='*10)\n    for i in range(len(sec_list)):\n        \n        if i == 0:\n            t0 = time.time()\n        close = close_list[i] #.loc[close_df.SecuritiesCode == code, :].fillna(method='ffill')\n        df = sec_list[i]\n#         display(df)\n        # test data의 open, high, low, close 중 nan 있으면 이전 값에서 가져와 채움\n        if df.loc[:, ['Open', 'High', 'Low', 'Close', 'Volume']].isna().any().any():\n            df.loc[:, ['Open', 'High', 'Low', 'Close', 'Volume']] = close.loc[close['Date'] == close.iloc[-1]['Date'], ['Open', 'High', 'Low', 'Close', 'Volume']].values \n        \n        if i == 0:\n            t1 = time.time()\n            print(t1 - t0, 's')\n        # shadows\n        df['upper_shadow'] = df['High'] - np.maximum(df['Open'], df['Close'])\n        df['lower_shadow'] = np.minimum(df['Open'], df['Close']) - df['Low']\n       \n        if i == 0:\n            t2 = time.time()\n            print(t2 - t1, 's')\n            \n        # lagged features\n        # 날짜 단위이므로 7일전, 30일전, 180일전, 360일전 \n        # lagged close, target (target 은 정확히 무엇? return인가)\n        \n        ## Rolling features ##\n        # TA-lib features - RSI, EMA 7-90\n        df = get_ta_features(df, close, train=False)\n\n        if i == 0:\n            t3 = time.time()\n            print(t3 - t2, 's')\n        \n#         for indicator in ta.get_function_groups()['Pattern Recognition']:\n#             df[str(indicator)] = getattr(ta,str(indicator))(df.Open, df.High, df.Low, df.Close)\n\n\n        # volatility\n        \n        df_list.append(df)\n    \n    df_feature_added = pd.concat(df_list)\n    print('='*10 + 'feature added' + '='*10)\n    return df_feature_added\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:07:14.898577Z","iopub.execute_input":"2022-06-20T07:07:14.898957Z","iopub.status.idle":"2022-06-20T07:07:14.911229Z","shell.execute_reply.started":"2022-06-20T07:07:14.898927Z","shell.execute_reply":"2022-06-20T07:07:14.910177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_train(df):\n    \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n#     minmax = MinMaxScaler()\n    stdsc = StandardScaler()\n    ordinal = OrdinalEncoder()\n\n    target = ['Target']\n#     minmax_features = ['Date']\n    ord_features = ['SecuritiesCode'] \n#     scaled_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'upper_shadow', 'lower_shadow',\n#                       'RSI', 'EMA7', 'EMA15', 'EMA30', 'EMA90'] + [c for c in df.columns if c.startswith('CDL')] # pattern recognition features\n    scaled_features = [i for i in df.columns if i not in ['Date', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag',\n                                                         'Target', 'SecuritiesCode']]\n    \n    \n#     date_scaled = minmax.fit_transform(dfc.loc[:,minmax_features])\n    date_code_ord = ordinal.fit_transform(dfc.loc[:,ord_features])\n    scaled = stdsc.fit_transform(dfc.loc[:,scaled_features])\n    \n#     display(pd.DataFrame(date_code_ord, columns=ord_features))\n#     display(pd.DataFrame(scaled, columns=scaled_features))\n    \n    \n    dfc_scaled = pd.concat([# pd.DataFrame(date_scaled, columns=minmax_features),\n                            pd.DataFrame(date_code_ord, columns=ord_features),\n                            pd.DataFrame(scaled, columns=scaled_features)], \n                            axis=1)\n    dfc_scaled = pd.concat([df['Date'].reset_index(drop=True), dfc_scaled],\n                           axis=1)\n    dfc_scaled = dfc_scaled.set_index(['Date'])\n\n    y = dfc.set_index(['Date']).loc[:, ['Target']]\n    \n    \n    return dfc_scaled, y, [ordinal, stdsc]\n    \n\n# X_scaled, y, trained_scalers = preprocess_train(df_added)  # 2021-12-06부터 test 시작이므로 그 전까지만 이용한다.\n\n# X_scaled\n# y","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:09:00.480351Z","iopub.execute_input":"2022-06-20T06:09:00.481243Z","iopub.status.idle":"2022-06-20T06:09:00.48955Z","shell.execute_reply.started":"2022-06-20T06:09:00.4812Z","shell.execute_reply":"2022-06-20T06:09:00.488739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_train_test1(df):\n    \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n#     minmax = MinMaxScaler()\n    stdsc = StandardScaler()\n#     ordinal = OrdinalEncoder()\n\n    target = ['Target']\n#     minmax_features = ['Date']\n\n    # not used in case1\n    #ord_features = ['SecuritiesCode'] \n    \n#     scaled_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'upper_shadow', 'lower_shadow',\n#                       'RSI', 'EMA7', 'EMA15', 'EMA30', 'EMA90'] + [c for c in df.columns if c.startswith('CDL')] # pattern recognition features\n    scaled_features = [i for i in df.columns if i not in ['Date', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag',\n                                                         'Target', 'SecuritiesCode']]\n    \n    \n#     date_scaled = minmax.fit_transform(dfc.loc[:,minmax_features])\n#     date_code_ord = ordinal.fit_transform(dfc.loc[:,ord_features])\n    scaled = stdsc.fit_transform(dfc.loc[:,scaled_features])\n    \n#     display(pd.DataFrame(date_code_ord, columns=ord_features))\n#     display(pd.DataFrame(scaled, columns=scaled_features))\n    \n    \n    dfc_scaled = pd.concat([dfc.loc[:, ['SecuritiesCode']].reset_index(drop=True),\n#                             pd.DataFrame(date_code_ord, columns=ord_features),\n                            pd.DataFrame(scaled, columns=scaled_features)], \n                            axis=1)\n    dfc_scaled = pd.concat([df['Date'].reset_index(drop=True), dfc_scaled],\n                           axis=1)\n    dfc_scaled = dfc_scaled.set_index(['Date'])\n\n    y = dfc.set_index(['Date']).loc[:, ['Target']]\n    \n    \n    return dfc_scaled, y, [stdsc]\n    \n\n# X_scaled, y, trained_scalers = preprocess_train(df_added)  # 2021-12-06부터 test 시작이므로 그 전까지만 이용한다.\n\n# X_scaled\n# y","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:41:44.168360Z","iopub.execute_input":"2022-06-20T14:41:44.168890Z","iopub.status.idle":"2022-06-20T14:41:44.183706Z","shell.execute_reply.started":"2022-06-20T14:41:44.168840Z","shell.execute_reply":"2022-06-20T14:41:44.182766Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def preprocess_inference_test1(df, trained_scalers: list):\n#     ordinal = trained_scalers[0]\n    stdsc = trained_scalers[0]\n    \n      \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n    target = ['Target']\n#     ord_features = ['SecuritiesCode'] \n#     scaled_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'upper_shadow', 'lower_shadow',\n#                       'RSI', 'EMA7', 'EMA15', 'EMA30', 'EMA90'] + [c for c in df.columns if c.startswith('CDL')] # pattern recognition features\n    scaled_features = [i for i in df.columns if i not in ['Date', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag',\n                                                         'Target', 'SecuritiesCode']]\n    \n#     date_code_ord = ordinal.transform(dfc.loc[:,ord_features])\n    scaled = stdsc.transform(dfc.loc[:,scaled_features])\n    dfc_scaled = pd.concat([dfc.loc[:, ['SecuritiesCode']].reset_index(drop=True),\n                            pd.DataFrame(scaled, columns=scaled_features)], axis=1)\n\n    \n    return dfc_scaled\n    \n\n# X_test_scaled = preprocess_train(df_added, trained_scalers)\n\n# X_test_scaled","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:43:02.044904Z","iopub.execute_input":"2022-06-20T14:43:02.045406Z","iopub.status.idle":"2022-06-20T14:43:02.054787Z","shell.execute_reply.started":"2022-06-20T14:43:02.045367Z","shell.execute_reply":"2022-06-20T14:43:02.053663Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def preprocess_inference(df, trained_scalers: list):\n    ordinal = trained_scalers[0]\n    stdsc = trained_scalers[1]\n    \n      \n    # remove columns - Date removed temporarily\n    dfc = df.drop(columns=['RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'])\n    \n    \n    target = ['Target']\n    ord_features = ['SecuritiesCode'] \n#     scaled_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'upper_shadow', 'lower_shadow',\n#                       'RSI', 'EMA7', 'EMA15', 'EMA30', 'EMA90'] + [c for c in df.columns if c.startswith('CDL')] # pattern recognition features\n    scaled_features = [i for i in df.columns if i not in ['Date', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag',\n                                                         'Target', 'SecuritiesCode']]\n    \n    date_code_ord = ordinal.transform(dfc.loc[:,ord_features])\n    scaled = stdsc.transform(dfc.loc[:,scaled_features])\n    dfc_scaled = pd.concat([pd.DataFrame(date_code_ord, columns=ord_features),\n                            pd.DataFrame(scaled, columns=scaled_features)], axis=1)\n\n    \n    return dfc_scaled\n    \n\n# X_test_scaled = preprocess_train(df_added, trained_scalers)\n\n# X_test_scaled","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:09:02.366222Z","iopub.execute_input":"2022-06-20T06:09:02.367222Z","iopub.status.idle":"2022-06-20T06:09:02.377304Z","shell.execute_reply.started":"2022-06-20T06:09:02.367152Z","shell.execute_reply":"2022-06-20T06:09:02.37631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:43:24.080258Z","iopub.execute_input":"2022-06-16T07:43:24.081744Z","iopub.status.idle":"2022-06-16T07:43:24.120993Z","shell.execute_reply.started":"2022-06-16T07:43:24.081651Z","shell.execute_reply":"2022-06-16T07:43:24.12018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T08:16:55.24414Z","iopub.execute_input":"2022-06-16T08:16:55.244647Z","iopub.status.idle":"2022-06-16T08:16:56.2363Z","shell.execute_reply.started":"2022-06-16T08:16:55.244612Z","shell.execute_reply":"2022-06-16T08:16:56.234198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base model - lgbm \nlgb = LGBMRegressor()\nlgb.fit(X_scaled, y)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T09:09:35.040741Z","iopub.execute_input":"2022-06-16T09:09:35.041323Z","iopub.status.idle":"2022-06-16T09:09:47.40886Z","shell.execute_reply.started":"2022-06-16T09:09:35.041287Z","shell.execute_reply":"2022-06-16T09:09:47.407755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"feature importance : only 9997","metadata":{}},{"cell_type":"code","source":"X_scaled, y, trained_scalers = preprocess_train(train_9997)\nlgb = LGBMRegressor()\nlgb.fit(X_scaled, y)\nfeature_imp = pd.DataFrame(\n    sorted(zip(lgb.feature_importances_, X_scaled.columns), reverse=True),\n    columns = ['Value', 'Feature'])\ndisplay(feature_imp)\nplt.figure(figsize=(10,35))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T08:46:41.361447Z","iopub.execute_input":"2022-06-16T08:46:41.362048Z","iopub.status.idle":"2022-06-16T08:46:42.934865Z","shell.execute_reply.started":"2022-06-16T08:46:41.361999Z","shell.execute_reply":"2022-06-16T08:46:42.933567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"feature importance - all securities","metadata":{}},{"cell_type":"code","source":"feature_imp = pd.DataFrame(\n    sorted(zip(lgb.feature_importances_, X_scaled.columns), reverse=True),\n    columns = ['Value', 'Feature'])\ndisplay(feature_imp)\nplt.figure(figsize=(10,35))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:45:37.78416Z","iopub.execute_input":"2022-06-16T07:45:37.786859Z","iopub.status.idle":"2022-06-16T07:45:39.031463Z","shell.execute_reply.started":"2022-06-16T07:45:37.786781Z","shell.execute_reply":"2022-06-16T07:45:39.030245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# top 15 features\nfeature_imp.iloc[:10]['Feature'].values","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:52:20.763241Z","iopub.execute_input":"2022-06-16T07:52:20.764323Z","iopub.status.idle":"2022-06-16T07:52:20.77382Z","shell.execute_reply.started":"2022-06-16T07:52:20.764283Z","shell.execute_reply":"2022-06-16T07:52:20.772681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# permutation importance - 1333\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(lgb, random_state=1).fit(X_scaled, y)\neli5.show_weights(perm, feature_names = X_scaled.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-06-16T08:41:22.622935Z","iopub.execute_input":"2022-06-16T08:41:22.623475Z","iopub.status.idle":"2022-06-16T08:41:34.405516Z","shell.execute_reply.started":"2022-06-16T08:41:22.623431Z","shell.execute_reply":"2022-06-16T08:41:34.403457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#handpicked features\n'MFI', 'HT_PHASOR_quadrature', 'CORREL', 'STDDEV', 'RSI', 'STOCH_slowk', 'EMA7', 'EMA30'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eval - 9997\nX_test_scaled, y_test, _ = preprocess_train(eval_9997)\n\n# permutation importance - 9997 - on test set\n\nperm = PermutationImportance(lgb, random_state=1).fit(X_test_scaled, y_test)\neli5.show_weights(perm, feature_names = X_test_scaled.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-06-16T08:55:11.353994Z","iopub.execute_input":"2022-06-16T08:55:11.354424Z","iopub.status.idle":"2022-06-16T08:55:13.245006Z","shell.execute_reply.started":"2022-06-16T08:55:11.354394Z","shell.execute_reply":"2022-06-16T08:55:13.244066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"def timeseriesCVerror(X, y, fold, loss, Model):#model, loss_function):\n    errors = []\n    tscv = TimeSeriesSplit(test_size=120)\n    # split input : index date\n    grid = X.index.unique()\n    \n    k = 0\n    for train_idx, test_idx in tscv.split(grid):\n        t0 = time.time()\n        k += 1\n        print(f'========Training fold {k}========')\n        \n        print('training size:', len(train_idx), '  test size:', len(test_idx))\n#         print(grid[train_idx])\n        X_train, y_train = X.loc[grid[train_idx], :], y.loc[grid[train_idx], :]\n        X_test, y_test = X.loc[grid[test_idx], :], y.loc[grid[test_idx], :]\n        \n        mod = Model.fit(X_train, y_train,           \n                        eval_set=[(X_test, y_test)],\n                        eval_metric=\"rmse\",\n                        callbacks=[early_stopping(stopping_rounds=10)]\n                       )\n        y_pred = mod.predict(X_test)\n        error = loss(y_pred, y_test)\n        errors.append(error)\n        \n        print(f'Fold {k} finished with loss: {error:.10f} in {time.time() - t0:.4f}s\\n\\n')\n        \n#     print(errors)\n    print(f'Average Loss: {np.mean(errors):.10f}')\n    return\n\n\ntimeseriesCVerror(X_scaled, y, 10, mean_squared_error, \n                  LGBMRegressor(seed=47)\n                 )","metadata":{"execution":{"iopub.status.busy":"2022-06-16T09:11:56.650294Z","iopub.execute_input":"2022-06-16T09:11:56.650793Z","iopub.status.idle":"2022-06-16T09:12:24.102103Z","shell.execute_reply.started":"2022-06-16T09:11:56.650758Z","shell.execute_reply":"2022-06-16T09:12:24.100902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(df):\n    df_added = add_features_train(sec_list)\n    X_scaled, y, trained_scalers = preprocess_train(df_added)\n\n    y_pred = lgb.predict(X_scaled)\n    print(mean_squared_error(y_pred, y))\n    return\n\nevaluate(supp_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T09:12:40.428145Z","iopub.execute_input":"2022-06-16T09:12:40.429172Z","iopub.status.idle":"2022-06-16T09:13:19.231686Z","shell.execute_reply.started":"2022-06-16T09:12:40.429137Z","shell.execute_reply":"2022-06-16T09:13:19.230819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(df):\n    df_added = add_features_train(sec_list)\n    X_scaled, y, trained_scalers = preprocess_train(df_added)\n\n    y_pred = lgb.predict(X_scaled)\n    print(mean_squared_error(y_pred, y))\n    return\n\nevaluate(supp_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T09:10:55.519647Z","iopub.execute_input":"2022-06-16T09:10:55.520069Z","iopub.status.idle":"2022-06-16T09:10:55.755809Z","shell.execute_reply.started":"2022-06-16T09:10:55.520036Z","shell.execute_reply":"2022-06-16T09:10:55.754305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"# import jpx_tokyo_market_prediction\n# env = jpx_tokyo_market_prediction.make_env()\n# iter_test = env.iter_test()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n#     t0 = time.time()\n    \n    # 이전 데이터와 합치고 최근 140일치만 이용한다\n    today = prices.iloc[0]['Date']\n    lastday = str(pd.to_datetime(today) - pd.DateOffset(140))\n    \n    if i == 0:\n        close_df = pd.concat([\n            train_with_supp.loc[\n                (train_with_supp['Date'] > lastday) & (train_with_supp['Date'] < today), \n                    ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']],\n                prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']]\n            ]).reset_index(drop=True)\n        \n    else:\n        close_df = pd.concat([\n            close_df.loc[\n                (close_df['Date'] > lastday) & (close_df['Date'] < today), \n                    ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']],\n                prices.loc[:, ['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close']]\n            ]).reset_index(drop=True)\n        \n\n    feat = add_features_infer(prices, close_df)\n    X = preprocess_inference(feat, trained_scalers)\n\n    # X, y\n    X['Target'] = lgb.predict(X)\n    X['Rank'] = (X['Target'].rank(method='first', ascending=False)-1).astype(int)\n    sample_prediction['Rank'] = X['Rank'].values\n    \n    # check Rank\n    assert sample_prediction[\"Rank\"].notna().all()\n    assert sample_prediction[\"Rank\"].min() == 0\n    assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n#     display(sample_prediction)\n\n    env.predict(sample_prediction)\n#     print(time.time() - t0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation - Sharpe ratio\n\ntimeseriesCV - mean sharpe ratio 계산  \n-- 비교 -- \n1. 기존 LGBM 모델\n2. TA Feature 더 추가되어 학습된 LGBM 모델","metadata":{}},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:30:23.557367Z","iopub.execute_input":"2022-06-20T14:30:23.557933Z","iopub.status.idle":"2022-06-20T14:30:23.571128Z","shell.execute_reply.started":"2022-06-20T14:30:23.557880Z","shell.execute_reply":"2022-06-20T14:30:23.569886Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test : calculate sharpe ratio over train period \n# 결과로 Date, target, rank를 붙인 df를 반환한다.\n# cv - train+supp (feature generated) 에서 7-fold time-series cv 이용\n\ndef evaluate(train, val):\n    \n    # train model\n    sec_list = divideSecurities(train)\n#     df_added = add_features_train(sec_list)\n    df_added = add_features_test1(sec_list)\n    X_scaled, y, trained_scalers = preprocess_train_test1(df_added)\n    \n    # base model - lgbm \n    lgb = LGBMRegressor()\n    lgb.fit(X_scaled, y)\n    \n    # predict evaluation data - same as train\n    sec_list_val = divideSecurities(val)\n    df_added_val = add_features_test1(sec_list_val)\n    X_scaled_val = preprocess_inference_test1(df_added_val, trained_scalers)\n    \n    y_pred = lgb.predict(X_scaled_val)\n    val['predict'] = y_pred\n    \n    # evaluate - eval set 일자별로 순위 계산하고, 이 결과를 바탕으로 sharpe ratio 계산\n    eval_dates = val['Date'].unique()  # supp 일자 리스트\n\n    predicted_df_list = []\n    for i, date in enumerate(tqdm(eval_dates)):\n        X = val[val['Date'] == date]\n        # X, y\n        X['Rank'] = (X['predict'].rank(method='first', ascending=False)-1).astype(int)\n\n        # check Rank\n        assert X[\"Rank\"].notna().all()\n        assert X[\"Rank\"].min() == 0\n        assert X[\"Rank\"].max() == len(X[\"Rank\"]) - 1\n        predicted_df_list.append(X)\n\n    predicted_df = pd.concat(predicted_df_list)\n#     display(predicted_df)\n    eval_score = calc_spread_return_sharpe(predicted_df)\n    print('evaluated score:', eval_score)\n    return eval_score","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:37:36.613435Z","iopub.execute_input":"2022-06-20T14:37:36.613854Z","iopub.status.idle":"2022-06-20T14:37:36.625822Z","shell.execute_reply.started":"2022-06-20T14:37:36.613818Z","shell.execute_reply":"2022-06-20T14:37:36.624515Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"test_train = train.loc[(train.Date > '2020-12-31')]\ntest_train.tail(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:44:38.541361Z","iopub.execute_input":"2022-06-20T06:44:38.542234Z","iopub.status.idle":"2022-06-20T06:44:38.710198Z","shell.execute_reply.started":"2022-06-20T06:44:38.542193Z","shell.execute_reply":"2022-06-20T06:44:38.709274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"supp_data.Date","metadata":{"execution":{"iopub.status.busy":"2022-06-20T06:32:56.287917Z","iopub.execute_input":"2022-06-20T06:32:56.288315Z","iopub.status.idle":"2022-06-20T06:32:56.298461Z","shell.execute_reply.started":"2022-06-20T06:32:56.288284Z","shell.execute_reply":"2022-06-20T06:32:56.297339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature set cases\n1. basic features - open, high, low, close, volume\n2. with TA features\n3. with rolling features only\n4. not using SecuritiesCode as features because many of them are missing certain period","metadata":{}},{"cell_type":"code","source":"# test1 - basic features - open, high, low, close, volume\n# evaluate on each CV\ntscv = TimeSeriesSplit(n_splits=5)#test_size=60)  # 2000개 주식이 모두 있는 기간이 300일 정도이다. 그래서 직전 기간까지 train하고 60일씩 5-fold로 쪼갬\n# split input : index date\ngrid = np.concatenate([train['Date'].unique(), supp_data['Date'].unique()])\n\n# train_cv = train.set_index('Date')\n# eval_cv = supp_data.set_index('Date')\ntrain_with_supp = pd.concat([train, supp_data]).set_index('Date')\n\nscores = []\nk = 0\nfor train_idx, eval_idx in tscv.split(grid):\n    k += 1\n    print(f'========Training fold {k}========')\n    t0 = time.time()\n    print('training size:', len(train_idx), '  test size:', len(eval_idx))\n    \n    print(\"Train Date range: {} to {}\".format(grid[train_idx].min(),grid[train_idx].max()))\n    print(\"Valid Date range: {} to {}\".format(grid[eval_idx].min(),grid[eval_idx].max()))\n    \n    score = evaluate(train_with_supp.loc[grid[train_idx]].reset_index(),\n                     train_with_supp.loc[grid[eval_idx]].reset_index())\n    print(f'Fold {k} evaluated in {time.time() - t0 :.3f}s')\n    print(f'Fold {k} score: {score :.6f}')\n    scores.append(score)\n                  \nprint(scores)\nprint('cv mean score:', np.mean(scores))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T14:43:05.967895Z","iopub.execute_input":"2022-06-20T14:43:05.968332Z","iopub.status.idle":"2022-06-20T14:46:01.379794Z","shell.execute_reply.started":"2022-06-20T14:43:05.968296Z","shell.execute_reply":"2022-06-20T14:46:01.378594Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"========Training fold 1========\ntraining size: 222   test size: 219\nTrain Date range: 2017-01-04 to 2017-11-24\nValid Date range: 2017-11-27 to 2018-10-16\nDivide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1886/1886 [00:01<00:00, 1728.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Divide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1920/1920 [00:00<00:00, 2435.20it/s]\n  0%|          | 0/219 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n100%|██████████| 219/219 [00:15<00:00, 14.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"evaluated score: 0.04946547769053153\nFold 1 evaluated in 27.682s\nFold 1 score: 0.049465\n========Training fold 2========\ntraining size: 441   test size: 219\nTrain Date range: 2017-01-04 to 2018-10-16\nValid Date range: 2018-10-17 to 2019-09-11\nDivide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1920/1920 [00:00<00:00, 2025.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Divide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1947/1947 [00:00<00:00, 2377.82it/s]\n  0%|          | 0/219 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n100%|██████████| 219/219 [00:15<00:00, 14.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"evaluated score: 0.14975844330676855\nFold 2 evaluated in 30.604s\nFold 2 score: 0.149758\n========Training fold 3========\ntraining size: 660   test size: 219\nTrain Date range: 2017-01-04 to 2019-09-11\nValid Date range: 2019-09-12 to 2020-08-11\nDivide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1947/1947 [00:01<00:00, 1731.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Divide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1981/1981 [00:00<00:00, 2397.57it/s]\n  0%|          | 0/219 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n100%|██████████| 219/219 [00:15<00:00, 13.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"evaluated score: 0.16294253484131702\nFold 3 evaluated in 34.736s\nFold 3 score: 0.162943\n========Training fold 4========\ntraining size: 879   test size: 219\nTrain Date range: 2017-01-04 to 2020-08-11\nValid Date range: 2020-08-12 to 2021-07-01\nDivide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1981/1981 [00:01<00:00, 1524.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Divide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:00<00:00, 2367.51it/s]\n  0%|          | 0/219 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n100%|██████████| 219/219 [00:16<00:00, 13.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"evaluated score: 0.0989874116256509\nFold 4 evaluated in 38.161s\nFold 4 score: 0.098987\n========Training fold 5========\ntraining size: 1098   test size: 219\nTrain Date range: 2017-01-04 to 2021-07-01\nValid Date range: 2021-07-02 to 2022-05-27\nDivide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:01<00:00, 1326.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Divide securities individually..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [00:00<00:00, 2203.57it/s]\n  0%|          | 0/219 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n100%|██████████| 219/219 [00:16<00:00, 13.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"evaluated score: -0.005139634784528597\nFold 5 evaluated in 43.411s\nFold 5 score: -0.005140\n[0.04946547769053153, 0.14975844330676855, 0.16294253484131702, 0.0989874116256509, -0.005139634784528597]\ncv mean score: 0.09120284653594787\n","output_type":"stream"}]},{"cell_type":"code","source":"np.concatenate([train['Date'].unique(), supp_data['Date'].unique()])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T08:21:17.094303Z","iopub.execute_input":"2022-06-20T08:21:17.094727Z","iopub.status.idle":"2022-06-20T08:21:17.240164Z","shell.execute_reply.started":"2022-06-20T08:21:17.094682Z","shell.execute_reply":"2022-06-20T08:21:17.239223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"12-23","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T08:45:45.819064Z","iopub.execute_input":"2022-06-20T08:45:45.819645Z","iopub.status.idle":"2022-06-20T08:45:45.840563Z","shell.execute_reply.started":"2022-06-20T08:45:45.81958Z","shell.execute_reply":"2022-06-20T08:45:45.839625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(grid[grid>'2020-12-23']) / 5","metadata":{"execution":{"iopub.status.busy":"2022-06-20T08:50:18.867596Z","iopub.execute_input":"2022-06-20T08:50:18.868133Z","iopub.status.idle":"2022-06-20T08:50:18.875366Z","shell.execute_reply.started":"2022-06-20T08:50:18.868098Z","shell.execute_reply":"2022-06-20T08:50:18.874538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tscv = TimeSeriesSplit(test_size=60)\nfor train_idx, test_idx in tscv.split(grid):\n    print(grid[train_idx[-1]])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T08:51:17.51592Z","iopub.execute_input":"2022-06-20T08:51:17.51632Z","iopub.status.idle":"2022-06-20T08:51:17.522169Z","shell.execute_reply.started":"2022-06-20T08:51:17.516289Z","shell.execute_reply":"2022-06-20T08:51:17.521412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid[train_idx]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T08:47:18.356671Z","iopub.execute_input":"2022-06-20T08:47:18.357074Z","iopub.status.idle":"2022-06-20T08:47:18.365202Z","shell.execute_reply.started":"2022-06-20T08:47:18.357038Z","shell.execute_reply":"2022-06-20T08:47:18.364394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T08:47:11.547972Z","iopub.execute_input":"2022-06-20T08:47:11.548326Z","iopub.status.idle":"2022-06-20T08:47:12.171643Z","shell.execute_reply.started":"2022-06-20T08:47:11.548298Z","shell.execute_reply":"2022-06-20T08:47:12.170467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dateg = train.loc[:, ['SecuritiesCode', 'Target']].groupby(train.index).count()\ndateg","metadata":{"execution":{"iopub.status.busy":"2022-06-20T08:44:28.380889Z","iopub.execute_input":"2022-06-20T08:44:28.381398Z","iopub.status.idle":"2022-06-20T08:44:28.439668Z","shell.execute_reply.started":"2022-06-20T08:44:28.381349Z","shell.execute_reply":"2022-06-20T08:44:28.438701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}